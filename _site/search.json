[
  {
    "objectID": "modules/module-3.5.html",
    "href": "modules/module-3.5.html",
    "title": "Module 3.5",
    "section": "",
    "text": "Tip\n\n\n\nBefore beginning this module, make sure to review:\n\nThe meaning of a proportion and sampling variability\nThe concepts of null and alternative hypotheses\nWhat p-values and significance levels represent"
  },
  {
    "objectID": "modules/module-3.5.html#prework",
    "href": "modules/module-3.5.html#prework",
    "title": "Module 3.5",
    "section": "",
    "text": "Tip\n\n\n\nBefore beginning this module, make sure to review:\n\nThe meaning of a proportion and sampling variability\nThe concepts of null and alternative hypotheses\nWhat p-values and significance levels represent"
  },
  {
    "objectID": "modules/module-3.5.html#motivation-a-claim-about-a-jobs-program",
    "href": "modules/module-3.5.html#motivation-a-claim-about-a-jobs-program",
    "title": "Module 3.5",
    "section": "Motivation: A Claim About a Jobs Program",
    "text": "Motivation: A Claim About a Jobs Program\nInternational development organizations sometimes run training programs to help people find employment. Suppose the national unemployment rate in a low-income country is 30%. One organization runs a jobs program and claims success because only 15 out of 60 participants are unemployed—a rate of 25%.\nIs this a meaningful improvement? Or could this difference just be due to random chance? This is where hypothesis testing comes in.\nTo begin, let’s simulate the data for this program in R:\n\nlibrary(tidyverse)\n\njobs_program &lt;- tibble(\n  outcome = c(rep(\"unemployed\", 15), rep(\"employed\", 45))\n)\n\njobs_program |&gt;\n  ggplot(aes(x = outcome)) +\n  geom_bar(fill = \"steelblue\") + theme_bw()"
  },
  {
    "objectID": "modules/module-3.5.html#understanding-hypothesis-testing",
    "href": "modules/module-3.5.html#understanding-hypothesis-testing",
    "title": "Module 3.5",
    "section": "Understanding Hypothesis Testing",
    "text": "Understanding Hypothesis Testing\nHypothesis testing is a statistical framework that allows us to make decisions about population parameters based on sample data. The process begins by establishing two competing hypotheses: the null hypothesis (\\(H_0\\)) and the alternative hypothesis (\\(H_A\\)). The null hypothesis represents the default position or “status quo”—it typically states that there is no effect, no difference, or that any observed pattern is simply due to random chance.\nYou can think of the null hypothesis as the skeptical position that says “nothing interesting is happening here.” The alternative hypothesis, in contrast, represents what we’re actually trying to find evidence for. It embodies our research question and claims that there is a real effect or meaningful difference that cannot be explained by random variation alone.\nThe logic of hypothesis testing follows a proof-by-contradiction approach. We begin by assuming that the null hypothesis is true, then examine our sample data to see whether it provides convincing evidence against this assumption. Specifically, we ask: “If the null hypothesis were actually true, how likely would it be to observe data like what we actually collected?” If our observed data would be reasonably likely under the null hypothesis, we fail to reject the null—we simply don’t have convincing evidence that anything unusual is going on. However, if our data would be extremely unlikely assuming the null hypothesis is true, we reject the null hypothesis in favor of the alternative, concluding that we have found evidence for a real effect.\nFor our example, we want to evaluate whether this jobs program under consideration genuinely reduced unemployment. To do this, we use hypothesis testing. First we state our null and alternative hypotheses:\nNull hypothesis (\\(H_0\\)): The unemployment rate among program participants is the same as the national rate (30%).\nAlternative hypothesis (\\(H_A\\)): The unemployment rate among participants is lower than the national rate.\nNext, we simulate what kinds of results we’d expect if the null hypothesis were true, and then see how our observed result compares. A p-value is the probability of seeing a result as extreme as ours—or more extreme—under the assumption that the null hypothesis is true.\nWe then compare the p-value to a critical value (\\(\\alpha\\)) which is the threshold at which we will reject the null hypothesis. If the p-value is less than \\(\\alpha\\), we reject the null hypothesis. A standard threshold for \\(\\alpha\\) is 0.05. But note that while the choice of \\(\\alpha\\) as .05 is standard, it is also somewhat arbitrary and so the choice of \\(\\alpha\\) can depend on the context of the study.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nThink about a real-world example where you might want to test a claim using hypothesis testing.\nThis could be a claim about a new product, a policy change, or an intervention in a community.\nWhat would your null hypothesis be?\nWhat would your alternative hypothesis be?"
  },
  {
    "objectID": "modules/module-3.5.html#simulating-a-null-distribution",
    "href": "modules/module-3.5.html#simulating-a-null-distribution",
    "title": "Module 3.5",
    "section": "Simulating a Null Distribution",
    "text": "Simulating a Null Distribution\nTo evaluate the claim about the jobs program, we need to simulate a null distribution of unemployment rates under the null hypothesis. A null distribution is a collection of simulated results that represent what we would expect to see if the null hypothesis were true (in other words “if nothing was going on”).\nTo simulate thousands of samples efficiently, we can use the infer package from the tidymodels framework. This package allows us to specify our null hypothesis, generate random samples, and calculate the proportion of unemployed individuals in each sample.\n\nlibrary(tidymodels)\n\nnull_dist &lt;- jobs_program |&gt;\n  specify(response = outcome, success = \"unemployed\") |&gt;\n  hypothesize(null = \"point\", p = c(\"unemployed\" = 0.30, \"employed\" = 0.70)) |&gt;\n  generate(reps = 10000, type = \"draw\") |&gt;\n  calculate(stat = \"prop\")\n\nnull_dist |&gt;\n  summarize(mean = mean(stat))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1 0.301\n\nggplot(data = null_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth = 0.05, fill = \"steelblue4\") +\n  labs(title = \"Null distribution\")  + theme_bw()\n\n\n\n\n\n\n\nHere we specified the response variable as outcome, with “unemployed” as the success category. We then hypothesized that the unemployment rate is 30% (the national rate) and generated 5000 random samples from this distribution. Finally, we calculated the proportion of unemployed individuals in each sample.\nNext, we can calculate the p-value for our observed resultusing the get_p_value() function from the infer package. Here, the p-value tells us in how many of the simulated samples the proportion of unemployed individuals was at least as extreme as the observed sample proportion (15 out of 60, or 25%).\n\n# Calculate p-value using infer\nnull_dist |&gt;\n  get_p_value(obs_stat = 15/60, direction = \"less\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.243\n\n\nThen we can visualize the null distribution and shade the area corresponding to our observed p-value.\n\n# Visualize p-value in context of null distribution\nvisualize(null_dist) +\n  shade_p_value(obs_stat = 15/60, direction = \"less\")\n\n\n\n\n\n\n\nIf the p-value is greater than 0.05, we say that this result is not statistically significant. That is, we do not have strong evidence that the program was associated with reduced unemployment.\nIn this case, we find that if the true unemployment rate were 30 percent and we draw samples of 60, about 23 percent of the time we will get an unemployment rate lower than the one among the participants in the program (simply due to random chance). Therefore, we do not reject the null hypothesis (because the p-value is greater than 0.05).\n\n\n\n\n\n\nNote\n\n\n\nIn this scenario, we could say that the program caused a reduction in unemployment, even if the p-value were below .05. This is because we do not know whether the program participants were randomly selected from the population to participate in the program, and there may be other factors at play that influence their employment status. So we are simply testing whether the observed difference is statistically significant, meaning it is unlikely to have occurred by chance alone.\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nWhat if the unemployment rate for the program was only 10%? Would you reject the null hypothesis in this case?\nTry changing the null unemployment rate to 50% and the observed rate to 23%. Simulate the null distribution and decide: do you reject the null?"
  },
  {
    "objectID": "modules/module-3.4.html",
    "href": "modules/module-3.4.html",
    "title": "Module 3.4",
    "section": "",
    "text": "Prework\n\n\n\nComplete the following before diving into this module:\n\nInstall the tidymodels package. This includes the infer and rsample packages, which we will use to simulate sampling and construct confidence intervals.\n\ninstall.packages(\"tidymodels\")\n\n\nInstall the openintro package, which contains the dataset we will use in this module: install.packages(\"openintro\")\n\nReview the documentation for the infer package and the rsample package.\nThink about sampling. What does the word “sample” mean to you? What is a sample you have encountered in your own life? What larger group was that sample meant to represent?"
  },
  {
    "objectID": "modules/module-3.4.html#overview",
    "href": "modules/module-3.4.html#overview",
    "title": "Module 3.4",
    "section": "Overview",
    "text": "Overview\nIn this module, we will explore the concept of statistical inference, which is the process through which we use samples to make informed guesses about a broader population. Since we rarely have access to data from every individual or item in a population, we must rely on samples to estimate quantities of interest, like proportions or averages.\nBut sampling introduces uncertainty. What if your sample isn’t typical? How much might your estimate differ if you sampled again?\nWe’ll build an understanding of what it means to sample from a population, how repeated sampling leads to a sampling distribution, and how to quantify uncertainty using standard errors and confidence intervals. We explore two ways to estimate uncertainty: 1) using mathematical formulas; and 2) using a computational approach called bootstrapping.\nAlong the way, you’ll simulate sampling, compute estimates, and build confidence intervals with R. By the end, you’ll be able to explain what a confidence interval means and construct one from data using the infer package.\nLet’s begin by understanding what sampling is, and why it’s essential to doing data science."
  },
  {
    "objectID": "modules/module-3.4.html#what-is-sampling",
    "href": "modules/module-3.4.html#what-is-sampling",
    "title": "Module 3.4",
    "section": "What is Sampling?",
    "text": "What is Sampling?\nImagine trying to learn something about a large group of people, like how many hours college students sleep each night. You could try asking every student on Earth, but that’s not very practical. Instead, you look at a smaller group that hopefully represents the whole or, in other words, you take a sample.\nThis is the core idea of sampling: selecting a subset of individuals from a larger population to learn something about the population as a whole. The population is the full group you’re interested in studying. The sample is a smaller subset of the population that you actually observe.\nRelated to this, we have two important concepts: a parameter and a statistic. A parameter is a true, but usually unknown, characteristic of the population (like the actual average sleep time). A statistic is the number you compute from your sample (like the sample average sleep time).\nYour goal as a data scientist is to use statistics to make educated guesses about parameters. This process is what we refer to as inference.\nThe Target Population and Sampling Frame\nIn practice, it’s important to define your target population—the group you want to learn about. Then you have to find a sampling frame, which is the actual list or method you use to select your sample.\nFor example:\n\n\nTarget population: All high school students in the U.S.\n\nSampling frame: Students enrolled in a particular school district’s database\n\nOften, the sampling frame doesn’t perfectly match the target population. This mismatch can introduce sampling bias, which we’ll revisit in a future module.\nNow that you understand what sampling is, let’s explore what happens when we repeat the sampling process.\nSampling Distributions and Uncertainty\nIf you take a single sample from a population and compute a statistic—say, the average—what do you get? One answer. But what if you took a different sample? Would your answer be the same? Probably not.\nThis variability is the heart of sampling distributions: the idea that every time you take a sample and compute a statistic, the result can change. If you repeat the process many times, those statistics themselves form a distribution.\n\n\n\n\n\n\nActivity: Sampling with M&Ms\n\n\n\nImagine you have a big bowl of M&Ms in front of you. You reach in and grab 20 at random. Count how many are blue and calculate the proportion. Now, put them all back and grab 20 more. Do it again. And again.\nEach time you grab a handful, you’re taking a sample. Because you’re putting them back in the bowl each time, this is called sampling with replacement. You’ll notice the proportion of blue M&Ms changes slightly from sample to sample.\nTry this at home if you have M&M fun packs where each fun pack represents a random sample of M&Ms. Record the proportion of blue M&Ms (your sample statistic) in a CSV file. This is your sampling distribution. You can even upload your CSV to R and use your data to create a histogram of the proportions.\nNow take an average of all of your recorded sample statistics. How close does it come to the actual known population parameter (the proportion of M&Ms that are blue)?\nThis simple activity mirrors what we mean by repeated sampling and helps you build intuition for how a sampling distribution works."
  },
  {
    "objectID": "modules/module-3.4.html#standard-error-measuring-the-spread",
    "href": "modules/module-3.4.html#standard-error-measuring-the-spread",
    "title": "Module 3.4",
    "section": "Standard Error: Measuring the Spread",
    "text": "Standard Error: Measuring the Spread\nThe spread of this sampling distribution is called the standard error (SE). It tells us, on average, how much a statistic varies from one sample to another. A smaller SE means more precise estimates while a larger SE means more variability from sample to sample.\nStandard errors depend on both the sample size and the variability in the data. Bigger samples tend to produce smaller standard errors.\nUnderstanding this variability is key to making smart inferences. Next, we’ll learn how to use this idea to construct confidence intervals—a powerful way to express uncertainty in our estimates."
  },
  {
    "objectID": "modules/module-3.4.html#estimating-uncertainty-with-confidence-intervals",
    "href": "modules/module-3.4.html#estimating-uncertainty-with-confidence-intervals",
    "title": "Module 3.4",
    "section": "Estimating Uncertainty with Confidence Intervals",
    "text": "Estimating Uncertainty with Confidence Intervals\nA confidence interval gives us a range of values within which we believe the true population parameter likely falls. It’s based on the idea of the sampling distribution and its standard error.\n\nWhen we interpret a confidence interval we say that we are “X% confident” that the true parameter lies within the interval. For example, if we say we are 95% confident that the true proportion of M&Ms that are blue is between 0.2 and 0.3, it means that if we repeated our sampling many times, about 95% of those intervals would contain the true proportion.\nThere are two distinct ways to calculate confidence intervals: using mathematical formulas or through computational methods like bootstrapping. Most of our focus in this course is going to be on the computational approach, or bootstrapping, but it’s important to understand the math behind confidence intervals as well."
  },
  {
    "objectID": "modules/module-3.4.html#math-based-confidence-intervals",
    "href": "modules/module-3.4.html#math-based-confidence-intervals",
    "title": "Module 3.4",
    "section": "Math-Based Confidence Intervals",
    "text": "Math-Based Confidence Intervals\nConfidence intervals can be calculated using formulas based on the sampling distribution of the statistic. The most common type is the normal approximation method, which assumes that the sampling distribution of the sample proportion is approximately normal when the sample size is large enough.\nHere’s the basic formula for a confidence interval:\n\\[\n\\text{Estimate} \\pm z \\times \\text{Standard Error}\n\\]\nThe \\(z\\) value depends on how confident you want to be:\n\nFor a 95% confidence level, \\(z \\approx 1.96\\)\n\nFor a 90% confidence level, \\(z \\approx 1.645\\)\n\n\nExample: Confidence Interval for a Proportion\nSuppose we survey 100 people, and 64 say they like pineapple on pizza. Our sample proportion \\(\\hat{p}\\) is 0.64.\nWe estimate the standard error using:\n\\[\nSE = \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n\\]\nPlugging in the numbers:\n\\[\nSE = \\sqrt{ \\frac{0.64 \\times 0.36}{100} } \\approx 0.048\n\\]\nThe 95% confidence interval is:\n\\[\n0.64 \\pm 1.96 \\times 0.048 = (0.546, 0.734)\n\\]\nWe are 95% confident that between 54.6% and 73.4% of the population likes pineapple on pizza.\nIn the next section, we’ll explore a more flexible approach: bootstrapping."
  },
  {
    "objectID": "modules/module-3.4.html#estimating-uncertainty-with-bootstrapping",
    "href": "modules/module-3.4.html#estimating-uncertainty-with-bootstrapping",
    "title": "Module 3.4",
    "section": "Estimating Uncertainty with Bootstrapping",
    "text": "Estimating Uncertainty with Bootstrapping\nBootstrapping is a resampling technique that involves repeatedly drawing samples, with replacement, from the observed data to estimate the sampling distribution of a statistic. Bootstrapping is an example of a nonparametric approach to inference. Nonparametric means that we do not assume a specific shape or distribution for the population (like normality). Instead of relying on formulas, we use the data we have to approximate what repeated sampling might look like.\n\nBootstrapping is a computational method for estimating the variability of a statistic when you only have one sample from the population. It works by simulating what might happen if you could sample again and again—from your existing data.\nTo understand the idea, imagine that your original sample is the “best guess” we have of the population. If we randomly resample with replacement from that sample, we can simulate what other samples might have looked like.\nBy computing the same statistic (e.g., a proportion or mean) from each resample, we build up a bootstrap distribution. From this, we can estimate standard errors and construct confidence intervals.\nThere are a number of good reasons to use bootstrapping:\n\nIt doesn’t require formulas or assumptions about the shape of the distribution;\nIt works well even with small sample sizes or skewed data;\nIt’s easy to implement using R and the infer package.\n\nWorked Example Using openintro and tidymodels\n\nLet’s use a dataset from the openintro package. In this Pew Research survey, 506 Russians were asked whether they believe their country tried to interfere in the 2016 U.S. presidential election. We’ll recode the responses into a binary variable.\n\n# Load packages\nlibrary(openintro)\nlibrary(tidyverse)\n\n# Load and inspect data\nglimpse(russian_influence_on_us_election_2016)\n\nRows: 506\nColumns: 1\n$ influence_2016 &lt;chr&gt; \"Did not try\", \"Did not try\", \"Did not try\", \"Don't kno…\n\n# Recode as binary variable\nrussiaData &lt;- russian_influence_on_us_election_2016 |&gt; \n  mutate(try_influence = ifelse(influence_2016 == \"Did try\", 1, 0))\n\n# Summary stats\nrussiaData |&gt; \n  summarize(mean = mean(try_influence), sd = sd(try_influence))\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.150 0.358\n\n\nThis tells us the observed proportion who believe Russia tried to influence the election.\nNext, we will create the bootstrap distribution:\n\nlibrary(tidymodels)\n\nset.seed(66)\n\nboot_dist &lt;- russiaData |&gt;\n  specify(response = try_influence) |&gt;\n  generate(reps = 10000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\")\n\nNow, let’s use get_ci() and visualize() from the infer package to compute and visualize the confidence interval for our bootstrap distribution:\n\n# Get confidence interval\nci &lt;- boot_dist |&gt; get_ci(level = 0.95)\n\n# Visualize bootstrap distribution with CI\nboot_dist |&gt;\n  visualize() +\n  shade_ci(ci, color = \"red\", fill = NULL) +\n  labs( \n    title = \"Distribution of the Means of the Bootstrap Samples\",\n    x = \"Mean\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nThis plot shows the bootstrap distribution of the mean proportion of Russians who believe their country interfered in the election. The shaded region marks the 95% confidence interval.\n\n\n\n\n\n\nWarning\n\n\n\nThe 95% confidence interval was calculated as (lower_bound, upper_bound). Which of the following best describes the correct interpretation?\n\n\n95% of the time the percentage of Russians who believe that Russia interfered is between these values.\n\n\n95% of all Russians believe the probability of interference is within the interval.\n\n\nWe are 95% confident that the proportion of Russians who believe that Russia interfered in the 2016 election is between these values.\n\n\nWe are 95% confident that the proportion of Russians who supported interference is between these values.\n\n\n\nCode# The answer is (c) We are 95% confident that the proportion of Russians who believe that Russia interfered in the 2016 election is between these values."
  },
  {
    "objectID": "modules/module-1.4.html",
    "href": "modules/module-1.4.html",
    "title": "Module 1.4",
    "section": "",
    "text": "Prework\n\n\n\n\nMake sure that the Tidyverse is installed. You can do this by running the install.packages(\"tidyverse\") command in the console if you have not done so already.\nFamiliarize yourself with the Tidyverse group of packages.\nWe will not be using all of these, but the first four (ggplot2, dplyr, tidyr, and readr) are essential for this course.\nCreate and save a QMD file for this module in your Module 1 project folder."
  },
  {
    "objectID": "modules/module-1.4.html#overview",
    "href": "modules/module-1.4.html#overview",
    "title": "Module 1.4",
    "section": "Overview",
    "text": "Overview\nData science involves a systematic workflow that takes us from raw data to meaningful insights. This journey typically includes data import, tidying, transformation, visualization, modeling, and communication of results. This is the workflow that we are going to be discussing for the rest of the course.\n\nAt the heart of this workflow and modern data science in R lies the Tidyverse, an ecosystem of packages designed to work harmoniously together. The Tidyverse represents not just a collection of tools but also a philosophy about how data analysis should be approached—with consistency, clarity, and a focus on human readability. To get a better sense of what the it is all about, watch this video of Hadley Wickham, the creator of the Tidyverse, discussing the importance of code maintenance and the evolution of the Tidyverse.\n\nIn this module, you’ll begin your journey with several core Tidyverse packages that form the foundation of data science work. While we will touch on most of the Tidyverse packages in this, course, there are four that are essential for our work:\n\n\nreadr streamlines the process of importing data into R\n\ndplyr provides intuitive verbs for data manipulation and transformation\n\nggplot2 enables creation of beautiful visualizations using the grammar of graphics\n\ntidyr will be helpful when we want to reshape (pivot) our data Today, we are going to get a general sense of the Tidyverse. We will also talk about how to use the readr package to read data into R and a couple of ways to view the contents of the data frame. Then we will cover ggplot2 in the next couple of modules and dplyr and tidyr will come into focus next week when we turn to data wrangling."
  },
  {
    "objectID": "modules/module-1.4.html#working-with-tidyverse-packages",
    "href": "modules/module-1.4.html#working-with-tidyverse-packages",
    "title": "Module 1.4",
    "section": "Working with Tidyverse Packages",
    "text": "Working with Tidyverse Packages\nWhen using Tidyverse packages, you have a few options for how to load and access their functions.\nInitially, we’ll load individual packages as needed. This approach helps you understand which functions come from which packages and allows you to be selective about which parts of the Tidyverse you’re using.\nFor example, to load the ggplot2 package you would go:\n\nlibrary(ggplot2)\n\nAs you become more comfortable with the Tidyverse ecosystem, you might prefer to load all the core packages at once:\n\nlibrary(tidyverse)\n\nThis command loads all of the core Tidyverse packages, including readr, ggplot2, dplyr, tidyr, and others.\nSometimes you might see code where the author loads a package using its namespace by using the :: operator, like this:\n\nggplot2::ggplot(data, aes(x = variable1, y = variable2))\n\nThis will not be as common a workflow for us, but it is helpful in context where you want to use a function from a package without loading the entire package. This can be helpful when putting code into production or writing packages because it helps to avoid conflicts between functions and minimize resources."
  },
  {
    "objectID": "modules/module-1.4.html#reading-data-into-r",
    "href": "modules/module-1.4.html#reading-data-into-r",
    "title": "Module 1.4",
    "section": "Reading Data into R",
    "text": "Reading Data into R\nThe first step in most data analysis projects is importing your data. The readr package makes this process straightforward and efficient, especially when working with CSV files, which are one of the most common data formats.\nLet’s start by loading the readr package and importing a dataset about democracy measures around the world. Download this data set and move it in your project folder as dem_summary.csv:\n📥 Download dem_summary.csv\nA best practice is to save your data files in a subfolder within your project directory. Usually we would call that folder “data” and reference the file as data/name_of_file.csv. This keeps your workspace organized and makes it easier to find your data files later.\nNow try reading in the data and storing it in an object using the read_csv() function from the readr package like this:\n\nlibrary(readr)\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")\n\nWhen you run this code, readr will display a message showing how it interpreted each column (e.g., as character, numeric, etc.). This is helpful for quickly identifying if any columns were parsed incorrectly.\n\n\n\n\n\n\nNote\n\n\n\nWe could also have used the base R read function read.csv() to read in the data, but read_csv() is generally faster and more efficient. It also has a number of advantages over the base R function. For example, it automatically handles column types, so you don’t have to specify them manually. read_csv() also provides better handling of missing values and other common issues that can arise when importing data."
  },
  {
    "objectID": "modules/module-1.4.html#viewing-the-data",
    "href": "modules/module-1.4.html#viewing-the-data",
    "title": "Module 1.4",
    "section": "Viewing the Data",
    "text": "Viewing the Data\nOnce you’ve imported your data, it’s important to take a look at it to ensure everything looks correct. One way to do this is to type View() in your console or (equivalently) click on the name of the object in your Environment tab to see the data in a spreadsheet:\n\n\n\n\nThis will open a new tab in RStudio with a spreadsheet-like view of your data frame. This is a great way to get a quick overview of your data.\nThe glimpse() function from the dplyr package is another great way to get an overview of your data frame. It provides a quick summary of the data, including the number of rows and columns, the names of the columns, and the data types of each column.\n\nlibrary(dplyr)\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")\n\nglimpse(dem_summary)\n\nRows: 6\nColumns: 5\n$ region    &lt;chr&gt; \"The West\", \"Latin America\", \"Eastern Europe\", \"Asia\", \"Afri…\n$ polyarchy &lt;dbl&gt; 0.8709230, 0.6371358, 0.5387451, 0.4076602, 0.3934166, 0.245…\n$ gdp_pc    &lt;dbl&gt; 37.913054, 9.610284, 12.176554, 9.746391, 4.410484, 21.134319\n$ flfp      &lt;dbl&gt; 52.99082, 48.12645, 50.45894, 50.32171, 56.69530, 26.57872\n$ women_rep &lt;dbl&gt; 28.12921, 21.32548, 17.99728, 14.45225, 17.44296, 10.21568\n\n\nHere we see that the data frame consists of five columns: region; polyarchy (a measure of democracy); GDP per capita (gdp_pc); female labor force participation rates (flfp); and levels of women’s representation (women_rep). We also see that region is a character variable and the other four columns are numeric variables. Finally, we see that there are six rows in the data frame representing the average values of these variables for each region in the world.\nNote that there are some other base R functions that can be useful for viewing data frames. You can use the head() function to view the first few rows of a data frame:\n\nhead(dem_summary)\n\n# A tibble: 6 × 5\n  region         polyarchy gdp_pc  flfp women_rep\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 The West           0.871  37.9   53.0      28.1\n2 Latin America      0.637   9.61  48.1      21.3\n3 Eastern Europe     0.539  12.2   50.5      18.0\n4 Asia               0.408   9.75  50.3      14.5\n5 Africa             0.393   4.41  56.7      17.4\n6 Middle East        0.246  21.1   26.6      10.2\n\n\nYou can also use the tail() function to view the last few rows of a data frame:\n\ntail(dem_summary)\n\n# A tibble: 6 × 5\n  region         polyarchy gdp_pc  flfp women_rep\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 The West           0.871  37.9   53.0      28.1\n2 Latin America      0.637   9.61  48.1      21.3\n3 Eastern Europe     0.539  12.2   50.5      18.0\n4 Asia               0.408   9.75  50.3      14.5\n5 Africa             0.393   4.41  56.7      17.4\n6 Middle East        0.246  21.1   26.6      10.2\n\n\nAnd the summary() function to get a summary of the data frame, including the minimum, maximum, mean, and median values for each column:\n\nsummary(dem_summary)\n\n    region            polyarchy          gdp_pc            flfp      \n Length:6           Min.   :0.2459   Min.   : 4.410   Min.   :26.58  \n Class :character   1st Qu.:0.3970   1st Qu.: 9.644   1st Qu.:48.68  \n Mode  :character   Median :0.4732   Median :10.961   Median :50.39  \n                    Mean   :0.5156   Mean   :15.832   Mean   :47.53  \n                    3rd Qu.:0.6125   3rd Qu.:18.895   3rd Qu.:52.36  \n                    Max.   :0.8709   Max.   :37.913   Max.   :56.70  \n   women_rep    \n Min.   :10.22  \n 1st Qu.:15.20  \n Median :17.72  \n Mean   :18.26  \n 3rd Qu.:20.49  \n Max.   :28.13  \n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nMake sure that you are able to do all of the above-mentioned steps in this lesson, e.g. download the dem_summary.csv file, read it into R and store it in an object, view the data frame, use the glimpse() function to summarize it, and use the head(), tail(), and summary() functions.\nRepeat the steps above using a different data frame. Download the dem_women.csv file and store it in an object called dem_women or some name that is reflective of its content.\nIn your Quarto document, below your code chunk, write a few short paragraphs describing the data frame.\n\nWhat are the data frame’s dimensions (e.g. how many rows and columns does it have)?\nWhat are the names of the columns? What do they represent and what are the data types?\nWhat are the first few rows of the data frame? The last few rows?\nWhat do the rows represent? How are these data different from the the data in the dem_summary data frame? (Hint: there are more rows in this data frame.)\nTake one or two columns and describe the summary statistics. What do these summary statistics represent? (Hint: there is a temporal dimension to consider.)"
  },
  {
    "objectID": "modules/module-3.3.html",
    "href": "modules/module-3.3.html",
    "title": "Module 3.3",
    "section": "",
    "text": "Prework\n\n\n\nClick on Code toggle below to unfold the setup code chunk. Then, copy and run the code in your Quarto notebook to load the necessary packages and create the data frame for this lesson.\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\nvdem2022 &lt;- fetchdem(indicators = c(\n    \"v2x_polyarchy\",\n    \"v2x_gender\",\n    \"v2cacamps\",\n    \"v2x_regime\",\n    \"e_regionpol_6C\"\n    ),\n    start_year = 2022, \n    end_year = 2022) |&gt;\n  rename(\n    country = country_name, \n    polyarchy = v2x_polyarchy, \n    women_empowerment = v2x_gender,\n    polarization = v2cacamps,\n    regime = v2x_regime, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, \n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\"),\n    regime = case_match(regime,\n                    0 ~ \"Closed Autocracy\",\n                    1 ~ \"Electoral Autocracy\",\n                    2 ~ \"Electoral Democracy\",\n                    3 ~ \"Liberal Democracy\")\n  )\n\n#glimpse(vdem2022)"
  },
  {
    "objectID": "modules/module-3.3.html#overview",
    "href": "modules/module-3.3.html#overview",
    "title": "Module 3.3",
    "section": "Overview",
    "text": "Overview\nIn this module, we go beyond the shape of a distribution to describe it more precisely using summary statistics. We begin by exploring measures of central tendency, such as the mean and median, which tell us where the center of a distribution lies. Then we turn to measures of spread, which help us understand how tightly or loosely the values are clustered around that center.\nWe’ll learn how to calculate and interpret the range, interquartile range (IQR), variance, and standard deviation, and see how each tells a different part of the story about our data. Along the way, we’ll visualize these concepts using histograms, box plots, and density plots. We’ll also compare distributions across groups, using summary statistics and ridge plots to uncover patterns that aren’t visible from center alone.\nBy the end of this module, you’ll have a well-rounded set of tools for describing and comparing continuous variables — and a better understanding of when the mean can be misleading, and why spread matters just as much as center."
  },
  {
    "objectID": "modules/module-3.3.html#measures-of-central-tendency",
    "href": "modules/module-3.3.html#measures-of-central-tendency",
    "title": "Module 3.3",
    "section": "Measures of Central Tendency",
    "text": "Measures of Central Tendency\nWhen we work with continuous variables, one of our first goals is to describe the center of the data — a typical or representative value that captures where most observations tend to fall. This is what we mean by measures of central tendency.\nThe two most common measures are the mean and the median:\n\nThe mean is the arithmetic average: add up all the values and divide by the number of observations.\nThe median is the middle value: half the observations fall below it, and half above.\n\nWe already know how to calculate both of these using the summarize() function from the dplyr package. Here is an example using the data frame that we created in the setup chunk:\n\nvdem2022 |&gt; \n  summarize(\n    mean_polarization = mean(polarization, na.rm = TRUE),\n    median_polarization = median(polarization, na.rm = TRUE)\n  )\n\n  mean_polarization median_polarization\n1         0.2347416              0.2265\n\n\nIn some datasets, the mean and median will be very close. But in others — particularly those with skewed distributions — they can diverge. The mean is pulled in the direction of extreme values, while the median resists the influence of outliers.\nThat’s why the mean, though widely used, is not always the most informative measure of central tendency. Its usefulness depends on the shape of the distribution. In a symmetric distribution, the mean and median will align. But in a skewed distribution — like GDP per capita, where a few wealthy countries drive up the average — the median may offer a better sense of the “typical” case.\n\n\n\n\n\n\nWhat About the Mode?\n\n\n\nThe mode is another measure of central tendency — it refers to the most frequently occurring value in a dataset. While it can be useful for categorical or discrete data (e.g., identifying the most common regime type or income bracket), it is rarely used with continuous variables. That’s because continuous data often don’t have exact repeat values, especially when measured with precision (e.g., GDP per capita like $10,542.87).\n\n\nLet’s look at two examples: one symmetric, one skewed. We’ll overlay vertical lines for the mean and median so you can see how they behave in each case.\n\n\n\n\n\n\n\n\nIn a symmetric distribution, the mean and median are nearly identical. In this example, both are around 50. This is why the mean is often a reliable summary when data are normally distributed.\n\n\n\n\n\n\n\n\nIn the right-skewed case, we can see that the mean is pulled to the right by a few large values, while the median stays closer to the bulk of the data. This is why the median can sometimes offer a more realistic picture of central tendency, especially when distributions are skewed.\nThe lesson here is to always look at how your data are distributed before analyzing them. When reading or in a presentation, you should ask yourself whether the mean make sense given the distribution of the measure. Could extreme values in a skewed distribution make the mean not as useful? Have the analysts shown you the distribution? If not, ask about it!\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nCalculate the mean and median for the women_empowerment variable in the vdem2022 data frame.\nNow overlay the mean and median on a density plot of women_empowerment using geom_vline().\nWhat do you notice about the relationship between the mean and median in this case?\nTry the same visualization with the polarization variable. Are the mean and median for polarization close together or far apart? How does this distance compare to the women_empowerment variable?"
  },
  {
    "objectID": "modules/module-3.3.html#measures-of-dispersion",
    "href": "modules/module-3.3.html#measures-of-dispersion",
    "title": "Module 3.3",
    "section": "Measures of Dispersion",
    "text": "Measures of Dispersion\nWe have seen how an important thing that we want to know about our data is how much variability there is, e.g. how tightly or loosely the values are clustered around the center. This variability is often referred to as the spread of the distribution.\nWhy should we be concerned with spread? Let’s start by comparing two distributions. Both have the same mean — zero — but one has values tightly clustered around that mean, while the other spreads out much more broadly.\n\n\n\n\n\n\n\n\nEven without doing any math, we can see that the second distribution is more dispersed. But to describe and compare distributions in a consistent way, we need to summarize that spread with numbers.\nRange: A Starting Point\nOne of the simplest ways to measure spread is the range, which is just the difference between the minimum and maximum values.\n\nvdem2022 |&gt;\n  summarize(min = min(polarization, na.rm = TRUE),\n            max = max(polarization, na.rm = TRUE))\n\n     min   max\n1 -3.115 3.538\n\n\nWhile easy to calculate, the range only tells us about the extremes. It doesn’t tell us where most values lie, and it’s highly sensitive to outliers. We need something more robust.\nThe Interquartile Range (IQR)\nA more useful measure of spread is the interquartile range (IQR) — the range of the middle 50% of the data. It spans from the 25th percentile (Q1) to the 75th percentile (Q3).\n\n\n\n\n\n\n\n\nThis gives us a much better sense of where the “typical” values are. In this example, most countries had political polarization scores between -0.73 and 1.15 in 2022.\n\nvdem2022 |&gt;\n  summarize(\n    IQRlow =  quantile(polarization, .25, na.rm = TRUE),\n    IQRhigh = quantile(polarization, .75, na.rm = TRUE),\n    IQRlength = IQR(polarization, na.rm = TRUE)\n  )\n\n   IQRlow IQRhigh IQRlength\n1 -0.7265 1.14975   1.87625\n\n\nVisualizing the IQR with a Box Plot\nThe box plot is a powerful tool for visually summarizing the spread of a distribution. It provides a standardized way to display key features of a dataset using what’s known as the five-number summary: the minimum, first quartile (Q1), median, third quartile (Q3), and maximum. In R we can make a boxplot with the geom_boxplot() function from the ggplot2 package.\n\nggplot(vdem2022, aes(x = \"\", y = polarization)) +\n  geom_boxplot(fill = \"steelblue\") + \n   labs(\n    x = \"\", \n    y = \"Polarization\", \n    title = \"Distribution of Political Polarization Scores, 2022\", \n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nAt its core, a box plot helps us quickly see where most values lie and how they are distributed across the range. The “box” itself shows the interquartile range — the middle 50% of the data — while the line inside the box marks the median. The “whiskers” extend to the smallest and largest values that fall within 1.5 times the interquartile range, and any values beyond that are plotted individually as potential outliers.\nThis makes box plots especially useful for comparing distributions across different groups. They allow us to see not just differences in center (like the median), but also differences in spread, skewness, and the presence of outliers.\nStandard Deviation: The Classic Measure of Spread\nThe standard deviation is a widely used summary of spread. It tells us, on average, how far each observation lies from the mean. A small standard deviation means values are tightly clustered; a large one means they are more spread out. Here is an example of how to calculate the mean and standard deviation for the polarization variable in the vdem2022 data frame:\n\nvdem2022 |&gt;\n  summarize(mean = mean(polarization, na.rm = TRUE),\n            stdDev = sd(polarization, na.rm = TRUE))\n\n       mean   stdDev\n1 0.2347416 1.408674\n\n\nStandard deviation is derived from the variance, which is the average of the squared deviations from the mean. The standard deviation is simply the square root of the variance — bringing the result back to the original scale of the data.\nUnder the hood, R is calculating the standard deviation with the following formula:\n\\[\ns = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2}\n\\]\nA Step-by-Step Breakdown of Standard Deviation\nTo better understand how standard deviation works, let’s walk through a toy example by breaking down the formula. We’ll follow the standard formula for sample standard deviation:\n\\[\ns = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2}\n\\]\nThis formula tells us to:\n\nSubtract the mean from each value (to get deviations),\nSquare those deviations,\nSum them,\nDivide by \\(n - 1\\),\nAnd take the square root.\n\nLet’s try this using a simple vector of evenly spaced numbers from 0 to 10.\n\n\n\n\n\n\nImportant\n\n\n\nPlay the interactive code chunks below to see how each step works. You can also change the numbers in the initial vector to see how the standard deviation changes with different data.\n\n\nFirst, we create the vector. Next, we calculate the mean (\\(\\bar{X}\\)) of the dataset and subtract it from each data point (\\(X_i\\)) to calculate its deviation from the mean: \\(e_i = X_i - \\bar{X}\\). We store that vector of deviations in a new variable called e:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow we square each deviation: \\(e_i^2 = (X_i - \\bar{X})^2\\). This removes negative signs and prepares the values for averaging:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNext, we sum up all of the squared deviations: \\(\\sum_{i=1}^{n} (X_i - \\bar{X})^2\\). This represents the total squared deviation from the mean or the sum of squares.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nDivide the total squared deviation by \\((n-1)\\) get the sample variance: \\(\\text{Variance} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\). Using \\((n-1)\\) ensures an unbiased estimate of the population variance when calculating from a sample.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFinally, we take the square root of the variance to get the standard deviation: \\(s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2}\\). Taking the square root converts the variance back to the units of the original data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow let’s compare our manual calculation with R’s built-in sd() function:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nYour Turn\n\n\n\n\nCalculate the range and interquartile range (IQR) for the women_empowerment variable in the vdem2022 data frame.\nCreate a box plot for women_empowerment and overlay the mean and median.\nCalculate the mean and standard deviation for women_empowerment and polarization in the vdem2022 data frame using R’s built-in sd() function.\nCompare your results with what we found for the polarization variable earlier in this module. What do you notice about the spread of women_empowerment compared to polarization?"
  },
  {
    "objectID": "modules/module-3.2.html",
    "href": "modules/module-3.2.html",
    "title": "Module 3.2",
    "section": "",
    "text": "Prework\n\n\n\nClick on Code toggle below to unfold the setup code chunk. Then, copy and run the code in your Quarto notebook to load the necessary packages and create the data frame for this lesson.\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\nvdem2022 &lt;- fetchdem(indicators = c(\n    \"v2x_polyarchy\",\n    \"v2x_gender\",\n    \"v2cacamps\",\n    \"v2x_regime\",\n    \"e_regionpol_6C\"\n    ),\n    start_year = 2022, \n    end_year = 2022) |&gt;\n  rename(\n    country = country_name, \n    polyarchy = v2x_polyarchy, \n    women_empowerment = v2x_gender,\n    polarization = v2cacamps,\n    regime = v2x_regime, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, \n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\"),\n    regime = case_match(regime,\n                    0 ~ \"Closed Autocracy\",\n                    1 ~ \"Electoral Autocracy\",\n                    2 ~ \"Electoral Democracy\",\n                    3 ~ \"Liberal Democracy\")\n  )\n\n#glimpse(vdem2022)"
  },
  {
    "objectID": "modules/module-3.2.html#overview",
    "href": "modules/module-3.2.html#overview",
    "title": "Module 3.2",
    "section": "Overview",
    "text": "Overview\nIn our last module, we explored tools available for examining categorical data—variables that capture groupings or labels, like regime type or world region. We looked at bar charts and how grouping by a categorical variable can help us uncover patterns in data. In this module, we turn our attention to continuous data.\nContinuous variables are numeric measurements that can take on an infinite range of values within a given interval. Think of indicators like GDP per capita, population size, or life expectancy—these are variables that allow us to compare magnitude, observe variation, and investigate relationships between quantities.\nIn this lesson, we will learn how to explore continuous variables. We will look at different types of distributions and learn how to visualize a single continuous variable with histograms and density plots. Then we will look at how to compare distributions across groups using ridge plots."
  },
  {
    "objectID": "modules/module-3.2.html#what-does-the-distribution-look-like",
    "href": "modules/module-3.2.html#what-does-the-distribution-look-like",
    "title": "Module 3.2",
    "section": "What Does the Distribution Look Like?",
    "text": "What Does the Distribution Look Like?\nTo understand continuous variables, we often begin by examining their distribution through a visual representation of how values are spread out across the range with a histogram or a density plot. These shapes tell us a lot about the nature of the data and can guide our decisions about how to summarize or transform variables.\nLet’s look at some histograms displaying common types of distributions you’re likely to encounter. A histogram shows how many observations fall into different ranges of values, allowing us to see patterns like skewness, modality, and clustering. They do this by grouping values into bins and counting how many observations fall into each one. Think of it as slicing up the number line into segments and stacking up bars based on how many countries fall into each slice.\nSymmetric and Bell-Shaped\nWhen most values are clustered around the center, with fewer values tapering off evenly on both sides, we call the distribution symmetric or bell-shaped. This kind of distribution is common in physical measurements and standardized test scores. It’s also the foundation of many statistical techniques that assume normality.\n\n\n\n\n\n\n\n\nRight-Skewed\nA right-skewed distribution (also known as positively skewed) has a long tail stretching to the right. This often occurs when values are bounded at zero but can stretch very far in the positive direction. GDP per capita is a classic example: most countries are clustered at the lower end, with a few very wealthy outliers pulling the tail to the right.\n\n\n\n\n\n\n\n\nLeft-Skewed\nLess common, but still important, are left-skewed (negatively skewed) distributions, where the tail stretches to the left. This might happen with variables that have an upper bound, like survey responses with a maximum score, where a majority of responses are at the top end but a few fall below.\n\n\n\n\n\n\n\n\nBimodal\nA bimodal distribution has two peaks — two distinct groups of values. This often signals that your data may actually come from two different populations. For example, if you combine data on voter turnout from democratic and authoritarian regimes, you might see one peak for each group.\n\n\n\n\n\n\n\n\nUniform\nA uniform distribution has no peaks or valleys — all values are equally likely. This is relatively rare in real-world data but can occur in random sampling or when measuring something that has been evenly distributed across a range.\n\n\n\n\n\n\n\n\nEach of these shapes tells a different story about how values are distributed — and that story helps us decide how to summarize the variable. For example, when a distribution is symmetric, the mean and median are usually close together. But in a skewed distribution, the mean gets pulled toward the long tail, making it a less reliable summary on its own."
  },
  {
    "objectID": "modules/module-3.2.html#creating-a-histogram",
    "href": "modules/module-3.2.html#creating-a-histogram",
    "title": "Module 3.2",
    "section": "Creating a Histogram",
    "text": "Creating a Histogram\nNow that we have been exposed to a number of different types of distributions, lets practice making histograms so that we can explore continous data on our own. To make a histogram in ggplot2, we use the geom_histogram() function. This function takes an aesthetic mapping (aes()) that specifies which variable to plot on the x-axis, and it automatically counts how many observations fall into each bin.\nLet’s use the vdem2022 data frame that we created in the setup chunk to visualize the distribution of the V-Dem polyarchy scores for 2022:\n\nggplot(vdem2022, aes(x = polyarchy)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\") +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of V-Dem Polyarchy Scores, 2022\",\n    x = \"Polyarchy Score\",\n    y = \"Frequency\"\n  )\n\n\n\n\n\n\n\nWhen we visualize polyarchy, we see that it is non-normal. The distribution appears to be multi-modal or perhaps even, with peaks occurring in multiple places. It is somewhat hard to tell what is going on with the data here."
  },
  {
    "objectID": "modules/module-3.2.html#visualizing-distributions-with-density-plots",
    "href": "modules/module-3.2.html#visualizing-distributions-with-density-plots",
    "title": "Module 3.2",
    "section": "Visualizing Distributions with Density Plots",
    "text": "Visualizing Distributions with Density Plots\nSometimes it can be easier to see what is happening with the distribution if we look at a density plot instead of a histogram. Density plots are similar in spirit to histograms but use a smoothed curve to estimate the shape of the distribution. Rather than count how many values fall into each bin, density plots estimate how likely it is to see values in different parts of the range.\nThey are particularly helpful when you want to compare distributions across groups or highlight subtler patterns that might be obscured by binning choices in a histogram. To create a density plot with ggplot2 we use the geom_density() function. This function also takes an aesthetic mapping (aes()) that specifies which variable to plot on the x-axis, and it automatically estimates the density of values across the range.\nHere’s how you could create a density plot of the polyarchy score:\n\nggplot(vdem2022, aes(x = polyarchy)) +\n  geom_density(fill = \"steelblue\", alpha = 0.6) +\n  theme_minimal() +\n  labs(\n    title = \"Smoothed Distribution of Polyarchy Scores, 2022\",\n    x = \"Polyarchy Score\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\nHere it looks like we have two distinct peaks in the distribution, which suggests a that there are two groups of countries–one concentrating around a low polyarchy score and another around a high polyarchy score. In other words, the distribution appears to be bimodal, with one peak occuring at a polyarchy score of 0.2 and another around 0.8.\nBecause density plots are continuous, they’re often more elegant for comparison across groups—especially when you overlay multiple distributions on the same plot. To do that, we can add a fill aesthetic inside aes() to color the density curves by region:\n\nggplot(vdem2022, aes(x = polyarchy, fill = region)) +\n  geom_density(alpha = 0.6) +\n  theme_minimal() +\n  labs(\n    title = \"Smoothed Distribution of Polyarchy Scores, 2022\",\n    x = \"GDP per Capita\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that if you use the fill aesthetic, you should remove the fill = argument from geom_density(). However, you should still set the alpha parameter to make the colors semi-transparent. This allows you to see overlapping areas more clearly.\n\n\nAn even better solution for this is a ridge plot, which is a type of density plot that displays multiple distributions stacked on top of each other. This allows us to see how the distributions compare across groups more clearly. To create a ridge plot, we can use the geom_density_ridges() function from the ggridges package:\n\nlibrary(ggridges)\n  ggplot(vdem2022, aes(x = polyarchy, y = region, fill = region)) +\n    geom_density_ridges() +\n  labs(\n    x = \"Electoral Democracy\",\n    y = \"Region\",\n    title = \"A Ridge Plot\",\n    caption = \"Source: V-Dem Institute\",\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nNow we can really see what is going on with these data. Each region has an almost unique distribution: the Middle East and Africa appear to be right-skewed; Latin America is left skewed; and the West is normally distributed. The distribution of democracy scores in other regions appears to be multimodal or unimodal. This is a very different conclusion that we would have reached had we just looked at a simple histogram!\n\n\n\n\n\n\nYour Turn!!\n\n\n\nLet’s practice visualizing a continuous variable with both histograms and density plots.\nUse the vdem2022 dataset to do the following:\n\nCreate a histogram of the women_empowerment variable. Try adjusting the number of bins to see how it affects the visualization.\nNow create a density plot of the women_empowerment variable. What do you notice about the shape? Does it make the distribution clearer than the histogram?\nTry adding a fill = region aesthetic inside aes() to visualize the distribution of population by region. What patterns do you see? How does the distribution of women’s empowerment vary across regions?\nNow try the above steps with the polarization variable. What does the distribution look like? How does it compare to the other variables?"
  },
  {
    "objectID": "modules/module-1.3.html",
    "href": "modules/module-1.3.html",
    "title": "Module 1.3",
    "section": "",
    "text": "Prework\n\n\n\n\nYou should have a project folder for Module 1 that you created in the last lesson. In it, create a Quarto document called module-1.3.qmd and use it to take notes and do the exercises in the module."
  },
  {
    "objectID": "modules/module-1.3.html#overview",
    "href": "modules/module-1.3.html#overview",
    "title": "Module 1.3",
    "section": "Overview",
    "text": "Overview\nThis module introduces the core concepts of programming in R. You will learn how R can be used as a calculator, how to store data in objects, and how to interact with R through both the console and code chunks in Quarto. We want to build comfort with R syntax and programming logic so you’re well-prepared for more advanced topics like data wrangling and visualization in upcoming modules."
  },
  {
    "objectID": "modules/module-1.3.html#what-can-r-do",
    "href": "modules/module-1.3.html#what-can-r-do",
    "title": "Module 1.3",
    "section": "What Can R Do?",
    "text": "What Can R Do?\nR is a versatile programming language renowned for its strengths in data analysis and visualization, making it a favorite among statisticians and data scientists. Beyond these core capabilities, R functions as a general-purpose language that supports a wide range of tasks—from building web applications to developing machine learning models. Its open-source nature ensures that it is freely available and constantly evolving, thanks to a vibrant and active community of contributors who expand its functionality through packages and collaborative development."
  },
  {
    "objectID": "modules/module-1.3.html#using-r-as-a-calculator",
    "href": "modules/module-1.3.html#using-r-as-a-calculator",
    "title": "Module 1.3",
    "section": "Using R as a Calculator",
    "text": "Using R as a Calculator\nR handles basic arithmetic with ease. You can enter simple expressions in the console, such as:\n\n2 + 2\n\n[1] 4\n\n\nAnd you can perform many other operations, like subtraction, multiplication, and division. Here are some common arithmetic operators in R:\n\n\n+ addition\n\n- subtraction\n\n* multiplication\n\n/ division\n\n^ exponentiation (also **)"
  },
  {
    "objectID": "modules/module-1.3.html#objects-in-r",
    "href": "modules/module-1.3.html#objects-in-r",
    "title": "Module 1.3",
    "section": "Objects in R",
    "text": "Objects in R\nIn R, everything is an object. An object is a named container that stores data or functions and associated metadata. Objects can hold simple data like numbers or more complex structures like vectors, lists, or data frames.\nYou create an object using the assignment operator &lt;-. To take a simple example from earlier in this module, we can take the sum of 2 and 2 and assign it to an object called sum_of_2plus2:\n\nsum_of_2plus2 &lt;- 2 + 2\n\nNow that we have the object created we can do lots of things with it. We could simply print what is in the object by calling it (printing it out) in another code chunk:\n\nsum_of_2plus2\n\n[1] 4\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the console, you need to use print() to display the contents of an object. In Quarto, you can just type the name of the object and it will print it out for you.\n\n\nOr we could use it in another calculation. For example, we could multiply it by 2:\n\nsum_of_2plus2 * 2\n\n[1] 8\n\n\nWe could then assign that to another object:\n\nsum_of_2plus2_times_2 &lt;- sum_of_2plus2 * 2\n\nAnd so on… But we can store lots of things in objects, not just 2 + 2. For example, we can store a string in an object. Let’s store the string “Hello, world!” in an object called my_string and then print it out:\n\nmy_string &lt;- \"Hello, world!\"\n\nmy_string\n\n[1] \"Hello, world!\"\n\n\nSometimes you want to store more than one number. In this case you can store a vector. A vector is a collection of numbers or characters. You can create a vector using the c() function, which stands for “combine.” For example, to create a vector of numbers from 1 to 5, you would do:\n\nmy_vector &lt;- c(1, 2, 3, 4, 5)\n\nmy_vector\n\n[1] 1 2 3 4 5\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nIn your Quarto document, create a vector of numbers from 1 to 10 and assign it to an object called my_vector.\nPrint out the object.\nCreate a new object called my_vector_squared that is equal to my_vector squared and print it out.\nCreate a new object with a string in it called another_string and print it out."
  },
  {
    "objectID": "modules/module-1.3.html#functions",
    "href": "modules/module-1.3.html#functions",
    "title": "Module 1.3",
    "section": "Functions",
    "text": "Functions\nA function is a set of instructions that produces some output. In R, you can use built-in functions to perform specific tasks. For example, you can use the mean() function to calculate the average of a set of numbers. Let’s take the mean of the vector that we created earlier:\n\nmean(my_vector)\n\n[1] 3\n\n\nHere we see that the mean of c(1, 2, 3, 4, 5) is 3. Some common functions in R include:\n\n\nmean() calculates the mean of a set of numbers\n\nmedian() calculates the median of a set of numbers\n\nsd() calculates the standard deviation of a set of numbers\n\nsum() calculates the sum of a set of numbers\n\nlength() calculates the length of a vector\n\nmax() and min() calculate the maximum and minimum values of a vector\n\nround() rounds a number to a specified number of decimal places\n\nsqrt() calculates the square root of a number\n\nlog() calculates the natural logarithm of a number\n\nexp() calculates the exponential of a number\n\nabs() calculates the absolute value of a number\n\nYou can even create your own functions in R. For example, we could create a function that takes a number and returns its square:\n\nsquare &lt;- function(x) {\n  x^2\n}\n\nThen you can use this function to square any number:\n\nsquare(2)\n\n[1] 4\n\n\nCreating functions is somewhat of an advanced topic, but it is a very useful one. You can use functions for all sorts of things, including data wrangling and visualization.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nCreate a vector of numbers from 1 to 20 and assign it to an object called another_vector.\nPrint out the object.\nNow apply the mean() function to another_vector and print out the result.\nNext try some of the other functions listed earlier in the module on another_vector and print out the results.\nFinally, create a function that takes a number and returns its cube. Call it cube(). Use it to cube the number 3 and print out the result."
  },
  {
    "objectID": "modules/module-1.3.html#data-frames",
    "href": "modules/module-1.3.html#data-frames",
    "title": "Module 1.3",
    "section": "Data Frames",
    "text": "Data Frames\n\nA data frame is a two-dimensional table-like structure that can hold different types of data. Each column can contain different types of data (e.g., numeric, character, factor), and each row represents an observation. Data frames are one of the most common data structures in R and are used to store and manipulate datasets.\nTo create a data frame, you can use the data.frame() function. For example, let’s create a simple data frame with two columns: x and y:\n\nmy_data &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"),\n  age = c(25, 30, 35, 40, 45),\n  height = c(5.5, 6.0, 5.8, 5.9, 5.7),\n  starwars_fan = c(TRUE, FALSE, TRUE, FALSE, TRUE)\n)\n\nHere we have a data frame with 5 rows and 4 columns. The first column is a character vector (name), the second (age) and third (height) columns are numeric vectors, and the last column (starwars_fan) is a logical vector. You can print out the data frame by simply typing its name:\n\nmy_data\n\n     name age height starwars_fan\n1   Alice  25    5.5         TRUE\n2     Bob  30    6.0        FALSE\n3 Charlie  35    5.8         TRUE\n4   David  40    5.9        FALSE\n5     Eve  45    5.7         TRUE\n\n\nNow you can access individual columns or rows of the data frame using the $ operator or by using indexing. For example, to access the age column, you can do:\n\nmy_data$age\n\n[1] 25 30 35 40 45\n\n\nYou may also see people using indexing to access columns. For example, you can access the first column of the data frame using:\n\nmy_data[, 1]\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"   \"Eve\"    \n\n\nWe will learn a lot more about manipulating data frames in subsequent modules when we talk about the dplyr package. For now, just know that data frames are a powerful and flexible way to store and manipulate data in R.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nCreate an original data frame with 5 rows and 3 columns. The first column should be a character vector, the second should be a numeric vector, and the third should be a logical vector. Call it my_data_2 and print it out.\nUse the $ operator to access the second column of my_data_2 and print it out.\nUse indexing to access the third row of my_data_2 and print it out."
  },
  {
    "objectID": "modules/module-2.6.html",
    "href": "modules/module-2.6.html",
    "title": "Module 2.6",
    "section": "",
    "text": "Prework\n\n\n\n\nStart a QMD file for this module. (At this point, I assume you know how to create a QMD file in a project and set the YAML header. So this will be the last time I mention it.)\nInstall the janitor package and have a look at the documentation."
  },
  {
    "objectID": "modules/module-2.6.html#overview",
    "href": "modules/module-2.6.html#overview",
    "title": "Module 2.6",
    "section": "Overview",
    "text": "Overview\nIn this module we are going to focus on how to clean up messy real world data. We are going to do this using data downloaded from the World Development Indicators. This will be our third iteration of working with World Bank data, so if you are a budding economist you are in luck! The first time we encountered World Bank data was through the WDI package where the data were tidy just by virtue of being accessed through the API. In the last module, we left off with a worked example where we used the tidyr version of some untidy World Bank’s World Development Indicators (WDI) dataset and transposed it to make it into a tidy dataset. In this lesson, we are going to download data directly from the WDI interface that is not only untidy but also contains some other issues that we will need to address."
  },
  {
    "objectID": "modules/module-2.6.html#downloading-and-reading-in-the-data",
    "href": "modules/module-2.6.html#downloading-and-reading-in-the-data",
    "title": "Module 2.6",
    "section": "Downloading and Reading in the Data",
    "text": "Downloading and Reading in the Data\n\nGo to the World Development Indicators portal at the World Bank’s Data Bank.\nUnder Countries, select the Countries tab and then select the little check mark ☑️ to select all of the countries. Be sure to select the Countries tab first, though, or you will also be downloading aggregate data for regions and groups of countries.\nNext, under Series, search for “labor force participation” and find labor force participation rates for women ages 15-64 (ILO modeled estimates). Check that series.\nNow go to Time and select the years from the last 50 years. Click Apply Changes, go to Download Options and download as a .csv file. Place the .csv file in the data directory that you created for this module. Save it as “messy_wb_data.csv” or something like that.\nNow we are going to read this messy World Bank data into R using the read_csv() function from the readr package.After we have read the data into R, we are going to have a look at it with glimpse().\n\n\n\n\n\n\nDid You Know?\n\n\n\nWhile comma delimited files are the most common kind of flat file, readr includes functions for parsing files with a wide range of delimiters including tabs (read_tsv()), semicolons (read_csv2()) and white spaces (read_table()). There is also a Tidyverse package for reading in Excel files called readxl.\n\n\n\nlibrary(readr) \nlibrary(dplyr) \n\nwb_data_messy &lt;- read_csv(\"data/messy_wb_data.csv\")\n\nglimpse(wb_data_messy)\n\nRows: 222\nColumns: 54\n$ `Country Name`  &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"American Samoa\",…\n$ `Country Code`  &lt;chr&gt; \"AFG\", \"ALB\", \"DZA\", \"ASM\", \"AND\", \"AGO\", \"ATG\", \"ARG\"…\n$ `Series Name`   &lt;chr&gt; \"Labor force participation rate, female (% of female p…\n$ `Series Code`   &lt;chr&gt; \"SL.TLF.ACTI.FE.ZS\", \"SL.TLF.ACTI.FE.ZS\", \"SL.TLF.ACTI…\n$ `1972 [YR1972]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1973 [YR1973]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1974 [YR1974]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1975 [YR1975]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1976 [YR1976]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1977 [YR1977]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1978 [YR1978]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1979 [YR1979]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1980 [YR1980]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1981 [YR1981]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1982 [YR1982]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1983 [YR1983]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1984 [YR1984]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1985 [YR1985]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1986 [YR1986]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1987 [YR1987]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1988 [YR1988]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1989 [YR1989]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `1990 [YR1990]` &lt;chr&gt; \"15.83\", \"60.63\", \"12.31\", \"..\", \"..\", \"76.73\", \"..\", …\n$ `1991 [YR1991]` &lt;chr&gt; \"15.89\", \"65.54\", \"12.33\", \"..\", \"..\", \"76.69\", \"..\", …\n$ `1992 [YR1992]` &lt;chr&gt; \"15.92\", \"66.56\", \"12.37\", \"..\", \"..\", \"76.66\", \"..\", …\n$ `1993 [YR1993]` &lt;chr&gt; \"15.91\", \"65.01\", \"12.41\", \"..\", \"..\", \"76.68\", \"..\", …\n$ `1994 [YR1994]` &lt;chr&gt; \"15.88\", \"63.64\", \"12.47\", \"..\", \"..\", \"76.64\", \"..\", …\n$ `1995 [YR1995]` &lt;chr&gt; \"15.92\", \"61.59\", \"12.56\", \"..\", \"..\", \"76.57\", \"..\", …\n$ `1996 [YR1996]` &lt;chr&gt; \"15.75\", \"60.28\", \"12.64\", \"..\", \"..\", \"76.55\", \"..\", …\n$ `1997 [YR1997]` &lt;chr&gt; \"15.59\", \"61.91\", \"12.59\", \"..\", \"..\", \"76.53\", \"..\", …\n$ `1998 [YR1998]` &lt;chr&gt; \"15.47\", \"60.62\", \"12.59\", \"..\", \"..\", \"76.53\", \"..\", …\n$ `1999 [YR1999]` &lt;chr&gt; \"15.4\", \"58.87\", \"12.63\", \"..\", \"..\", \"76.51\", \"..\", \"…\n$ `2000 [YR2000]` &lt;chr&gt; \"15.35\", \"57.89\", \"12.71\", \"..\", \"..\", \"76.49\", \"..\", …\n$ `2001 [YR2001]` &lt;chr&gt; \"15.5\", \"56.71\", \"12.85\", \"..\", \"..\", \"76.48\", \"..\", \"…\n$ `2002 [YR2002]` &lt;chr&gt; \"15.7\", \"56.06\", \"13.02\", \"..\", \"..\", \"76.44\", \"..\", \"…\n$ `2003 [YR2003]` &lt;chr&gt; \"15.92\", \"55.3\", \"13.24\", \"..\", \"..\", \"76.41\", \"..\", \"…\n$ `2004 [YR2004]` &lt;chr&gt; \"16.13\", \"54.57\", \"13.5\", \"..\", \"..\", \"76.38\", \"..\", \"…\n$ `2005 [YR2005]` &lt;chr&gt; \"16.33\", \"53.88\", \"13.79\", \"..\", \"..\", \"76.36\", \"..\", …\n$ `2006 [YR2006]` &lt;chr&gt; \"16.12\", \"53.43\", \"14.12\", \"..\", \"..\", \"76.39\", \"..\", …\n$ `2007 [YR2007]` &lt;chr&gt; \"15.91\", \"53.07\", \"14.47\", \"..\", \"..\", \"76.42\", \"..\", …\n$ `2008 [YR2008]` &lt;chr&gt; \"15.74\", \"52.78\", \"14.87\", \"..\", \"..\", \"76.46\", \"..\", …\n$ `2009 [YR2009]` &lt;chr&gt; \"15.65\", \"51.57\", \"15.31\", \"..\", \"..\", \"76.53\", \"..\", …\n$ `2010 [YR2010]` &lt;chr&gt; \"15.65\", \"52.75\", \"15.49\", \"..\", \"..\", \"76.59\", \"..\", …\n$ `2011 [YR2011]` &lt;chr&gt; \"16\", \"60.59\", \"16.45\", \"..\", \"..\", \"76.67\", \"..\", \"55…\n$ `2012 [YR2012]` &lt;chr&gt; \"16.44\", \"55.1\", \"17.48\", \"..\", \"..\", \"76.73\", \"..\", \"…\n$ `2013 [YR2013]` &lt;chr&gt; \"17.42\", \"50.58\", \"18.29\", \"..\", \"..\", \"76.79\", \"..\", …\n$ `2014 [YR2014]` &lt;chr&gt; \"18.46\", \"50.18\", \"16.68\", \"..\", \"..\", \"76.83\", \"..\", …\n$ `2015 [YR2015]` &lt;chr&gt; \"19.55\", \"54.05\", \"17.5\", \"..\", \"..\", \"76.87\", \"..\", \"…\n$ `2016 [YR2016]` &lt;chr&gt; \"20.7\", \"56.4\", \"18.33\", \"..\", \"..\", \"76.9\", \"..\", \"56…\n$ `2017 [YR2017]` &lt;chr&gt; \"21.91\", \"55.54\", \"19.19\", \"..\", \"..\", \"76.91\", \"..\", …\n$ `2018 [YR2018]` &lt;chr&gt; \"22.32\", \"59.12\", \"18.95\", \"..\", \"..\", \"76.9\", \"..\", \"…\n$ `2019 [YR2019]` &lt;chr&gt; \"22.74\", \"61.46\", \"18.7\", \"..\", \"..\", \"76.88\", \"..\", \"…\n$ `2020 [YR2020]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n$ `2021 [YR2021]` &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", …\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\nFollow along with the video and the steps described avove to download the data and read it into R."
  },
  {
    "objectID": "modules/module-2.6.html#reshaping-the-data",
    "href": "modules/module-2.6.html#reshaping-the-data",
    "title": "Module 2.6",
    "section": "Reshaping the Data",
    "text": "Reshaping the Data\n\nRecall from the last module that in order for the data to be tidy, we want each column to represent a variable and each row to represent an observation.\nBut here again we see that the World Bank data are in wide form, meaning that each column represents a year and each row represents a country. This entails that each row represents multiple observations, violating tidy principles.\nTo rectify this, we need to reshape the data from wide form to long form using pivot_longer()](https://tidyr.tidyverse.org/reference/pivot_longer.html) function from the tidyr package. Recall that thepivot_longer()` function takes three basic arguments:\n\n\ncols - which columns you want to pivot\n\nnames_to - the name of the column where the old column names are going to\n\nvalues_to - the name of the column where the values are going to\n\nIn our case, we want to reshape all of the year columns and have the years represented in the rows. We want the newly created column to be called “year” and the values are going to represent the data on female labor force participation we downloaded (female labor force participation rates).\n\n# Load tidyr\nlibrary(tidyr)\n\nwb_data &lt;- wb_data_messy |&gt; \n  pivot_longer(         \n    cols = `1972 [YR1972]`: `2021 [YR2021]`,\n    names_to = \"year\", \n    values_to = \"flfp\" \n  ) \n\nglimpse(wb_data)\n\nRows: 11,100\nColumns: 6\n$ `Country Name` &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanist…\n$ `Country Code` &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\",…\n$ `Series Name`  &lt;chr&gt; \"Labor force participation rate, female (% of female po…\n$ `Series Code`  &lt;chr&gt; \"SL.TLF.ACTI.FE.ZS\", \"SL.TLF.ACTI.FE.ZS\", \"SL.TLF.ACTI.…\n$ year           &lt;chr&gt; \"1972 [YR1972]\", \"1973 [YR1973]\", \"1974 [YR1974]\", \"197…\n$ flfp           &lt;chr&gt; \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"..\", \"…\n\n\nNotice that when we specify the years in our pivot_longer() call we encapsulate them in backticks (``). This is because the years, as they were imported from the WDI dataset, have spaces in them. Typically we want to avoid this scenario by writing our variable names in snake_case.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nFollow along with the video and the steps described above to reshape the data. Make sure to note and use backticks when specifying the years in the pivot_longer() call."
  },
  {
    "objectID": "modules/module-2.6.html#cleaning-up-our-data",
    "href": "modules/module-2.6.html#cleaning-up-our-data",
    "title": "Module 2.6",
    "section": "Cleaning up Our Data",
    "text": "Cleaning up Our Data\n\nNow that our data are transposed, we can start to clean up a few remaining issues. For example, the year variable is stored as a character string that includes both the year and a redundant label in brackets—e.g., \"1972 [YR1972]\". In addition, the variable flfp (female labor force participation) is stored as a character when it should be numeric.\nTo fix these issues, we’ll use the mutate() function from dplyr. First, we call mutate() along with substring() to extract just the first four characters from the year column. Then, we use the across() function inside mutate() to convert both year and flfp to numeric.\n\nwb_data &lt;- wb_data |&gt; \n  mutate(year = substring(year, 1, 4)) |&gt;  \n  mutate(across(c(\"year\", \"flfp\"), as.numeric))  \n\nglimpse(wb_data)\n\nRows: 11,100\nColumns: 6\n$ `Country Name` &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanist…\n$ `Country Code` &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\",…\n$ `Series Name`  &lt;chr&gt; \"Labor force participation rate, female (% of female po…\n$ `Series Code`  &lt;chr&gt; \"SL.TLF.ACTI.FE.ZS\", \"SL.TLF.ACTI.FE.ZS\", \"SL.TLF.ACTI.…\n$ year           &lt;dbl&gt; 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1…\n$ flfp           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nThe last thing we are going to do is to fix the variable names. Specifically, we want to remove the spaces from the remaining variables and conver them from title case to snake case. To do this, we will use the clean_names() function from the janitor package.\nAs a final step, we can export our clean data to a new .csv file with the write.csv() function from readr.\n\n# Load janitor\nlibrary(janitor)\n\nwb_data_clean &lt;- wb_data |&gt;  \n  clean_names() \n\nwrite_csv(wb_data_clean, \"data/wb_data_clean.csv\")\n\nglimpse(wb_data_clean)\n\nRows: 11,100\nColumns: 6\n$ country_name &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan…\n$ country_code &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"…\n$ series_name  &lt;chr&gt; \"Labor force participation rate, female (% of female popu…\n$ series_code  &lt;chr&gt; \"SL.TLF.ACTI.FE.ZS\", \"SL.TLF.ACTI.FE.ZS\", \"SL.TLF.ACTI.FE…\n$ year         &lt;dbl&gt; 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 198…\n$ flfp         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\nFollow along with the video and the steps described above to clean up the data. Use mutate() and across() to:\n\nTruncate the year variable so that it only includes the four-digit year,\nConvert year and flfp to numeric using across() inside mutate().\n\nThen, use clean_names() from the janitor package to clean up the variable names—removing spaces and converting to snake_case. Finally, export your cleaned dataset to a new .csv file using write_csv() from the readr package.\n💡 Challenge yourself!\nDownload a new dataset from the World Bank with more than one variable. Use pivot_longer() and pivot_wider() to reshape the data, then follow the steps above to clean and prepare it for analysis."
  },
  {
    "objectID": "modules/module-2.5.html",
    "href": "modules/module-2.5.html",
    "title": "Module 2.5",
    "section": "",
    "text": "Prework\n\n\n\n\nStart a QMD file for this module.\nReview the concepts of tidy data and read through the tidyr documentation.\nHave a quick look at the tidyr cheatsheet to explore the capabilities of the package."
  },
  {
    "objectID": "modules/module-2.5.html#overview",
    "href": "modules/module-2.5.html#overview",
    "title": "Module 2.5",
    "section": "Overview",
    "text": "Overview\nEarlier in the course, we introduced the idea of tidy data—a consistent way of structuring datasets that makes them easier to work with in R. In a tidy dataset, each variable forms a column, each observation forms a row, and each cell contains a single value. This structure is especially powerful when paired with the tidyverse tools you’ve already seen, like ggplot2 and dplyr.\nIn this module, we’ll focus on how to reshape or pivot messy data into tidy format using functions from the tidyr package. We’ll start by examining example datasets included in the package, then work with real-world data from the World Bank. Along the way, you’ll learn to convert data between wide and long formats using pivot_longer() and pivot_wider(), and you’ll practice fixing common issues like inconsistent variable names and incorrect data types."
  },
  {
    "objectID": "modules/module-2.5.html#are-these-data-tidy",
    "href": "modules/module-2.5.html#are-these-data-tidy",
    "title": "Module 2.5",
    "section": "Are These Data Tidy?",
    "text": "Are These Data Tidy?\nLet’s start by loading the tidyr package and examining a few of its built-in datasets. One of the datasets is called smiths. You can access this dataset by calling:\n\nlibrary(tidyr)\n\nsmiths\n\n# A tibble: 2 × 5\n  subject     time   age weight height\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 John Smith     1    33     90   1.87\n2 Mary Smith     1    NA     NA   1.54\n\n\nAre these data tidy? The answer is “yes.” These data are tidy because each variable is in its own column, each observation is in its own row, and each cell contains a single value. It might not look like a typical dataset, because it is small (only two rows) and because it has missing values, but it meets the tidy data principles.\nHere is another example of tidy data from the tidyr package, called table1.\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nWhat about these data? Are they tidy? Again the answer is “yes.” There are four columns, each of which represents a variable: country, year, cases, and population. Each row represents a single observation, and each cell contains a single value.\nOK let’s try one more. This one is called table2.\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\nThis one is a little tricky because, on the one hand we could say that it is tidy data in “long form” if we treat type as a single variable. But if I told you that “cases” here refers to the number of case of tuberculosis and “population” refers to the population of the country, then we would say that this is not tidy data. The reason is that the type variable is not a variable in its own right but rather a label for the two variables cases and population and, in that case, cases and population should be in separate columns (as in table1).\n\n\n\n\n\n\nLong vs. Wide Format\n\n\n\nA dataset is in long format when each row represents a single observation, and multiple categories or types that could be defined as separate variables are stacked into one column—often paired with a corresponding value column. It’s in wide format when different categories have their own columns.\nWhich format is better? It depends on how you define your variables.\nSometimes, long format is clearly appropriate. For example, if you’re comparing regions or species, it makes sense to keep those as a single column so you can facet or color by them in a ggplot plot.\nBut when you want to compare values across categories, or analyze how the values of the categories relate to a third variable, wide format is more appropriate.\n\n\nNow let’s look at another rendition of these same data.\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nCan you tell what is going on here? Are these data tidy? No. In this case, the rate variable is not the actual rate of infection but rather two values (cases and population) separated by a slash. So here we have two values in one cell, which violates the tidy data principle that each cell should contain a single value.\nFinally, let’s look at one more example. Here istable4a from the tidyr package.\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\nWhat is the issue here? In this case, the data are not tidy because the year variable is spread across multiple columns (1999 and 2000). The years represent observations and should therefore be in their own rows in the context of a single column, not in separate columns.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nExplore more datasets in the tidyr package, type data(package = \"tidyr\") in your console or in a R codechunk in your notebook. Then, view the data frames and ask yourself whether the data in each dataset are tidy or not. Many of these datasets are used as examples in Chapter 5 of Hadley Wickham’s R for Data Science book, which was assigned for this module."
  },
  {
    "objectID": "modules/module-2.5.html#tidying-data",
    "href": "modules/module-2.5.html#tidying-data",
    "title": "Module 2.5",
    "section": "Tidying Data",
    "text": "Tidying Data\nLet’s now talk about how we can use tidyr to tidy our data. The tidyr package provides several functions to help you reshape data into tidy format. The most commonly used functions are pivot_longer() and pivot_wider(). But we can also make use of separate() when we need to split a single column into multiple columns.\npivot_longer() is helpful when we have a situation like we saw in table4a, where we have multiple columns representing different observations of the same variable. It allows us to “pivot” those columns into a single column with an additional column indicating the observation type (e.g., year).\npivot_longer() takes three main arguments: cols to identify which columns you want to pivot; names_to to identify the name of the column where the old column names are going to (the identifier), and values_to, the name of the column where the values are going to. Let’s give it a try with table4a:\n\ntable4a_pivot &lt;- table4a |&gt;\n  pivot_longer(cols = c(`1999`, `2000`), \n               names_to = \"year\", \n               values_to = \"cases\")\n\ntable4a_pivot\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\nThis reshapes the data so that we have a single year column and a single cases column, with the observations (country-years) in the rows.\nNow let’s take the case of table2, which is in long format but not tidy because the type variable is not a separate variable. We can use pivot_wider() to reshape it into a tidy format.\npivot_wider() takes three main arguments: names_from to identify the column whose values will become new column names, values_from to identify the column containing the values that will fill the new columns, and values_fill, an optional argument that specifies what to use for missing values (e.g., NA, 0).\nLet’s try reshaping table2 into a tidy format using pivot_wider():\n\ntable2_tidy &lt;- table2 |&gt;\n  pivot_wider(names_from = type, \n              values_from = count)\n\ntable2_tidy\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nFinally, let’s look at how to use separate() to split a single column into multiple columns. This is useful when you have a column that contains multiple pieces of information, like the rate column in table3.\n\ntable3_separated &lt;- table3 |&gt;\n  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\")\n\ntable3_separated\n\n# A tibble: 6 × 4\n  country      year cases  population\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\nTry using pivot_longer(), pivot_wider(), and separate() on the tidyr datasets you explored earlier. If you really want to challenge yourself, try some of the worked examples in Chapter 5 of R for Data Science."
  },
  {
    "objectID": "modules/module-2.5.html#world-bank-worked-example",
    "href": "modules/module-2.5.html#world-bank-worked-example",
    "title": "Module 2.5",
    "section": "World Bank Worked Example",
    "text": "World Bank Worked Example\nAs a worked example, let’s look at the tidyr world_bank_pop dataset, which simulates a typical structure you might get from downloading World Bank data manually. Let’s take a closer look at the world_bank_pop dataset. You can view it using:\n\nworld_bank_pop\n\n# A tibble: 1,064 × 20\n   country indicator      `2000`  `2001`  `2002`  `2003`  `2004`  `2005`  `2006`\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 ABW     SP.URB.TOTL    4.16e4 4.20e+4 4.22e+4 4.23e+4 4.23e+4 4.24e+4 4.26e+4\n 2 ABW     SP.URB.GROW    1.66e0 9.56e-1 4.01e-1 1.97e-1 9.46e-2 1.94e-1 3.67e-1\n 3 ABW     SP.POP.TOTL    8.91e4 9.07e+4 9.18e+4 9.27e+4 9.35e+4 9.45e+4 9.56e+4\n 4 ABW     SP.POP.GROW    2.54e0 1.77e+0 1.19e+0 9.97e-1 9.01e-1 1.00e+0 1.18e+0\n 5 AFE     SP.URB.TOTL    1.16e8 1.20e+8 1.24e+8 1.29e+8 1.34e+8 1.39e+8 1.44e+8\n 6 AFE     SP.URB.GROW    3.60e0 3.66e+0 3.72e+0 3.71e+0 3.74e+0 3.81e+0 3.81e+0\n 7 AFE     SP.POP.TOTL    4.02e8 4.12e+8 4.23e+8 4.34e+8 4.45e+8 4.57e+8 4.70e+8\n 8 AFE     SP.POP.GROW    2.58e0 2.59e+0 2.61e+0 2.62e+0 2.64e+0 2.67e+0 2.70e+0\n 9 AFG     SP.URB.TOTL    4.31e6 4.36e+6 4.67e+6 5.06e+6 5.30e+6 5.54e+6 5.83e+6\n10 AFG     SP.URB.GROW    1.86e0 1.15e+0 6.86e+0 7.95e+0 4.59e+0 4.47e+0 5.03e+0\n# ℹ 1,054 more rows\n# ℹ 11 more variables: `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;,\n#   `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;\n\n\nAt first glance, this dataset appears to be in a wide format. Each row represents a country and indicator (such as “SP.POP.TOTL” for total population), and each year from 2000 to 2017 is stored in its own column. While this might seem convenient, it violates the tidy data principle that each variable should form its own column. Here, the years are actually values of a single variable called “year”—but they are spread across multiple columns.\nTo tidy this data, our first step is to convert it from wide format to long format using the pivot_longer() function from the tidyr package. Specifically, we want to pivot all the year columns into a single “year” column, with a corresponding “pop” column for the population values.\nHere’s how we do it:\n\nlong_pop_data &lt;- world_bank_pop |&gt;\n  pivot_longer(\n    cols = `2000`:`2017`,   # The year columns to pivot\n    names_to = \"year\",      # New column for year values\n    values_to = \"pop\"       # New column for population values\n  )\n\nlong_pop_data\n\n# A tibble: 19,152 × 4\n   country indicator   year    pop\n   &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 ABW     SP.URB.TOTL 2000  41625\n 2 ABW     SP.URB.TOTL 2001  42025\n 3 ABW     SP.URB.TOTL 2002  42194\n 4 ABW     SP.URB.TOTL 2003  42277\n 5 ABW     SP.URB.TOTL 2004  42317\n 6 ABW     SP.URB.TOTL 2005  42399\n 7 ABW     SP.URB.TOTL 2006  42555\n 8 ABW     SP.URB.TOTL 2007  42729\n 9 ABW     SP.URB.TOTL 2008  42906\n10 ABW     SP.URB.TOTL 2009  43079\n# ℹ 19,142 more rows\n\n\nThis transformation makes the dataset much more flexible. Now, each row corresponds to a single observation: a country-indicator-year combination with a single value for population. This structure is ideal for most types of analysis and visualization. However, it’s worth asking—are we fully done tidying?\nNot quite. The current dataset is in long format, and that’s great in many situations. But in this case, each country-year may have data for multiple indicators (e.g., total population, urban population), and in most cases we would prefer to have each indicator in its own column.\nTo do that, we can use pivot_wider() to reshape the dataset so that the indicator codes become column names and each row represents a country-year combination.\n\ntidy_pop_data &lt;- long_pop_data |&gt;\n  pivot_wider(\n    names_from = indicator, \n    values_from = pop\n  )\n\ntidy_pop_data\n\n# A tibble: 4,788 × 6\n   country year  SP.URB.TOTL SP.URB.GROW SP.POP.TOTL SP.POP.GROW\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 ABW     2000        41625      1.66         89101       2.54 \n 2 ABW     2001        42025      0.956        90691       1.77 \n 3 ABW     2002        42194      0.401        91781       1.19 \n 4 ABW     2003        42277      0.197        92701       0.997\n 5 ABW     2004        42317      0.0946       93540       0.901\n 6 ABW     2005        42399      0.194        94483       1.00 \n 7 ABW     2006        42555      0.367        95606       1.18 \n 8 ABW     2007        42729      0.408        96787       1.23 \n 9 ABW     2008        42906      0.413        97996       1.24 \n10 ABW     2009        43079      0.402        99212       1.23 \n# ℹ 4,778 more rows\n\n\nNow, the dataset is both tidy and wide: one row per country and year, with separate columns for each indicator. This makes it easy to perform operations like plotting urban versus total population or calculating the share of the population that is urban.\nThis kind of workflow—going from wide to long and back to wide—is common when working with real-world datasets. You might start with a messy spreadsheet or export, use pivot_longer() to get it into a form that’s easier to work with, and then use pivot_wider() to tailor the structure to your specific analysis needs."
  },
  {
    "objectID": "modules/module-4.2.html",
    "href": "modules/module-4.2.html",
    "title": "Module 4.2",
    "section": "",
    "text": "Prework\n\n\n\nRun this code chunk to load the necessary packages and data for this module:\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\n# Load V-Dem data for 2019\nmodel_data &lt;- fetchdem(\n  indicators = c(\n  \"v2x_libdem\", \n  \"e_gdppc\", \n  \"v2cacamps\"),\n  start_year = 2019, \n  end_year = 2019\n  ) |&gt;\n  rename(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc,\n    polarization = v2cacamps\n    ) |&gt;\n  filter(!is.na(lib_dem), !is.na(wealth))"
  },
  {
    "objectID": "modules/module-4.2.html#overview",
    "href": "modules/module-4.2.html#overview",
    "title": "Module 4.2",
    "section": "Overview",
    "text": "Overview\nIn Module 4.1, you learned how to fit regression lines and interpret them. But how does R actually find the “best” line among all possible lines? This module dives into the mathematical optimization behind least squares regression. You’ll understand why it’s called “least squares” and develop intuition for the cost function that R minimizes when fitting your models.\nBy the end of this module, you’ll be able to: - Explain why we minimize the sum of squared residuals - Calculate and interpret a cost function - Understand the optimization process behind lm() - Connect mathematical theory to practical regression output"
  },
  {
    "objectID": "modules/module-4.2.html#the-optimization-problem",
    "href": "modules/module-4.2.html#the-optimization-problem",
    "title": "Module 4.2",
    "section": "The Optimization Problem",
    "text": "The Optimization Problem\nWhen we fit a regression line in Module 4.1, we used R’s lm() function and got specific values for our intercept and slope. But think about it - there are infinitely many possible lines we could draw through any set of points. How does the computer choose which one is “best”?\nLet’s return to our democracy and GDP example from Module 4.1. We found that the relationship between log GDP per capita and democracy scores could be modeled as:\n\\[\\widehat{Democracy}_i = 0.13 + 0.12 \\times \\log(GDP)_i\\]\nBut why these specific numbers? Why not \\(\\widehat{Democracy}_i = 0.10 + 0.15 \\times \\log(GDP)_i\\) or any other combination?\n\n\n\n\n\n\n\n\nThe answer lies in a mathematical optimization problem. We want to find the line that makes the “best” predictions - the line that minimizes our prediction errors across all the data points."
  },
  {
    "objectID": "modules/module-4.2.html#understanding-the-cost-function",
    "href": "modules/module-4.2.html#understanding-the-cost-function",
    "title": "Module 4.2",
    "section": "Understanding the Cost Function",
    "text": "Understanding the Cost Function\nTo understand how we measure “best,” let’s watch Andrew Ng explain the intuition behind the cost function:\n\nAs Andrew explains, we need a way to measure how well our line fits the data. Remember from Module 4.1 that a residual is the difference between an actual value and our predicted value:\n\\[\\text{residual}_i = y_i - \\hat{y}_i\\]\nThe cost function (also called the loss function) measures the total error across all our predictions. For least squares regression, we use the sum of squared residuals (SSR):\n\\[\\text{Cost} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nBut why do we square the residuals instead of just adding them up? There are several good reasons.First, squaring the residuals ensures that the cost function is always positive, which is important for optimization. Squaring ensures that a prediction that’s too high (+2) is penalized the same as a prediction that’s too low (-2). Second, larger errors get bigger penalties. An error of 4 contributes 16 to the cost, while an error of 2 contributes only 4. Finally, for mathematical convenience. Squared functions have nice mathematical properties that make optimization easier.\n\n\n\n\n\n\nNote\n\n\n\nSome definitions of the cost function divide the SSR by the number of observations \\(n\\) (or two times the number of observations \\(2n\\)), yielding the mean squared error (MSE). This doesn’t change which line is best—it just rescales the cost.\n\n\nThe line that minimizes this cost function is our “best” line - the least squares regression line.\nLet’s build intuition with a very simple example. Consider these three data points: - (1, 1) - (2, 2) - (3, 3)\nWhat line would you draw through these points? Let’s test different lines and see which has the lowest cost.\n\n# Create our simple dataset\nsimple_data &lt;- tibble(\n  x = c(1, 2, 3),\n  y = c(1, 2, 3)\n)\n\nsimple_data\n\n# A tibble: 3 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     1\n2     2     2\n3     3     3\n\n\n\n\n\n\n\n\n\n\nTest Line 1: \\(\\hat{y} = 0 + 1 \\times x\\) (intercept = 0, slope = 1)\nFor each point, let’s calculate the predicted value and residual: - Point (1,1): \\(\\hat{y} = 0 + 1(1) = 1\\), residual = \\(1 - 1 = 0\\) - Point (2,2): \\(\\hat{y} = 0 + 1(2) = 2\\), residual = \\(2 - 2 = 0\\)\n- Point (3,3): \\(\\hat{y} = 0 + 1(3) = 3\\), residual = \\(3 - 3 = 0\\)\nSum of squared residuals = \\(0^2 + 0^2 + 0^2 = 0\\)\nPerfect! This line goes exactly through all points.\n\n\n\n\n\n\n\n\nTest Line 2: \\(\\hat{y} = 0 + 2 \\times x\\) (intercept = 0, slope = 2)\nThis is a steeper line with a slope of 2. Let’s calculate the residuals:\n\nPoint (1,1): \\(\\hat{y} = 0 + 0(1) = 0\\), residual = \\(1 - 2 = -1\\)\n\nPoint (2,2): \\(\\hat{y} = 0 + 0(2) = 0\\), residual = \\(2 - 4 = -2\\)\n\nPoint (3,3): \\(\\hat{y} = 0 + 0(3) = 0\\), residual = \\(3 - 6 = -3\\)\n\n\nSum of squared residuals = \\(-1^2 + -2^2 + -3^2 = 1 + 4 + 9 = 14\\)\nMuch worse!\n\n\n\n\n\n\nYour Turn!!\n\n\n\nAssuming the same data points as in the above example, calculate the sum of squared residuals for the following lines:\n\n\n\\(\\hat{y} = 0 + 3 \\times x\\) (intercept = 0, slope = 3)\n\n\\(\\hat{y} = 0 + 0 \\times x\\) (intercept = 0, slope = 0)\n\n\\(\\hat{y} = 0 - 1 \\times x\\) (intercept = 0, slope = -1)\n\nChange the values in this interactive code chunk to perform your calculations:\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCheck your answers below when you are finished:\n\nCode# Calculate the sum of squared residuals for the line ŷ = 0 + 3x\n#ssr1 &lt;- (1-3)^2 + (2-6)^2 + (3-9)^2\n#ssr1\n# Answer: 56\n\n# Calculate the sum of squared residuals for the line ŷ = 0 + 0x\n#ssr2 &lt;- (1-0)^2 + (2-0)^2 + (3-0)^2\n#ssr2\n# Answer: 14\n\n# Calculate the sum of squared residuals for the line ŷ = 0 -1x\n#ssr3 &lt;- (1+1)^2 + (2+2)^2 + (3+3)^2\n#ssr3\n# Answer: 56"
  },
  {
    "objectID": "modules/module-4.2.html#visualizing-the-optimization",
    "href": "modules/module-4.2.html#visualizing-the-optimization",
    "title": "Module 4.2",
    "section": "Visualizing the Optimization",
    "text": "Visualizing the Optimization\nLet’s see how the cost function behaves as we change the slope parameter. Andrew Ng provides excellent visualization of this concept:\n\nFor our simple three-point example, let’s use this Shiny app to plot the cost function for different slope values (keeping intercept = 0). Move the slide to choose a different slope. See how this changes the fit of the line relative to the points on the left, and how it affects the cost function on the right.\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\ndata_points &lt;- data.frame(x = c(1, 2, 3), y = c(1, 2, 3))\n\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Cost Function Explorer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"slope\", \n                  \"Slope Parameter:\", \n                  min = -2, max = 4, value = 1, step = 0.1),\n      br(),\n      h4(\"Current Values:\"),\n      textOutput(\"current_slope\"),\n      textOutput(\"current_equation\"),\n      textOutput(\"current_ssr\"),\n      br(),\n      p(\"Move the slider to see how the slope affects:\"),\n      tags$ul(\n        tags$li(\"The regression line (left plot)\"),\n        tags$li(\"Your position on the cost function (right plot)\")\n      )\n    ),\n    \n    mainPanel(\n      plotOutput(\"combined_plot\", height = \"400px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  # Fixed: Calculate predictions and residuals properly\n  current_calculations &lt;- reactive({\n    predictions &lt;- input$slope * data_points$x\n    residuals &lt;- data_points$y - predictions\n    ssr &lt;- sum(residuals^2)\n    list(predictions = predictions, residuals = residuals, ssr = ssr)\n  })\n  \n  cost_data &lt;- reactive({\n    slopes &lt;- seq(-2, 4, by = 0.1)\n    ssr_values &lt;- sapply(slopes, function(b) {\n      preds &lt;- b * data_points$x\n      resids &lt;- data_points$y - preds\n      sum(resids^2)\n    })\n    list(slopes = slopes, ssr = ssr_values)\n  })\n  \n  output$current_slope &lt;- renderText({\n    paste(\"Slope:\", round(input$slope, 2))\n  })\n  \n  output$current_equation &lt;- renderText({\n    paste0(\"Equation: Ŷ = 0 + \", round(input$slope, 2), \" * X\")\n  })\n  \n  # Fixed: Use the corrected reactive calculations\n  output$current_ssr &lt;- renderText({\n    calc &lt;- current_calculations()\n    ssr_terms &lt;- paste0(\"(\", data_points$y, \" - \", round(calc$predictions, 2), \")^2\", collapse = \" + \")\n    ssr_value &lt;- round(calc$ssr, 2)\n    paste0(\"SSR: \", ssr_terms, \" = \", ssr_value)\n  })\n  \n  output$combined_plot &lt;- renderPlot({\n    par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))\n    \n    # Left: Regression plot\n    plot(data_points$x, data_points$y, pch = 19, col = \"blue\",\n         xlim = c(0, 4), ylim = c(-2, 6),\n         xlab = \"x\", ylab = \"y\", main = \"Regression Line\")\n    abline(0, input$slope, col = \"red\", lwd = 2)\n    \n    # Add residual lines for visualization\n    calc &lt;- current_calculations()\n    segments(data_points$x, data_points$y, data_points$x, calc$predictions, \n             col = \"gray\", lty = 2)\n    \n    # Right: Cost function\n    cost &lt;- cost_data()\n    plot(cost$slopes, cost$ssr, type = \"l\", col = \"darkred\", lwd = 2,\n         xlab = \"Slope Parameter\", ylab = \"Sum of Squared Residuals\",\n         main = \"Cost Function\")\n    points(input$slope, calc$ssr, col = \"red\", pch = 19, cex = 1.5)\n  })\n}\n\nshinyApp(ui = ui, server = server)\nNotice that the cost function forms a parabola with its minimum at slope = 1. This is exactly where we found SSR = 0! The optimization problem is to find the slope (and intercept) that minimizes this cost function."
  },
  {
    "objectID": "modules/module-4.2.html#worked-example-democracy-and-gdp",
    "href": "modules/module-4.2.html#worked-example-democracy-and-gdp",
    "title": "Module 4.2",
    "section": "Worked Example: Democracy and GDP",
    "text": "Worked Example: Democracy and GDP\nLet’s apply this same thinking to our democracy and GDP data. We’ll manually test a few different potential regression lines and calculate their costs.\nFirst, let’s fit the actual least squares line to remind ourselves what R found:\n\n# Fit the model\ndemocracy_model &lt;- lm(lib_dem ~ log(wealth), data = model_data)\n\n# Show the summary\nsummary(democracy_model)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,    Adjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n\n\nThe least squares solution is approximately: \\(\\widehat{Democracy} = 0.13 + 0.12 \\times \\log(GDP)\\)\nLet’s test some alternative lines and see how they compare:\n\n\n\n\n\n\n\n\nNow let’s calculate the sum of squared residuals (SSR) for each of these lines to see which one has the lowest cost:\n\n\nLeast squares line (intercept = 0.13, slope = 0.12): SSR = 8.57\n\nSteeper slope line (intercept = 0.13, slope = 0.15): SSR = 9.58\n\nGentler slope line (intercept = 0.13, slope = 0.08): SSR = 10.49\n\nDifferent intercept line (intercept = 0.00, slope = 0.12): SSR = 11.58\n\nNotice how the least squares line has the lowest SSR! Any other combination of intercept and slope results in a higher cost, confirming that our optimization algorithm found the truly optimal solution.\nNotice how the least squares line has the lowest SSR! Any other combination of intercept and slope will result in a higher cost."
  },
  {
    "objectID": "modules/module-4.2.html#from-math-to-r-output",
    "href": "modules/module-4.2.html#from-math-to-r-output",
    "title": "Module 4.2",
    "section": "From Math to R Output",
    "text": "From Math to R Output\nWhen you run lm() in R, the computer is solving this optimization problem automatically. It searches through all possible combinations of intercept and slope values to find the ones that minimize the sum of squared residuals.\nFor simple linear regression, there’s actually a mathematical formula to find the optimal parameters directly (no searching required). But the key insight is that R is giving you the parameter values that make your predictions as accurate as possible, on average, across all your data points.\nThis is why we can trust the output from lm() - it’s not arbitrary, it’s the result of a principled mathematical optimization.\n\n# Our model from before\nsummary(democracy_model)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,    Adjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n\n# The SSR for this model\nactual_ssr &lt;- sum(residuals(democracy_model)^2)\ncat(\"Sum of squared residuals for least squares line:\", round(actual_ssr, 3))\n\nSum of squared residuals for least squares line: 8.574\n\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nIn the last line of this code chunk we apply the cat() function to print the sum of squared residuals (SSR) for the least squares line. The cat() function is used to concatenate and print text and variables together in a single output. The round() function is applied to format the SSR value to three decimal places for better readability.\n\n\n\n\n\n\n\n\nYour Turn!\n\n\n\n\nTry running a regression with the polarization variable as the predictor of liberal democracy instead of wealth.\nCalculate the sum of squared residuals (SSR) for this new model."
  },
  {
    "objectID": "modules/module-4.2.html#summary",
    "href": "modules/module-4.2.html#summary",
    "title": "Module 4.2",
    "section": "Summary",
    "text": "Summary\nThe “least squares” in least squares regression refers to the optimization principle: find the line that minimizes the sum of squared residuals. This mathematical framework ensures that: 1) your predictions are as accurate as possible on average across all data points; 2) the solution is unique in that there only one best line for any dataset; the method is principled (not arbitrary) because it is based on mathematical optimization; and 4) R’s output is trustworthy because lm() is finding the genuinely best-fitting line.\nUnderstanding this optimization principle helps you appreciate why regression works and gives you confidence in interpreting the results. When you see regression coefficients, you now know they represent the solution to a well-defined mathematical problem: finding the line that makes the best predictions for your data."
  },
  {
    "objectID": "modules/module-2.3.html",
    "href": "modules/module-2.3.html",
    "title": "Module 2.3",
    "section": "",
    "text": "Prework\n\n\n\n\nStart a QMD file for this module.\nReview the concepts of filter(), select(), and mutate() from our previous lesson.\nRead about the vdemlite package.\n\nvdemlite is not on CRAN, so you will need to install it from GitHub using the pak package.\n\nFirst install pak by typing install.packages(\"pak\") in your console.\nThen, install the vdemlite package by typing pak::pkg_install(\"eteitelbaum/vdemlite\") in your console."
  },
  {
    "objectID": "modules/module-2.3.html#overview",
    "href": "modules/module-2.3.html#overview",
    "title": "Module 2.3",
    "section": "Overview",
    "text": "Overview\nIn this lesson, you’ll learn how to summarize data by groups using the powerful group_by(), summarize(), and arrange() functions from the dplyr package. This sequence of operations is one of the most common and useful workflows in data science. We’ll apply it to real-world data from the Varieties of Democracy (V-Dem) project, a rich dataset that measures democratic attributes across countries and years. You’ll gain experience calculating summary statistics for different regions and time periods, and ranking countries or groups based on those statistics."
  },
  {
    "objectID": "modules/module-2.3.html#the-v-dem-dataset-and-the-vdemlite-package",
    "href": "modules/module-2.3.html#the-v-dem-dataset-and-the-vdemlite-package",
    "title": "Module 2.3",
    "section": "The V-Dem Dataset (and the vdemlite package)",
    "text": "The V-Dem Dataset (and the vdemlite package)\nThe V-Dem project stands for Varieties of Democracy. It provides detailed, expert-coded data on the quality of democracy across countries and years. The full dataset, accessible via the vdemdata package, includes over 4,000 variables dating back to the 18th century. But this dataset is quite large and complex, making it less practical for many applications.\nFor this class we are going to mainly rely on a package called vdemlite. This package includes several hundred widely used indicators from 1970 onward and is optimized for quick access and online teaching environments.\nThe vdemlite package comes with several convenient functions. searchdem() is a convenience function that helps you look up specific indicators or find the underlying components of composite indices.\n\nlibrary(vdemlite)\n\nsearchdem()\n\n\n\n\nVariable Tags and Descriptions\n\n\n\n\n\n\n\nWhen you call searchdem() you get a searchable table of all the indicators in the vdemlite package that allows you to search by the indicator tab/label and descriptor. Try it out!\nOnce you have identified a variable that you are interested in, you can call summarizedem(), which generates summary statistics for an indicator. Let’s summarize the polyarchy score, a widely used measure of electoral democracy.\n\nsummarizedem(indicator = \"v2x_polyarchy\")\n\n\n\n\nSummary of v2x_polyarchy by Country\nYears: 1970 - 2023\n\n\n\n\n\n\nFinally, you can use fetchdem() to retrieve a subset of the data filtered by indicators, countries, and years.\n\nlibrary(dplyr)\n\ndem_indicators &lt;- fetchdem(\n  indicators = c(\"v2x_polyarchy\", \"v2xel_frefair\"),                   \n  start_year = 2000, end_year = 2020,\n  countries = c(\"USA\", \"SWE\")\n  )\n\nglimpse(dem_indicators)\n\nRows: 42\nColumns: 6\n$ country_name    &lt;chr&gt; \"Sweden\", \"Sweden\", \"Sweden\", \"Sweden\", \"Sweden\", \"Swe…\n$ country_text_id &lt;chr&gt; \"SWE\", \"SWE\", \"SWE\", \"SWE\", \"SWE\", \"SWE\", \"SWE\", \"SWE\"…\n$ country_id      &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, …\n$ year            &lt;dbl&gt; 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, …\n$ v2x_polyarchy   &lt;dbl&gt; 0.914, 0.914, 0.914, 0.915, 0.915, 0.915, 0.915, 0.916…\n$ v2xel_frefair   &lt;dbl&gt; 0.962, 0.962, 0.962, 0.963, 0.963, 0.963, 0.963, 0.965…\n\n\nHere, fetchdem includes filtering arguments for indicators, countries, and years. We could also have used the dplyr select() and filter() verbs to narrow down the data after fetching it, and this is actually what is going on “under the hood.” These fetchdem() arguments just make it easier to work with the data without having to write a lot of code.\n\n\n\n\n\n\nYour Turn!\n\n\n\n\nUse searchdem() to find an indicator of interest.\nUse summarizedem() to view its summary statistics.\nUse fetchdem() to download that variable for a country or set of countries."
  },
  {
    "objectID": "modules/module-2.3.html#grouping-and-summarizing",
    "href": "modules/module-2.3.html#grouping-and-summarizing",
    "title": "Module 2.3",
    "section": "Grouping and Summarizing",
    "text": "Grouping and Summarizing\n\nLet’s use the V-Dem data to illustrate how to group and summarize data. One of the most common sequences in data wrangling involves group_by(), followed by summarize(), and then arrange() to sort the results.\nWe can start by downloading some data for all of the countries. Let’s download the polyarchy score (v2x_polyarchy), along with V-Dem’s liberal democracy score (v2x_libdem), a women’s political empowerment index (v2x_gender) and per capital gdp (v2x_gdp_pc). Let’s also download the region of each country (e_regionpol_6C) so that we can have something to group and summarize by.\nLet’s then save those data in a data frame called democracy and let’s then pipe the data into the rename function so that we can have more intuitive names for our variables.\n\ndemocracy &lt;- \n  fetchdem(\n    indicators = c(\n      \"v2x_polyarchy\", \n      \"v2x_libdem\", \n      \"v2x_gender\", \n      \"e_gdppc\", \n      \"e_regionpol_6C\"\n      ) \n    ) |&gt;\n    rename(\n      polyarchy = v2x_polyarchy,\n      libdem = v2x_libdem,\n      womens_emp = v2x_gender,\n      gdp_pc = e_gdppc,\n      region = e_regionpol_6C\n    )\n\nglimpse(democracy)\n\nRows: 9,170\nColumns: 9\n$ country_name    &lt;chr&gt; \"Mexico\", \"Mexico\", \"Mexico\", \"Mexico\", \"Mexico\", \"Mex…\n$ country_text_id &lt;chr&gt; \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\"…\n$ country_id      &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ year            &lt;dbl&gt; 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, …\n$ polyarchy       &lt;dbl&gt; 0.250, 0.248, 0.249, 0.249, 0.251, 0.251, 0.262, 0.276…\n$ libdem          &lt;dbl&gt; 0.111, 0.110, 0.111, 0.111, 0.111, 0.111, 0.115, 0.123…\n$ womens_emp      &lt;dbl&gt; 0.421, 0.421, 0.421, 0.426, 0.426, 0.433, 0.446, 0.454…\n$ gdp_pc          &lt;dbl&gt; 7.890, 8.082, 8.463, 8.845, 9.189, 9.480, 9.673, 9.914…\n$ region          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n\n\nNotice here that we are just downloading the data for these variables for all of the years and all of the countries. We are not using the countries or years arguments, so we get all of the data.\nAlso notice that the region variable is a series of region codes. This is going to make it hard to understand what the regions are, so we will need to do some additional work to make this more interpretable. Let’s call mutate() along with case_match() to create a new variable that classifies countries into named regions based on the e_regionpol_6C variable. We will save the new data frame as democracy again, overwriting the previous version.\n\ndemocracy &lt;- democracy |&gt;\n  mutate(\n    region = case_match(region, # replace the values with country names\n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n  )\n\nglimpse(democracy)\n\nRows: 9,170\nColumns: 9\n$ country_name    &lt;chr&gt; \"Mexico\", \"Mexico\", \"Mexico\", \"Mexico\", \"Mexico\", \"Mex…\n$ country_text_id &lt;chr&gt; \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\"…\n$ country_id      &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ year            &lt;dbl&gt; 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, …\n$ polyarchy       &lt;dbl&gt; 0.250, 0.248, 0.249, 0.249, 0.251, 0.251, 0.262, 0.276…\n$ libdem          &lt;dbl&gt; 0.111, 0.110, 0.111, 0.111, 0.111, 0.111, 0.115, 0.123…\n$ womens_emp      &lt;dbl&gt; 0.421, 0.421, 0.421, 0.426, 0.426, 0.433, 0.446, 0.454…\n$ gdp_pc          &lt;dbl&gt; 7.890, 8.082, 8.463, 8.845, 9.189, 9.480, 9.673, 9.914…\n$ region          &lt;chr&gt; \"Latin America\", \"Latin America\", \"Latin America\", \"La…\n\n\nOnce we have the data ready, we can summarize these variables by region. Below we have an example of how to group the data by region, summarize the average democracy score, the median libdem score, the standard deviation of womens_emp, and the minimum value of gdp_pc. We then arrange the results in descending order of the average democracy score.\n\ndemocracy |&gt;\n  group_by(region) |&gt;\n  summarize(\n    polyarchy_mean = mean(polyarchy, na.rm = TRUE),\n    libdem_median = median(libdem, na.rm = TRUE),\n    womens_emp_sd = sd(womens_emp, na.rm = TRUE),\n    gdp_pc_min = min(gdp_pc, na.rm = TRUE)\n  ) |&gt;\n  arrange(desc(polyarchy_mean))\n\n# A tibble: 6 × 5\n  region         polyarchy_mean libdem_median womens_emp_sd gdp_pc_min\n  &lt;chr&gt;                   &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1 The West                0.846         0.802         0.101      4.04 \n2 Latin America           0.532         0.401         0.201      1.19 \n3 Eastern Europe          0.475         0.338         0.161      1.47 \n4 Asia                    0.354         0.212         0.193      0.726\n5 Africa                  0.315         0.143         0.205      0.286\n6 Middle East             0.213         0.118         0.183      1.49 \n\n\n\n\n\n\n\n\nCode Detail: na.rm = TRUE\n\n\n\nNote how we add na.rm = TRUE to the mean(), median(), and sd() functions. This is important because it tells R to ignore any missing values (NAs) when calculating these statistics. If you don’t include this argument, R will return NA for the entire summary if any of the values are missing.\n\n\nThis pattern — group, summarize, arrange — is at the core of many descriptive analyses. You’re grouping the data by a categorical variable (region), summarizing one or more numeric variables (e.g., polyarchy, libdem), and then sorting the results to highlight interesting patterns.\nWhat if we wanted to get the same summary statistics for multiple variables at once? To do this, we can use the across() function within summarize(). This allows us to apply the same function (like mean(), median(), etc.) to multiple columns without repeating code.\n\ndemocracy |&gt;\n  group_by(region) |&gt;\n  summarize(\n    across(\n      c(polyarchy, libdem, womens_emp, gdp_pc),\n           mean,\n           na.rm = TRUE,\n           .names = \"mean_{col}\"\n      )\n  ) |&gt;\n  arrange(desc(mean_polyarchy))\n\n# A tibble: 6 × 5\n  region         mean_polyarchy mean_libdem mean_womens_emp mean_gdp_pc\n  &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n1 The West                0.846       0.776           0.867       31.5 \n2 Latin America           0.532       0.392           0.656        8.37\n3 Eastern Europe          0.475       0.357           0.739       11.6 \n4 Asia                    0.354       0.265           0.557        7.38\n5 Africa                  0.315       0.215           0.537        3.83\n6 Middle East             0.213       0.165           0.422       20.7 \n\n\nHere we are grouping the data by region and then summarizing the mean of several indicators: polyarchy, libdem, women_rep, and flfp. The .names = \"mean_{col}\" argument allows us to create new column names that include the original variable names, prefixed with “mean_”.\n\n\n\n\n\n\nYour Turn!\n\n\n\n\nFind a few indicators of interest in vdemlite using searchdem().\nUse summarizedem() to view their summary statistics.\nUse fetchdem() to download the data for those indicators and region codes that you can use to group the data, filtering for a subset of years if you like, but retaining all of the countries. Save those data in a data frame called democracy or something similar.\nUse a group_by(), summarize(), and arrange() sequence to calculate some summary statistics for the data. Don’t forget to include na.rm = TRUE in your summary functions to handle missing values.\nThen, use across() to compute one statistic (i.e. mean or median) of several indicators."
  },
  {
    "objectID": "modules/shinylive-cost-function.html",
    "href": "modules/shinylive-cost-function.html",
    "title": "Untitled",
    "section": "",
    "text": "#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Simple data points\ndata_points &lt;- data.frame(x = c(1, 2, 3), y = c(1, 2, 3))\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Cost Function Explorer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"slope\", \n                  \"Slope Parameter:\", \n                  min = -2, max = 4, value = 1, step = 0.1),\n      br(),\n      h4(\"Current Values:\"),\n      textOutput(\"current_slope\"),\n      textOutput(\"current_ssr\"),\n      br(),\n      p(\"Move the slider to see how the slope affects:\"),\n      tags$ul(\n        tags$li(\"The regression line (left plot)\"),\n        tags$li(\"Your position on the cost function (right plot)\"),\n        tags$li(\"The sum of squared residuals\")\n      )\n    ),\n    \n    mainPanel(\n      plotOutput(\"combined_plot\", height = \"400px\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  \n  # Calculate SSR for current slope\n  current_ssr &lt;- reactive({\n    predictions &lt;- 0 + input$slope * data_points$x\n    residuals &lt;- data_points$y - predictions\n    sum(residuals^2)\n  })\n  \n  # Create cost function data\n  cost_data &lt;- reactive({\n    slopes &lt;- seq(-2, 4, by = 0.1)\n    ssr_values &lt;- sapply(slopes, function(b) {\n      predictions &lt;- 0 + b * data_points$x\n      residuals &lt;- data_points$y - predictions\n      sum(residuals^2)\n    })\n    data.frame(slope = slopes, ssr = ssr_values)\n  })\n  \n  output$current_slope &lt;- renderText({\n    paste(\"Slope:\", round(input$slope, 2))\n  })\n  \n  output$current_ssr &lt;- renderText({\n    paste(\"Sum of Squared Residuals:\", round(current_ssr(), 2))\n  })\n  \n  output$combined_plot &lt;- renderPlot({\n    # Left plot: Data points and regression line\n    p1 &lt;- ggplot(data_points, aes(x = x, y = y)) +\n      geom_point(size = 4, color = \"darkblue\") +\n      geom_abline(intercept = 0, slope = input$slope, color = \"red\", linewidth = 1.2) +\n      xlim(0, 4) + ylim(-2, 6) +\n      labs(title = \"Regression Line\", x = \"x\", y = \"y\") +\n      theme_minimal() +\n      theme(plot.title = element_text(hjust = 0.5))\n    \n    # Right plot: Cost function\n    cost_df &lt;- cost_data()\n    current_point &lt;- data.frame(slope = input$slope, ssr = current_ssr())\n    \n    p2 &lt;- ggplot(cost_df, aes(x = slope, y = ssr)) +\n      geom_line(color = \"darkred\", linewidth = 1.2) +\n      geom_point(data = current_point, aes(x = slope, y = ssr), \n                 color = \"red\", size = 4) +\n      labs(title = \"Cost Function\", \n           x = \"Slope Parameter\", \n           y = \"Sum of Squared Residuals\") +\n      theme_minimal() +\n      theme(plot.title = element_text(hjust = 0.5))\n    \n    # Combine plots\n    grid.arrange(p1, p2, ncol = 2)\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "modules/shinylive-cost-function.html#shinylive-app",
    "href": "modules/shinylive-cost-function.html#shinylive-app",
    "title": "Untitled",
    "section": "",
    "text": "#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Simple data points\ndata_points &lt;- data.frame(x = c(1, 2, 3), y = c(1, 2, 3))\n\n# Define UI\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Cost Function Explorer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"slope\", \n                  \"Slope Parameter:\", \n                  min = -2, max = 4, value = 1, step = 0.1),\n      br(),\n      h4(\"Current Values:\"),\n      textOutput(\"current_slope\"),\n      textOutput(\"current_ssr\"),\n      br(),\n      p(\"Move the slider to see how the slope affects:\"),\n      tags$ul(\n        tags$li(\"The regression line (left plot)\"),\n        tags$li(\"Your position on the cost function (right plot)\"),\n        tags$li(\"The sum of squared residuals\")\n      )\n    ),\n    \n    mainPanel(\n      plotOutput(\"combined_plot\", height = \"400px\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n  \n  # Calculate SSR for current slope\n  current_ssr &lt;- reactive({\n    predictions &lt;- 0 + input$slope * data_points$x\n    residuals &lt;- data_points$y - predictions\n    sum(residuals^2)\n  })\n  \n  # Create cost function data\n  cost_data &lt;- reactive({\n    slopes &lt;- seq(-2, 4, by = 0.1)\n    ssr_values &lt;- sapply(slopes, function(b) {\n      predictions &lt;- 0 + b * data_points$x\n      residuals &lt;- data_points$y - predictions\n      sum(residuals^2)\n    })\n    data.frame(slope = slopes, ssr = ssr_values)\n  })\n  \n  output$current_slope &lt;- renderText({\n    paste(\"Slope:\", round(input$slope, 2))\n  })\n  \n  output$current_ssr &lt;- renderText({\n    paste(\"Sum of Squared Residuals:\", round(current_ssr(), 2))\n  })\n  \n  output$combined_plot &lt;- renderPlot({\n    # Left plot: Data points and regression line\n    p1 &lt;- ggplot(data_points, aes(x = x, y = y)) +\n      geom_point(size = 4, color = \"darkblue\") +\n      geom_abline(intercept = 0, slope = input$slope, color = \"red\", linewidth = 1.2) +\n      xlim(0, 4) + ylim(-2, 6) +\n      labs(title = \"Regression Line\", x = \"x\", y = \"y\") +\n      theme_minimal() +\n      theme(plot.title = element_text(hjust = 0.5))\n    \n    # Right plot: Cost function\n    cost_df &lt;- cost_data()\n    current_point &lt;- data.frame(slope = input$slope, ssr = current_ssr())\n    \n    p2 &lt;- ggplot(cost_df, aes(x = slope, y = ssr)) +\n      geom_line(color = \"darkred\", linewidth = 1.2) +\n      geom_point(data = current_point, aes(x = slope, y = ssr), \n                 color = \"red\", size = 4) +\n      labs(title = \"Cost Function\", \n           x = \"Slope Parameter\", \n           y = \"Sum of Squared Residuals\") +\n      theme_minimal() +\n      theme(plot.title = element_text(hjust = 0.5))\n    \n    # Combine plots\n    grid.arrange(p1, p2, ncol = 2)\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course: DATS 1001-Data Science for All\nCredit Hours: 3.0.\nInstructor: Prof. Emmanuel Teitelbaum\nEmail: ejt@gwu.edu\nVirtual Office Hours: By appointment only.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-information",
    "href": "course-syllabus.html#course-information",
    "title": "Syllabus",
    "section": "",
    "text": "Course: DATS 1001-Data Science for All\nCredit Hours: 3.0.\nInstructor: Prof. Emmanuel Teitelbaum\nEmail: ejt@gwu.edu\nVirtual Office Hours: By appointment only.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#learning-objectives",
    "href": "course-syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDevelop Proficiency in R Programming: Students will learn the fundamentals of R programming, including basic syntax, data manipulation, and the use of R packages, enabling them to write and execute R scripts effectively.\nMaster Data Visualization Techniques: Students will gain a deep understanding of the grammar of graphics and best practices in data visualization, allowing them to create clear, accurate, and impactful visual representations of data.\nApply Data Wrangling Techniques: Students will learn to organize, clean, and transform datasets into tidy formats using R, preparing them for analysis by applying grouping, summarizing, and joining operations.\nUnderstand and Implement Statistical Models: Students will learn the principles of statistical modeling, including linear, multiple, and logistic regression, and will apply these models to real-world data to make predictions and informed decisions.\nConduct Hypothesis Testing and Interpret Results: Students will learn the concepts of sampling, uncertainty, and hypothesis testing, enabling them to design research studies, test hypotheses, and draw valid conclusions from data.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#readings",
    "href": "course-syllabus.html#readings",
    "title": "Syllabus",
    "section": "Readings",
    "text": "Readings\nStudents will read approximately 20 pages per week of academic material during the semester.\nAll academic readings will be linked to the course website. There are no textbooks that need to be purchased for this course.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#assignments",
    "href": "course-syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\n\nQuizzes (20%)\nCoding assignments (50%)\nFinal project (30% of final grade)\n\nProposal (5%)\nData Preparation (5%)\nPresentation (20%)\n\n\n\nQuizzes\nStudents will complete approximately ten graded quizzes. These exercises will be oriented towards strengthening the student’s ability to manipulate, visualize and communicate data with R and Quarto.\n\n\nCoding Assignments\nStudents will complete five weekly coding assignments. These assignments are designed to provide students with the opportunity to apply their newly-acquired skills to real world data and to develop their ability to write and execute R scripts.\n\n\nFinal Project\nEach student will complete a final project that will be developed throughout the semester. Students will work in randomly assigned groups to complete the project. The ultimate objective of this project is to design and execute an original data analysis. Details on the project will be provided in class over the course of the semester.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-grading",
    "href": "course-syllabus.html#course-grading",
    "title": "Syllabus",
    "section": "Course Grading",
    "text": "Course Grading\nThe grading scale below maps your final point or percentage total to your final letter grade for the course.\n\n\n\nRange\nLetter Grade\n\n\n\n\n94-100\nA\n\n\n90-93\nA-\n\n\n87-89\nB+\n\n\n84-86\nB\n\n\n80-83\nB-\n\n\n77-79\nC+\n\n\n74-76\nC\n\n\n70-73\nC-\n\n\n67-69\nD+\n\n\n64-66\nD\n\n\n60-63\nD-\n\n\n&lt;59\nF",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#late-work",
    "href": "course-syllabus.html#late-work",
    "title": "Syllabus",
    "section": "Late Work",
    "text": "Late Work\nLate assignments will incur a penalty of 1 percentage point per day. Late work will not be accepted more than 7 days past the deadline. Please also note that there will be no makeups for missed quizzes.\nStudents with exceptional circumstances may receive accommodations for late work under exceptional circumstances, and in consultation with the students’ academic advisor, when a verified medical excuse written by a qualified medical professional is provided.\nAccommodations are automatically approved for university-approved absences such as sporting events, religious holidays, etc. but these should be provided to the instructor by the end of the second week of the semester.\nNo exceptions will be made to the late work policy.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-communication",
    "href": "course-syllabus.html#course-communication",
    "title": "Syllabus",
    "section": "Course Communication",
    "text": "Course Communication\n\nDiscord\nThe best way to ask questions about concepts and coding issues or anything related to the course material (including assignment requirements and due dates) is via a public post on the course Discord server. This is going to be our main communication channel for a few reasons:\n\nYou can insert code snippets and images directly into your questions, which will make it easier for us to understand your problem and provide you with a solution (whereas with email you cannot);\nOther students can see your question and the answer, which will everyone to learn from the discussion. You may think you are the only one with a question, but chances are that if you are thinking something so are others in the course;\nDiscord helps to establish a sense of community among students, and we want this course to feel like a modern, supportive and inclusive data science community.\n\nI will check the Discord at least once a day and will respond to your questions as soon as possible.\nPlease send questions about the course material via Discord. Please also do not send direct messages (DMs) or friend requests on Discord. I will only respond to public messages. Although it may be very kind to send them, friend requests will be ignored due to the need to maintain professional boundaries between students and instructors.\nThe link to the Discord server will be distributed at the start of the semester via Blackboard. Please make sure to join the server as soon as possible as the link will expire after a few days.\n\n\nOffice hours\nIf you have asked your question on Discord and could not get an answer, you can come to office hours. You can also contact me for an office hours appointment if you have a personal matter that you would like to discuss.\nTo make the meeting more effective, you can:\n\nGather materials (assignments, notes, etc.) ready in advance\nBe ready to take notes during office hours\nAsk follow up questions if you need clarification\nConfirm any action plan at the close of the meeting\nFollowing through on any action plan\n\n\n\nEmail\nEmails should be kept to a minimum in this course. But if you have tried the Discord and office hours and still have a question, you can email me. Please make sure to include “DATS 1001” in the subject line of your email. I will do my best to respond to your email within 24 hours.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#netiquette",
    "href": "course-syllabus.html#netiquette",
    "title": "Syllabus",
    "section": "Netiquette",
    "text": "Netiquette\nBehind every name there is a person.\nTo ensure safe and meaningful learning experiences for everyone, we all agree to:\n\nRemain professional, respectful, and courteous at all times on all platforms.\nKeep in mind this is a college class. Something that would be inappropriate in an in-person classroom is also inappropriate in an online classroom.\nWhen upset, we’ll wait a day or two prior to posting. Messages posted or emailed in anger are often regretted later.\nAsk one another for clarification if we find a communication offensive or difficult to understand.\nAvoid sweeping generalizations. Back up our stated opinions with facts and reliable sources.\nUnderstand that we may disagree and that exposure to other people’s opinions is part of the learning experience.\nJust as we would like our privacy respected, we will respect the privacy of other course participants and what they share.\n\nI (the instructor) reserve the right to delete any post or communication in our course that is deemed inappropriate without prior notification to the student. This includes anything containing language that is offensive, rude, profane, racist, or hateful. Items that are seriously off-topic or serve no purpose other than to vent frustration will also be removed.\nUsing outside communication apps\nI am aware that you and your peers might communicate using tools outside of GW’s Blackboard, my course website, our course Discord channel, or email systems. Rules of netiquette and appropriate communication extend to these tools as well as to Blackboard. If you see any tool being used inappropriately (i.e., any communication containing language that is offensive, rude, profane, racist, or hateful; uses that promote cheating of any kind), contact me as soon as possible to speak privately about it.\n(Adapted from Lake Superior Connect, Creative Commons Attribution 3.0)",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#policy-on-ai-tools",
    "href": "course-syllabus.html#policy-on-ai-tools",
    "title": "Syllabus",
    "section": "Policy on AI Tools",
    "text": "Policy on AI Tools\n\nOverview\nThe use of Large Language Models (LLMs) can be a valuable tool for students in this course, but it is important to use it ethically and appropriately. By following the guidelines and expectations outlined in this syllabus policy, students can maximize the benefits of using LLMs while also demonstrating their own critical thinking, research, and programming skills.\nAs part of this course, students will have the option to use ChatGPT and other LLMs to assist in the writing of their R scripts for specific course assignments. Such tools can provide students with a starting point for their code but often the code is incomplete or error-prone. It is important to recognize that LLMs are a tool and not a substitute for critical thinking, research, or programming skills. Therefore, this syllabus policy outlines the guidelines and expectations for using LLMs in this course.\n\n\nGuidelines\nIt is also important to note that some uses of LLMs are more appropriate than others. For example, it is acceptable to use LLMs to generate boilerplate code or to help with syntax, but it is not acceptable to use an LLM to complete entire assignments or assessments. Students should use LLMs as a starting point and then modify and expand on the generated code to demonstrate their own understanding and skills.\nOne way to think of LLMs is as a tutor or classmate. What would it be appropriate to ask a tutor or classmate for help with? What would you not be permitted ask a tutor or classmate for help with?",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#prerequisites",
    "href": "course-syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nAcademic\nThere are no academic prerequisites for this course.\n\n\nTechnological\n\nConfiguration and software\nTo fully participate in our course, you will need regular access to broadband Internet access as well as other technology components. Please consult GW Online’s Technical Requirements and Support for details on recommended configurations and software available to you. You will need to use the following tools and platforms:\n\nPosit Cloud: a cloud-based computing platform that allows you to run R code and render Quarto documents in the cloud.You will need to purchase a student subscription ($5/month) to access the class workspace.\nRStudio: an IDE for generating data visualizations using the programming language, R.\nQuarto: a document generation tool that allows you to create documents that combine code, data, and text.\nDiscord: a real-time chat application.\n\n\n\n\nSkills\nFor our course, you should be able to:\n\nAccess and use GW’s Blackboard system.\nUse your GW email for university-related communications per university policy.\nUse productivity software (e.g., Office 365, Google Suite) to collaborate with peers and submit assignments.\nUse web conferencing tools (e.g., Zoom, Webex) to collaborate with peers and me.\nUse a mobile device and/or computer to upload documents, images, and recordings.\nSeek technology help and tools by contacting GW Information Technology | (202)-994-4948 | ithelp@gwu.edu.\n\nIf you need assistance with technology tools we’ll use in this course, please visit the Technology Support link in the left navigation menu in our course on Blackboard.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-credit-hour-policy",
    "href": "course-syllabus.html#course-credit-hour-policy",
    "title": "Syllabus",
    "section": "Course Credit Hour Policy",
    "text": "Course Credit Hour Policy\nSummer courses are more than twice as intensive as those held during the academic year. Over 6 weeks, students will spend approximately 18.75 hours per week doing independent learning, including, but not limited to, readings, quizzes, assignments and a final project. This amounts to 36 hours of direct-instruction and 76.5 hours independent coursework.\n\nHow this applies to you\nUse the credit hour policy to plan and manage your workload and time spent on this course. Please contact me if you are having difficulty managing your workload, and we can discuss strategies to help you succeed in the course.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#demonstrating-academic-integrity",
    "href": "course-syllabus.html#demonstrating-academic-integrity",
    "title": "Syllabus",
    "section": "Demonstrating Academic Integrity",
    "text": "Demonstrating Academic Integrity\nAll of us in the course will comply with the GW Code of Academic Integrity. It states that “we, the Students, Faculty, Librarians, Staff, and Administration of The George Washington University, believing academic integrity to be central to the mission of the University, commit ourselves to promoting high standards for the integrity of academic work. Commitment to academic integrity upholds educational equity, development, and dissemination of meaningful knowledge, and mutual respect that our community values and nurtures. The George Washington University Code of Academic Integrity is established to further this commitment.”\nAcademic dishonesty is defined as cheating of any kind, including misrepresenting one’s own work, taking credit for the work of others without crediting them and without appropriate authorization, and the fabrication of information. For details and complete code, see the Code of Academic Integrity.Common examples of academic dishonesty include cheating, fabrication, plagiarism, falsification, forgery of University academic documents, and facilitating academic dishonesty by others. Learn more about avoiding these:\n\nGW guidance for students on academic integrity.\nPlagiarism: What is it and how to avoid it from GW Libraries.\nMaintaining academic honesty can be a challenging skill to learn. If you have questions about maintaining our course standards, please talk with me early on.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#aiming-for-success",
    "href": "course-syllabus.html#aiming-for-success",
    "title": "Syllabus",
    "section": "Aiming For Success",
    "text": "Aiming For Success\nI care about your learning and also about this subject matter, and I am here to help you have a meaningful learning experience. I expect you to take ownership of your learning: you can get more out of the course by thoughtfully participating in discussions, actively taking notes on readings and lectures, and giving your best effort overall. I will hold you to the highest standards for academic honesty and integrity in your work. I will also encourage you to collaborate and learn from your peers through thoughtful and respectful discussion. It is recommended you log in daily order to keep up with course requirements. I must highlight that communication is vital, so I hope you feel comfortable reaching out to me if you are struggling or have concerns or need accommodations beyond accessibility. We can determine strategies to set you up for success. Finally, I look forward to collaborating with you in this course to create a meaningful experience for everyone.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#policies",
    "href": "course-syllabus.html#policies",
    "title": "Syllabus",
    "section": "Policies",
    "text": "Policies\nTo make this a meaningful learning experience for everyone, please read and understand the following policies. All GW policies can be found on the GW Office of Ethics, Compliance, and Privacy site. All GW community members are responsible for adhering to and activating in accordance with all university policies. Please contact me if you have any questions.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#accessibility-and-accommodations",
    "href": "course-syllabus.html#accessibility-and-accommodations",
    "title": "Syllabus",
    "section": "Accessibility and Accommodations",
    "text": "Accessibility and Accommodations\n\nGW’s Disability Support Services\nIf you are a student with a disability, or think you may have a disability, you can let me know, and/or you can talk to GW’s Office of Disability Support Services (DSS). DSS works with both students with disabilities and instructors to identify reasonable accommodations. Contact the DSS office at (202) 994-8250, by email on dss@gwu.edu, or in-person in Rome Hall Suite 102 to establish eligibility and to coordinate reasonable accommodations. If you have already been approved for accommodations, please send me your accommodation letter and meet with me so we can develop an implementation plan together.\nHow are course technology tools accessible to everyone? To find out, access Technology Support Technology Tools Policies in the Blackboard course menu.\n\n\nAccommodations Beyond Disability\nEveryone has different needs for learning. If you don’t have a documented disability but feel that you would benefit from learning support for other reasons, please don’t hesitate to talk to me. If you have substantial non-academic obligations or other concerns (e.g., food insecurity, work, childcare, athletic commitments, language barriers, financial issues, technology access, commuting, etc.) that make learning difficult, please contact me. I’ll keep this information confidential, and together, we can brainstorm ways to meet your needs.\n\n\nOther Needs\nAny student who has difficulty affording groceries or accessing sufficient food to eat every day, or who lacks a safe and stable place to live, and believes this may affect their performance in the course, is urged to contact GW’s Office of Student Financial Assistance for support. Furthermore, please notify me if you are comfortable doing so. Some other resources to support you are found under the course menu item Student Resources and include support for academic achievement and personal well-being. (Adapted from Goldrick-Rab, 2017)",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#counseling-and-psychological-services",
    "href": "course-syllabus.html#counseling-and-psychological-services",
    "title": "Syllabus",
    "section": "Counseling and Psychological Services",
    "text": "Counseling and Psychological Services\nGW’s Health Center offers counseling and psychological services to GW students. Please note that staff is licensed to offer short term therapy to students in Washington, DC, Maryland, and Virginia. If you are living outside these regions, the office may be able to refer you elsewhere. Assistance and referrals 24 hours a day, 365 days a year and can be reached on (202) 994-5300.\nThe Center provides assistance and referral to address students’ personal, social, career, and study skills problems. Services for students include: crisis and emergency mental health consultations, confidential assessment, counseling services (individual and small group), and referrals.\nVirtual Workshops are open to any student regardless of geographic location. These can be exceptionally valuable and help you build essential skills and cope with common ongoing mental health concerns. Please contact the GW Health Center on (202) 994-5300 for more information.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#religious-observances",
    "href": "course-syllabus.html#religious-observances",
    "title": "Syllabus",
    "section": "Religious Observances",
    "text": "Religious Observances\nAs members of the GW community, you have the right to observe religious holidays. University policy requires that students notify their instructors during the first week of the semester if they plan to be absent from class on days of religious observance. For further details, please consult the university policy on religious holiday observance.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#key-dates",
    "href": "course-syllabus.html#key-dates",
    "title": "Syllabus",
    "section": "Key Dates",
    "text": "Key Dates\nPlease defer to the due dates listed on the course website. You can also view due dates in the gradebook and under each individual course assignment item in Blackboard Ultra.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course-syllabus.html#incomplete-grades",
    "href": "course-syllabus.html#incomplete-grades",
    "title": "Syllabus",
    "section": "Incomplete Grades",
    "text": "Incomplete Grades\n\nUndergraduate students\nIncomplete grades may be given to undergraduate students only if for reasons beyond the student’s control (such as medical or family emergency) they are unable to complete the final work of the course. Faculty should not assign an Incomplete grade if not asked by the student.\nA contract must be signed by the instructor and the student and filed in the department office. A copy should be submitted to the Academic Advising office in Phillips 107. A student has up to a calendar year to finish the coursework for the class, and when completed a grade change form must be submitted to the Academic Advising office to update the grade.\nFor further policy and contract information for undergraduate students, please consult with your advisor and also visit the website for Columbian College of Arts and Sciences Academic Advising.\n\n\nGraduate students\nIncomplete grades may be given to graduate students only if for reasons beyond the student’s control (such as medical or family emergency) they are unable to complete the final work of the course. Faculty should not assign an Incomplete grade if not asked by the student.\nFor further information, please consult with your advisor and complete a CCAS graduate student incomplete grade form.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "project/project-assignment-2.html",
    "href": "project/project-assignment-2.html",
    "title": "Project Assignment 2",
    "section": "",
    "text": "In this assignment, you will take the next step in your project by planning the type of analysis you will perform. You will scope out the specific analytical approach, write regression equations relevant to your question, and prepare your data for analysis. This assignment builds directly on the work you did in Project Assignment 1, where you identified your research question, hypotheses, and potential data sources.",
    "crumbs": [
      "Project",
      "Assignment 2"
    ]
  },
  {
    "objectID": "project/project-assignment-2.html#overview",
    "href": "project/project-assignment-2.html#overview",
    "title": "Project Assignment 2",
    "section": "",
    "text": "In this assignment, you will take the next step in your project by planning the type of analysis you will perform. You will scope out the specific analytical approach, write regression equations relevant to your question, and prepare your data for analysis. This assignment builds directly on the work you did in Project Assignment 1, where you identified your research question, hypotheses, and potential data sources.",
    "crumbs": [
      "Project",
      "Assignment 2"
    ]
  },
  {
    "objectID": "project/project-assignment-2.html#analytical-approach",
    "href": "project/project-assignment-2.html#analytical-approach",
    "title": "Project Assignment 2",
    "section": "Analytical Approach",
    "text": "Analytical Approach\nStart by briefly describing the type of analysis you plan to use to answer your question. This could include exploratory data analysis (EDA) techniques, descriptive statistics, or regression analysis. Focus on why these methods are appropriate for testing your hypotheses and addressing your research question. Consider the following:\n\nType of Analysis: Will you conduct a linear regression, logistic regression, or perhaps time-series analysis?\nJustification: Why is this approach suitable for answering your research question?\nExpected Outcomes: What will this analysis allow you to determine or predict?\n\nExample: “For the question of which emerging markets are best suited for expansion, we will perform multiple regression analysis, testing how factors like political stability, GDP growth, and trade openness predict market profitability.”\nIn your discussion, elaborate as much as possible on the type of analysis you plan to conduct. This will help you clarify your thinking and prepare for the next steps in your project.",
    "crumbs": [
      "Project",
      "Assignment 2"
    ]
  },
  {
    "objectID": "project/project-assignment-2.html#regression-equations",
    "href": "project/project-assignment-2.html#regression-equations",
    "title": "Project Assignment 2",
    "section": "Regression Equations",
    "text": "Regression Equations\nNext, translate your hypotheses into regression equations. Each equation should represent a specific relationship that you are testing in your data.\n\nDefine Your Variables: Identify the dependent and independent variables in your analysis.\nWrite the Equations: Formulate the equations that correspond to each hypothesis.\n\nFor example, if you hypothesized that political stability (measured as a stability index) positively influences foreign direct investment (FDI), your regression equation might look like:\n\\[\n\\text{FDI} = \\beta_0 + \\beta_1 \\cdot \\text{StabilityIndex} + \\beta_2 \\cdot \\text{GDPGrowth} + \\epsilon\n\\]\nProvide 2-3 equations if your research question has multiple hypotheses. This will help clarify your analytical plan. The discuss the euqations and what hypotheses they are supposed to test.",
    "crumbs": [
      "Project",
      "Assignment 2"
    ]
  },
  {
    "objectID": "project/project-assignment-2.html#data-preparation",
    "href": "project/project-assignment-2.html#data-preparation",
    "title": "Project Assignment 2",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn this section, prepare your data for analysis. List the specific steps you will take to get the data ready, including cleaning, transforming, and merging datasets if necessary. Consider the following:\n\nData Cleaning: Describe any initial steps to handle missing values, outliers, or inconsistent data.\nVariable Transformation: Indicate if any variables need to be transformed (e.g., logged, scaled) for regression analysis.\nData Merging: Describe any steps needed to combine multiple datasets, if applicable.\n\nWrite a list of tasks below that you expect to complete to prepare your data for analysis. Make it as comprehensive as possible.",
    "crumbs": [
      "Project",
      "Assignment 2"
    ]
  },
  {
    "objectID": "project/project-assignment-2.html#submission",
    "href": "project/project-assignment-2.html#submission",
    "title": "Project Assignment 2",
    "section": "Submission",
    "text": "Submission\nPlease post a document outlining your analytical approach, regression equations, and data preparation steps to your team’s workspace. Make sure that Prof. T is added as an admin to the workspace. Then, export this document and submit it via Blackboard by the due date. Each group member should submit a copy for grading purposes.",
    "crumbs": [
      "Project",
      "Assignment 2"
    ]
  },
  {
    "objectID": "project/project-datasets.html",
    "href": "project/project-datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Here are some datasets that you might consider using for your final project:\n\nfredr is an R package that provides access to the Federal Reserve Economic Data (FRED) API. FRED is a comprehensive database of economic data maintained by the Federal Reserve Bank of St. Louis. The package allows you to search for and download data from FRED directly into R.\nGoogle Public Data Explorer contains information about dozens of databases related to governance and the economy. You cannot download the raw data from Google, but you can use the site to visualize the data and then follow the link to the original source.\nILOSTAT is the statistical database of the International Labour Organisation. It has data pertaining to labor, working conditions, industrial relations, poverty and inequality.\nKaggle has data on just about anything you can think of. Very usable, clean data. Just stick to the social science stuff for your project. You can easily download CSV files from Kaggle but you can also access the data through the Kaggle API.\nOECD DATA provides data related to the performance of high income countries.\nOur World in Data is a good general resource for political economy data. The site is centered around blog posts but you can also search for a topic, view a visualization related to that topic and then download the data used to create it.\npeacesciencer is an R package maintained by Steve Miller that compiles data from a number of sources that are useful for peace and conflict studies analysis\nStatista is a good place to look for data on more niche topics.\nUNCTADstat is the United Nations Conference on Trade and Development statistical database. It provides harmonized data on a range of topics related to economic performance, trade and statistics.\nThe UN Human Development Reports include a number of important indicators related to human development, gender and sustainable development goals (SDGs).\nThe unvotes package provides data on United Nations General Assembly voting patterns.\nVarieties of Democracy (V-DEM) provides original measures of the quality of democracy for every country dating back to the 18th century. You can access vdem data through the vdemdata package.\nWorld Bank Development Indicators (WDI) is the primary World Bank database for development data from officially-recognized international sources. You can access WB Development Indicators through the WDI package or the wbstats package.\nThe World Bank DataBank provides access to dozens of additional World Bank databases on topics such as regional development, governance, education, gender and the environment. You can access world bank data through the wbstats package.\n\nFor information on more specific resources available, see this page on the Gelman Library website.",
    "crumbs": [
      "Project",
      "Datasets"
    ]
  },
  {
    "objectID": "slides/week-9.2.html#binary-outcomes-1",
    "href": "slides/week-9.2.html#binary-outcomes-1",
    "title": "Logistic Regression",
    "section": "Binary Outcomes",
    "text": "Binary Outcomes\n\n\nSo far we have looked at continuous or numerical outcomes (response variables)\nWe are often also interested in outcome variables that are binary (Yes/No, or 1/0)\n\nDid violence happen, or not?\nClassification: is this email spam?"
  },
  {
    "objectID": "slides/week-9.2.html#example-conflict-onset",
    "href": "slides/week-9.2.html#example-conflict-onset",
    "title": "Logistic Regression",
    "section": "Example: Conflict Onset",
    "text": "Example: Conflict Onset\n\n\nDid a civil war begin in a given country in a given year? (yes/no)\nPredictors: wealth, democracy, terrain, ethnic diversity, etc.\nSeminal work by Fearon and Laitin (2003)\nWe can use logistic regression to model this binary outcome"
  },
  {
    "objectID": "slides/week-9.2.html#modeling",
    "href": "slides/week-9.2.html#modeling",
    "title": "Logistic Regression",
    "section": "Modeling",
    "text": "Modeling\n\n\nWe can treat each outcome (conflict onset) as successes and failures arising from separate Bernoulli trials\nBernoulli trial: a random experiment with exactly two possible outcomes, “success” and “failure”, in which the probability of success is the same every time the experiment is conducted\nSuccess is usually coded as 1, failure as 0\nSo ironically, conflict onset is a “success” in this context"
  },
  {
    "objectID": "slides/week-9.2.html#modeling-1",
    "href": "slides/week-9.2.html#modeling-1",
    "title": "Logistic Regression",
    "section": "Modeling",
    "text": "Modeling\n\nEach Bernoulli trial can have a separate probability of success\n\n\\[ y_i ∼ Bern(p) \\]"
  },
  {
    "objectID": "slides/week-9.2.html#modeling-2",
    "href": "slides/week-9.2.html#modeling-2",
    "title": "Logistic Regression",
    "section": "Modeling",
    "text": "Modeling\n\n\nWe can then use the predictor variables to model that probability of success, \\(p_i\\)\nWe can’t really use a linear model for \\(p_i\\) (since \\(p_i\\) must be between 0 and 1) but we can transform the linear model to have the appropriate range"
  },
  {
    "objectID": "slides/week-9.2.html#generalized-linear-models",
    "href": "slides/week-9.2.html#generalized-linear-models",
    "title": "Logistic Regression",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\n\nThis is a very general way of addressing many problems in regression and the resulting models are called generalized linear models (GLMs)\nLogistic regression is a very common example"
  },
  {
    "objectID": "slides/week-9.2.html#glms",
    "href": "slides/week-9.2.html#glms",
    "title": "Logistic Regression",
    "section": "GLMs",
    "text": "GLMs\n\nAll GLMs have the following three characteristics:\n\nA probability distribution describing a generative model for the outcome variable\nA linear model: \\[\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\\]\nA link function that relates the linear model to the parameter of the outcome distribution"
  },
  {
    "objectID": "slides/week-9.2.html#logistic-regression",
    "href": "slides/week-9.2.html#logistic-regression",
    "title": "Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\nLogistic regression is a GLM used to model a binary categorical outcome (0 or 1)\nIn logistic regression, the link function that connects \\(\\eta_i\\) to \\(p_i\\) is the logit function\nLogit function: For \\(0\\le p \\le 1\\)\n\n\\[logit(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]"
  },
  {
    "objectID": "slides/week-9.2.html#logit-function",
    "href": "slides/week-9.2.html#logit-function",
    "title": "Logistic Regression",
    "section": "Logit Function",
    "text": "Logit Function"
  },
  {
    "objectID": "slides/week-9.2.html#logistic-regression-model",
    "href": "slides/week-9.2.html#logistic-regression-model",
    "title": "Logistic Regression",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\n\n\n\\(y_i \\sim \\text{Bern}(p_i)\\)\n\\(\\eta_i = \\beta_0+ \\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}\\)\n\\(\\text{logit}(p_i) = \\eta_i\\)"
  },
  {
    "objectID": "slides/week-9.2.html#logistic-regression-model-1",
    "href": "slides/week-9.2.html#logistic-regression-model-1",
    "title": "Logistic Regression",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\n\n\n\\(\\text{logit}(p_i) = \\eta_i = \\beta_0+ \\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}\\)\nNow take inverse logit to get \\(p\\)\n\n\\[p_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}{1+\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}\\]"
  },
  {
    "objectID": "slides/week-9.2.html#the-peacesciencer-package",
    "href": "slides/week-9.2.html#the-peacesciencer-package",
    "title": "Logistic Regression",
    "section": "The peacesciencer Package",
    "text": "The peacesciencer Package\n\nThe peacesciencer package provides a number of datasets and functions for analyzing conflict and peace\nProvides data from a number of important datasets in the field of conflict studies, e.g.\n\nCorrelates of War (CoW) project\nUppsala Conflict Data Program (UCDP)\nMilitarized Interstate Dispute (MID) dataset\n\nProvides functions for analyzing conflict and adding control variables to the dataset"
  },
  {
    "objectID": "slides/week-9.2.html#using-the-peacesciencer-package",
    "href": "slides/week-9.2.html#using-the-peacesciencer-package",
    "title": "Logistic Regression",
    "section": "Using the peacesciencer Package",
    "text": "Using the peacesciencer Package\n\n\nlibrary(peacesciencer)\nlibrary(tidymodels)\n\nconflict_df &lt;- create_stateyears(system = 'gw') |&gt;\n  filter(year %in% c(1946:1999)) |&gt;\n  add_ucdp_acd(type=c(\"intrastate\"), only_wars = FALSE) |&gt;\n  add_democracy() |&gt;\n  add_creg_fractionalization() |&gt;\n  add_sdp_gdp() |&gt;\n  add_rugged_terrain()\n\nglimpse(conflict_df)\n\nRows: 7,036\nColumns: 20\n$ gwcode         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ statename      &lt;chr&gt; \"United States of America\", \"United States of America\",…\n$ year           &lt;dbl&gt; 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1…\n$ ucdpongoing    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ucdponset      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ maxintensity   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ conflict_ids   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ v2x_polyarchy  &lt;dbl&gt; 0.605, 0.587, 0.599, 0.599, 0.587, 0.602, 0.601, 0.594,…\n$ polity2        &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,…\n$ xm_qudsest     &lt;dbl&gt; 1.259180, 1.259180, 1.252190, 1.252190, 1.270106, 1.259…\n$ ethfrac        &lt;dbl&gt; 0.2226323, 0.2248701, 0.2271561, 0.2294918, 0.2318781, …\n$ ethpol         &lt;dbl&gt; 0.4152487, 0.4186156, 0.4220368, 0.4255134, 0.4290458, …\n$ relfrac        &lt;dbl&gt; 0.4980802, 0.5009111, 0.5037278, 0.5065309, 0.5093204, …\n$ relpol         &lt;dbl&gt; 0.7769888, 0.7770017, 0.7770303, 0.7770729, 0.7771274, …\n$ wbgdp2011est   &lt;dbl&gt; 28.539, 28.519, 28.545, 28.534, 28.572, 28.635, 28.669,…\n$ wbpopest       &lt;dbl&gt; 18.744, 18.756, 18.781, 18.804, 18.821, 18.832, 18.848,…\n$ sdpest         &lt;dbl&gt; 28.478, 28.456, 28.483, 28.469, 28.510, 28.576, 28.611,…\n$ wbgdppc2011est &lt;dbl&gt; 9.794, 9.762, 9.764, 9.730, 9.752, 9.803, 9.821, 9.857,…\n$ rugged         &lt;dbl&gt; 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073, 1.073,…\n$ newlmtnest     &lt;dbl&gt; 3.214868, 3.214868, 3.214868, 3.214868, 3.214868, 3.214…"
  },
  {
    "objectID": "slides/week-9.2.html#running-a-logistic-regression",
    "href": "slides/week-9.2.html#running-a-logistic-regression",
    "title": "Logistic Regression",
    "section": "Running a Logistic Regression",
    "text": "Running a Logistic Regression\n\n\nImplementation is not very different from a linear model\nWe just need to update our code to run a GLM\n\nspecify the model with logistic_reg()\nuse \"glm\" instead of \"lm\" as the engine\n\ndefine family = \"binomial\" for the link function to be used in the model"
  },
  {
    "objectID": "slides/week-9.2.html#bivariate-logistic-regression",
    "href": "slides/week-9.2.html#bivariate-logistic-regression",
    "title": "Logistic Regression",
    "section": "Bivariate Logistic Regression",
    "text": "Bivariate Logistic Regression\n\n\nconflict_model &lt;- glm(ucdponset ~ wbgdppc2011est,\n                  data= conflict_df,\n                  family = \"binomial\")\n\nsummary(conflict_model)\n\n\nCall:\nglm(formula = ucdponset ~ wbgdppc2011est, family = \"binomial\", \n    data = conflict_df)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -1.16241    0.42606  -2.728  0.00637 ** \nwbgdppc2011est -0.33082    0.05261  -6.289  3.2e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1389.6  on 7035  degrees of freedom\nResidual deviance: 1358.4  on 7034  degrees of freedom\nAIC: 1362.4\n\nNumber of Fisher Scoring iterations: 7"
  },
  {
    "objectID": "slides/week-9.2.html#interpreting-the-results",
    "href": "slides/week-9.2.html#interpreting-the-results",
    "title": "Logistic Regression",
    "section": "Interpreting the Results",
    "text": "Interpreting the Results\n\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.16-0.33\\times \\text{logGDPpc}\\]"
  },
  {
    "objectID": "slides/week-9.2.html#interpreting-the-results-1",
    "href": "slides/week-9.2.html#interpreting-the-results-1",
    "title": "Logistic Regression",
    "section": "Interpreting the Results",
    "text": "Interpreting the Results\n\n\nFor a quick interpretation of the coefficients, we can exponentiate them\nThe exponentiated coefficient is the odds ratio\nFor each one-unit increase in the independent variable, the odds of the outcome occurring increase (or decrease) by a factor of the exponentiated coefficient"
  },
  {
    "objectID": "slides/week-9.2.html#interpreting-the-results-2",
    "href": "slides/week-9.2.html#interpreting-the-results-2",
    "title": "Logistic Regression",
    "section": "Interpreting the Results",
    "text": "Interpreting the Results\n\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.16-0.33\\times \\text{logGDPpc}\\]\n\nFor each one unit increase in log GDP per capita, the odds of the outcome occurring are multiplied by approximately 0.718, assuming other variables in the model are held constant.\n\nThis means that an increase in GDP per capita is associated with a decrease in the odds of the outcome occurring. The odds of the outcome decrease by about 28.1% for each unit increase in GDP per capita (on average)."
  },
  {
    "objectID": "slides/week-9.2.html#your-turn",
    "href": "slides/week-9.2.html#your-turn",
    "title": "Logistic Regression",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nRun a bivariate logistic regression using ucdp onset as the outcome variable\nFirst replicate the results using GDP per capita as the predictor\nNow try a different predictor\nInterpret the results\n\nWhat is the average effect of the predictor on conflict onset?\nHow do you interpret that effect in terms of the odds of conflict onset?\n\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-9.2.html#calculating-predicted-probabilities",
    "href": "slides/week-9.2.html#calculating-predicted-probabilities",
    "title": "Logistic Regression",
    "section": "Calculating Predicted Probabilities",
    "text": "Calculating Predicted Probabilities\n\nProbability of conflict onset for a country with a log per capita GDP of 9 (about $8,000):\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.16-0.33\\times 9\\] \\[\\log\\left(\\frac{p}{1-p}\\right) = -4.13\\]\n\\[\\frac{p}{1-p} = \\exp(-4.13)\\]\n\\[\\frac{p}{1-p} = 0.016\\]\n\\[p = 0.016 \\times (1 - p)\\] \\[p = 0.016 - 0.016p\\]\n\\[1.016p = 0.016\\] \\[p = 0.016 / 1.016\\] \\[p = 0.0158\\]"
  },
  {
    "objectID": "slides/week-9.2.html#using-marginaleffects",
    "href": "slides/week-9.2.html#using-marginaleffects",
    "title": "Logistic Regression",
    "section": "Using marginaleffects",
    "text": "Using marginaleffects\n\n# load the marginaleffects library\nlibrary(marginaleffects)\n\n# select some countries for a given year\nselected_countries &lt;- conflict_df |&gt;\n  filter(\n    statename %in% c(\"United States of America\", \"Venezuela\", \"Rwanda\"),\n    year == 1999)\n\n# calculate margins for the subset\nmarg_effects &lt;- predictions(conflict_model, newdata = selected_countries)\n\n# view the results\nlibrary(broom)\n\ntidy(marg_effects) |&gt;\n  select(statename, estimate, p.value, conf.low, conf.high)"
  },
  {
    "objectID": "slides/week-9.2.html#using-marginaleffects-1",
    "href": "slides/week-9.2.html#using-marginaleffects-1",
    "title": "Logistic Regression",
    "section": "Using marginaleffects",
    "text": "Using marginaleffects\n\n\n\n# A tibble: 3 × 5\n  statename                estimate   p.value conf.low conf.high\n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 United States of America  0.00853 4.20e-161  0.00606    0.0120\n2 Venezuela                 0.0141  0          0.0113     0.0175\n3 Rwanda                    0.0311  1.36e-250  0.0256     0.0377"
  },
  {
    "objectID": "slides/week-9.2.html#your-turn-1",
    "href": "slides/week-9.2.html#your-turn-1",
    "title": "Logistic Regression",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nSelect your favorite three countries and a recent year\nCalculate the predicted proability of conflict onset for that year using the marginal effects package\nIf you have time, try to do the calcualation by hand as well\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-9.2.html#what-is-an-odds-ratio",
    "href": "slides/week-9.2.html#what-is-an-odds-ratio",
    "title": "Logistic Regression",
    "section": "What is an Odds Ratio?",
    "text": "What is an Odds Ratio?\n\nDefinition: An odds ratio (OR) is a measure of association between a predictor variable and the outcome, showing how the odds of the outcome change with a one-unit increase in the predictor.\nInterpretation:\n\nOR &gt; 1: The odds of the outcome increase as the predictor increases.\nOR &lt; 1: The odds of the outcome decrease as the predictor increases."
  },
  {
    "objectID": "slides/week-9.2.html#examples",
    "href": "slides/week-9.2.html#examples",
    "title": "Logistic Regression",
    "section": "Examples",
    "text": "Examples\n\n\nOR = 1.5: For each one-unit increase in the predictor, the odds of the outcome increase by 50%.\nOR = 0.7: For each one-unit increase in the predictor, the odds of the outcome decrease by 30%."
  },
  {
    "objectID": "slides/week-9.2.html#our-conflict-model",
    "href": "slides/week-9.2.html#our-conflict-model",
    "title": "Logistic Regression",
    "section": "Our Conflict Model",
    "text": "Our Conflict Model\n\n\n\nVariable\nLogit Coefficient\nOdds Ratio\n\n\n\n\n(Intercept)\n-5.3342\n0.0048\n\n\nv2x_polyarchy\n-0.6304\n0.5325\n\n\nethfrac\n0.7615\n2.1418\n\n\nrelfrac\n-0.4569\n0.6332\n\n\nwbpopest\n0.2851\n1.3298\n\n\nwbgdppc2011est\n-0.3826\n0.6821"
  },
  {
    "objectID": "slides/week-9.2.html#interpretation-of-odds-ratios",
    "href": "slides/week-9.2.html#interpretation-of-odds-ratios",
    "title": "Logistic Regression",
    "section": "Interpretation of Odds Ratios",
    "text": "Interpretation of Odds Ratios\n\n(Intercept): An OR of 0.0048 indicates the baseline odds of (y) when all predictors are zero.\nv2x_polyarchy: An OR of 0.5325 implies that for each one-unit increase in v2x_polyarchy, the odds of (y) decrease by approximately 46.8%.\nethfrac: An OR of 2.1418 indicates that for each one-unit increase in ethfrac, the odds of (y) increase by approximately 114.2%.\nrelfrac: An OR of 0.6332 suggests that for each one-unit increase in relfrac, the odds of (y) decrease by approximately 36.7%.\nwbpopest: An OR of 1.3298 implies that for each one-unit increase in wbpopest, the odds of (y) increase by about 32.98%.\nwbgdppc2011est: An OR of 0.6821 indicates that for each one-unit increase in wbgdppc2011est, the odds of (y) decrease by about 31.8%."
  },
  {
    "objectID": "slides/week-8.1.html#modeling",
    "href": "slides/week-8.1.html#modeling",
    "title": "Linear Regression",
    "section": "Modeling",
    "text": "Modeling\n\nUse models to explore the relationship between variables and to make predictions\nExplaining relationships (usually interested in causal relationships, but not always)\n\nDoes oil wealth impact regime type?\n\nPredictive modeling\n\nWhere is violence most likely to happen in (country X) during their next election?\nIs this email spam?"
  },
  {
    "objectID": "slides/week-8.1.html#modeling-1",
    "href": "slides/week-8.1.html#modeling-1",
    "title": "Linear Regression",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "slides/week-8.1.html#modeling-2",
    "href": "slides/week-8.1.html#modeling-2",
    "title": "Linear Regression",
    "section": "Modeling",
    "text": "Modeling"
  },
  {
    "objectID": "slides/week-8.1.html#pull-in-the-vdem-data",
    "href": "slides/week-8.1.html#pull-in-the-vdem-data",
    "title": "Linear Regression",
    "section": "Pull in the VDEM Data",
    "text": "Pull in the VDEM Data\n\n\nlibrary(vdemlite)\n\nmodel_data &lt;- fetchdem(indicators = c(\"v2x_libdem\", \"e_gdppc\"),\n                       start_year = 2019, end_year = 2019) |&gt;\n  rename(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc\n    ) \n\nglimpse(model_data)"
  },
  {
    "objectID": "slides/week-8.1.html#plot-the-data",
    "href": "slides/week-8.1.html#plot-the-data",
    "title": "Linear Regression",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "slides/week-8.1.html#plot-the-data-1",
    "href": "slides/week-8.1.html#plot-the-data-1",
    "title": "Linear Regression",
    "section": "Plot the Data",
    "text": "Plot the Data\n\n\nggplot(model_data, aes(x = wealth, y = lib_dem)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  scale_x_log10(label = scales::label_dollar(suffix = \"k\")) +\n  labs(\n    title = \"Wealth and Democracy, 2019\",\n    x = \"GPD per capita\", \n    y = \"Liberal Democracy Index\") +\n  theme_bw()"
  },
  {
    "objectID": "slides/week-8.1.html#models-as-functions",
    "href": "slides/week-8.1.html#models-as-functions",
    "title": "Linear Regression",
    "section": "Models as Functions",
    "text": "Models as Functions\n\nWe can represent relationships between variables using functions\nA function is a mathematical concept: the relationship between an output and one or more inputs\n\nPlug in the inputs and receive back the output\n\nExample: The formula \\(y = 3x + 7\\) is a function with input \\(x\\) and output \\(y\\).\n\nIf \\(x\\) is \\(5\\), \\(y\\) is \\(22\\),\n\\(y = 3 \\times 5 + 7 = 22\\)"
  },
  {
    "objectID": "slides/week-8.1.html#quant-lingo",
    "href": "slides/week-8.1.html#quant-lingo",
    "title": "Linear Regression",
    "section": "Quant Lingo",
    "text": "Quant Lingo\n\n\nResponse variable: Variable whose behavior or variation you are trying to understand, on the y-axis in the plot\n\nDependent variable\nOutcome variable\nY variable\n\nExplanatory variables: Other variables that you want to use to explain the variation in the response, on the x-axis in the plot\n\nIndependent variables\nPredictors"
  },
  {
    "objectID": "slides/week-8.1.html#section",
    "href": "slides/week-8.1.html#section",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear model with one explanatory variable…\n\n\\(Y = a + bX\\)\n\\(Y\\) is the outcome variable\n\\(X\\) is the explanatory variable\n\\(a\\) is the intercept: the predicted value of \\(Y\\) when \\(X\\) is equal to 0\n\\(b\\) is the slope of the line (rise over run)"
  },
  {
    "objectID": "slides/week-8.1.html#quant-lingo-1",
    "href": "slides/week-8.1.html#quant-lingo-1",
    "title": "Linear Regression",
    "section": "Quant Lingo",
    "text": "Quant Lingo\n\n\nPredicted value: Output of the model function\n\nThe model function gives the typical (expected) value of the response variable conditioning on the explanatory variables\nWe often call this \\(\\hat{Y}\\) to differentiate the predicted value from an observed value of Y in the data\n\nResiduals: A measure of how far each case is from its predicted value (based on a particular model)\n\nResidual = Observed value (\\(Y\\)) - Predicted value (\\(\\hat{Y}\\))\nHow far above/below the expected value each case is"
  },
  {
    "objectID": "slides/week-8.1.html#section-1",
    "href": "slides/week-8.1.html#section-1",
    "title": "Linear Regression",
    "section": "",
    "text": "Caution\n\n\nNote that for the next few examples we will be analyzing GDP per capita on a log scale."
  },
  {
    "objectID": "slides/week-8.1.html#residuals",
    "href": "slides/week-8.1.html#residuals",
    "title": "Linear Regression",
    "section": "Residuals",
    "text": "Residuals"
  },
  {
    "objectID": "slides/week-8.1.html#linear-model",
    "href": "slides/week-8.1.html#linear-model",
    "title": "Linear Regression",
    "section": "Linear Model",
    "text": "Linear Model\n\\(\\hat{Y} = a  + b \\times X\\)\n\\(\\hat{Y} = 0.13  + 0.12 \\times X\\)"
  },
  {
    "objectID": "slides/week-8.1.html#linear-model-interpretation",
    "href": "slides/week-8.1.html#linear-model-interpretation",
    "title": "Linear Regression",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation\n\n\\(\\hat{Y} = a  + b \\times X\\)\n\\(\\hat{Y} = 0.13  + 0.12 \\times X\\)\nWhat is the interpretation of our estimate of \\(a\\)?\n\n\n\\(\\hat{Y} = 0.13  + 0.12 \\times 0\\)\n\\(\\hat{Y} = 0.13\\)\n\\(a\\) is our predicted level of democracy when GDP per capita is 0."
  },
  {
    "objectID": "slides/week-8.1.html#linear-model-interpretation-1",
    "href": "slides/week-8.1.html#linear-model-interpretation-1",
    "title": "Linear Regression",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation\n\n\\(\\hat{Y} = a  + b \\times X\\)\n\\(\\hat{Y} = 0.13  + 0.12 \\times X\\)\nWhat is interpretation of our estimate of \\(b\\)?\n\n\n\\(\\hat{Y} = a  + \\frac{Rise}{Run} \\times X\\)\n\\(\\hat{Y} = a  + \\frac{Change Y}{Change X} \\times X\\)"
  },
  {
    "objectID": "slides/week-8.1.html#linear-model-interpretation-2",
    "href": "slides/week-8.1.html#linear-model-interpretation-2",
    "title": "Linear Regression",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation\n\n\\(b = \\frac{Change Y}{Change X}\\)\n\\(0.12 = \\frac{Change Y}{Change X}\\)\n\\({Change Y} = 0.12 * {ChangeX}\\)\n\n\nWhen \\(ChangeX = 1\\):\n\\({Change Y = 0.12}\\)\n\n\n\n\\(b\\) is the predicted change in \\(Y\\) associated with a ONE unit change in X."
  },
  {
    "objectID": "slides/week-8.1.html#linear-model-interpretation-3",
    "href": "slides/week-8.1.html#linear-model-interpretation-3",
    "title": "Linear Regression",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation"
  },
  {
    "objectID": "slides/week-8.1.html#linear-model-interpretation-4",
    "href": "slides/week-8.1.html#linear-model-interpretation-4",
    "title": "Linear Regression",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation"
  },
  {
    "objectID": "slides/week-8.1.html#linear-model-interpretation-5",
    "href": "slides/week-8.1.html#linear-model-interpretation-5",
    "title": "Linear Regression",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation"
  },
  {
    "objectID": "slides/week-8.1.html#interpreting-the-coefficient-for-logwealth",
    "href": "slides/week-8.1.html#interpreting-the-coefficient-for-logwealth",
    "title": "Linear Regression",
    "section": "Interpreting the Coefficient for log(Wealth)",
    "text": "Interpreting the Coefficient for log(Wealth)\n\\[\n\\text{Democracy} = 0.12 \\times \\log(\\text{Wealth})\n\\]\nFor small percentage changes in a “log-linear” model, we can use the approximation rule (divide by 100):\n\nA 1% increase in GDP per capita is associated with a 0.0012 point increase in the democracy score.\n\nRule: \\(0.12 / 100 = 0.0012\\)"
  },
  {
    "objectID": "slides/week-8.1.html#for-larger-changes-we-log-the-change",
    "href": "slides/week-8.1.html#for-larger-changes-we-log-the-change",
    "title": "Linear Regression",
    "section": "For Larger Changes We Log the Change",
    "text": "For Larger Changes We Log the Change\n\n\nA 10% increase (e.g., from $10,000 to $11,000) increases the democracy score by 0.0114 points since:\n\\[\n0.12 \\times \\ln(1.1) \\approx 0.12 \\times 0.0953 = 0.0114\n\\]"
  },
  {
    "objectID": "slides/week-8.1.html#similarly",
    "href": "slides/week-8.1.html#similarly",
    "title": "Linear Regression",
    "section": "Similarly…",
    "text": "Similarly…\n\nDoubling GDP per capita (e.g., $10,000 → $20,000) increases the democracy score by:\n\\[\n0.12 \\times \\ln(2) \\approx 0.12 \\times 0.693 = 0.083\n\\]\nTripling GDP per capita (e.g., $10,000 → $30,000) increases the democracy score by:\n\\[\n0.12 \\times \\ln(3) \\approx 0.12 \\times 1.099 = 0.132\n\\]"
  },
  {
    "objectID": "slides/week-8.1.html#linear-model-interpretation-6",
    "href": "slides/week-8.1.html#linear-model-interpretation-6",
    "title": "Linear Regression",
    "section": "Linear Model: Interpretation",
    "text": "Linear Model: Interpretation\n\nIs this the causal effect of GDP per capita on liberal democracy?\n\n\nNo! It is only the association…\n\n\n\nTo identify causality we need other methods (beyond the scope of this course)."
  },
  {
    "objectID": "slides/week-8.1.html#your-task",
    "href": "slides/week-8.1.html#your-task",
    "title": "Linear Regression",
    "section": "Your Task",
    "text": "Your Task\n\nAn economist is interested in the relationship between years of education and hourly wages. They estimate a linear model with estimates of \\(a\\) and \\(b\\) as follows:\n\n\\(\\hat{Y} = 9 + 1.60*{YrsEdu}\\)\n\n1. Interpret \\(a\\) and \\(b\\)\n2. What is the predicted hourly wage for those with 10 years of education?\n3. How about for those with a high school diploma? (12 yrs)\n4. What about a college degree? (16 yrs)"
  },
  {
    "objectID": "slides/week-8.1.html#next-step",
    "href": "slides/week-8.1.html#next-step",
    "title": "Linear Regression",
    "section": "Next step",
    "text": "Next step\n\n\nLinear model with one predictor: \\(Y = a + bX\\)\nFor any given data…\nHow do we figure out what the best values are for \\(a\\) and \\(b\\)??"
  },
  {
    "objectID": "slides/week-8.1.html#linear-model-with-single-predictor",
    "href": "slides/week-8.1.html#linear-model-with-single-predictor",
    "title": "Linear Regression",
    "section": "Linear Model with Single Predictor",
    "text": "Linear Model with Single Predictor\n\nGoal: Estimate Democracy score (\\(\\hat{Y_{i}}\\)) of a country given level of GDP per capita (\\(X_{i}\\)).\n\nOr: Estimate relationship between GDP per capita and democracy."
  },
  {
    "objectID": "slides/week-8.1.html#linear-model-with-single-predictor-1",
    "href": "slides/week-8.1.html#linear-model-with-single-predictor-1",
    "title": "Linear Regression",
    "section": "Linear Model with Single Predictor",
    "text": "Linear Model with Single Predictor"
  },
  {
    "objectID": "slides/week-8.1.html#estimate-model",
    "href": "slides/week-8.1.html#estimate-model",
    "title": "Linear Regression",
    "section": "Estimate Model",
    "text": "Estimate Model\n\nmodel1 &lt;-  lm(lib_dem ~ log_wealth, data = modelData) \n\nsummary(model1)\n\n\nCall:\nlm(formula = lib_dem ~ log_wealth, data = modelData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog_wealth   0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.2805,    Adjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14"
  },
  {
    "objectID": "slides/week-8.1.html#section-2",
    "href": "slides/week-8.1.html#section-2",
    "title": "Linear Regression",
    "section": "",
    "text": "In equation form… How do we interpret the model?\n\n\\[\\widehat{Democracy}_{i} = 0.13 + 0.12 * {loggdppc}_{i}\\]"
  },
  {
    "objectID": "slides/week-8.1.html#question",
    "href": "slides/week-8.1.html#question",
    "title": "Linear Regression",
    "section": "Question",
    "text": "Question\n\nHow do we get the “best” values for the slope and intercept?"
  },
  {
    "objectID": "slides/week-8.1.html#how-would-you-draw-the-best-line",
    "href": "slides/week-8.1.html#how-would-you-draw-the-best-line",
    "title": "Linear Regression",
    "section": "How would you draw the “best” line?",
    "text": "How would you draw the “best” line?"
  },
  {
    "objectID": "slides/week-8.1.html#how-would-you-draw-the-best-line-1",
    "href": "slides/week-8.1.html#how-would-you-draw-the-best-line-1",
    "title": "Linear Regression",
    "section": "How would you draw the “best” line?",
    "text": "How would you draw the “best” line?"
  },
  {
    "objectID": "slides/week-8.1.html#least-squares-regression",
    "href": "slides/week-8.1.html#least-squares-regression",
    "title": "Linear Regression",
    "section": "Least squares regression",
    "text": "Least squares regression\n\n\nRemember the residual is the difference between the actual value and the predicted value\n\n\n\nThe regression line minimizes the sum of squared residuals."
  },
  {
    "objectID": "slides/week-8.1.html#least-squares-regression-1",
    "href": "slides/week-8.1.html#least-squares-regression-1",
    "title": "Linear Regression",
    "section": "Least squares regression",
    "text": "Least squares regression\n\n\nResidual for each point is: \\(e_i = y_i - \\hat{y}_i\\)\nLeast squares regression line minimizes \\(\\sum_{i = 1}^n e_i^2\\).\n\n\n\nWhy do we square the residual?\n\n\n\n\nWhy not take absolute value?\n\nPrinciple: larger penalty for residuals further away\nMath: makes the math easier and some nice properties (not our concern here…)"
  },
  {
    "objectID": "slides/week-8.1.html#least-squares-regression-2",
    "href": "slides/week-8.1.html#least-squares-regression-2",
    "title": "Linear Regression",
    "section": "Least squares regression",
    "text": "Least squares regression"
  },
  {
    "objectID": "slides/week-8.1.html#very-simple-example",
    "href": "slides/week-8.1.html#very-simple-example",
    "title": "Linear Regression",
    "section": "Very Simple Example",
    "text": "Very Simple Example\nWhat should the slope and intercept be?"
  },
  {
    "objectID": "slides/week-8.1.html#example",
    "href": "slides/week-8.1.html#example",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\n\\(\\hat{Y} = 0 + 1*X\\)"
  },
  {
    "objectID": "slides/week-8.1.html#example-1",
    "href": "slides/week-8.1.html#example-1",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nWhat is the sum of squared residuals?"
  },
  {
    "objectID": "slides/week-8.1.html#example-2",
    "href": "slides/week-8.1.html#example-2",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nWhat is sum of squared residuals for \\(y = 0 + 0*X\\)?"
  },
  {
    "objectID": "slides/week-8.1.html#example-3",
    "href": "slides/week-8.1.html#example-3",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nWhat is sum of squared residuals for \\(y = 0 + 0*X\\)?\n\n\n(1-0)^2 + (2-0)^2 + (3-0)^2\n\n[1] 14"
  },
  {
    "objectID": "slides/week-8.1.html#example-4",
    "href": "slides/week-8.1.html#example-4",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nWhat is sum of squared residuals for \\(y = 0 + 2*X\\)?"
  },
  {
    "objectID": "slides/week-8.1.html#example-5",
    "href": "slides/week-8.1.html#example-5",
    "title": "Linear Regression",
    "section": "Example",
    "text": "Example\nWhat is sum of squared residuals for \\(y = 0 + 2*X\\)?\n\n\n(1-2)^2 + (2-4)^2 + (3-6)^2\n\n[1] 14"
  },
  {
    "objectID": "slides/week-8.1.html#one-more",
    "href": "slides/week-8.1.html#one-more",
    "title": "Linear Regression",
    "section": "One more…",
    "text": "One more…\nWhat is sum of squared residuals for \\(y = 0 + -1*X\\)?"
  },
  {
    "objectID": "slides/week-8.1.html#one-more-1",
    "href": "slides/week-8.1.html#one-more-1",
    "title": "Linear Regression",
    "section": "One more…",
    "text": "One more…\nWhat is sum of squared residuals for \\(y = 0 + -1*X\\)?\n\n\n(1+1)^2 + (2+2)^2 + (3+3)^2\n\n[1] 56"
  },
  {
    "objectID": "slides/week-8.1.html#cost-function",
    "href": "slides/week-8.1.html#cost-function",
    "title": "Linear Regression",
    "section": "Cost Function",
    "text": "Cost Function\nSum of Squared Residuals as function of possible values of \\(b\\)"
  },
  {
    "objectID": "slides/week-8.1.html#least-squares-regression-3",
    "href": "slides/week-8.1.html#least-squares-regression-3",
    "title": "Linear Regression",
    "section": "Least Squares Regression",
    "text": "Least Squares Regression\n\n\nWhen we estimate a least squares regression, it is looking for the line that minimizes sum of squared residuals\nIn the simple example, I set \\(a=0\\) to make it easier. More complicated when searching for combination of \\(a\\) and \\(b\\) that minimize, but same basic idea"
  },
  {
    "objectID": "slides/week-8.1.html#least-squares-regression-4",
    "href": "slides/week-8.1.html#least-squares-regression-4",
    "title": "Linear Regression",
    "section": "Least Squares Regression",
    "text": "Least Squares Regression\n\n\nThere is a way to solve for this analytically for linear regression (i.e., by doing math…)\n– They made us do this in grad school…\n\n\n\nIn machine learning, people also use gradient descent algorithm in which the computer searches over possible combinations of \\(a\\) and \\(b\\) until it settles on the lowest point."
  },
  {
    "objectID": "slides/week-8.1.html#least-squares-regression-5",
    "href": "slides/week-8.1.html#least-squares-regression-5",
    "title": "Linear Regression",
    "section": "Least Squares Regression",
    "text": "Least Squares Regression"
  },
  {
    "objectID": "slides/week-8.1.html#least-squares-regression-6",
    "href": "slides/week-8.1.html#least-squares-regression-6",
    "title": "Linear Regression",
    "section": "Least Squares Regression",
    "text": "Least Squares Regression"
  },
  {
    "objectID": "slides/week-6.1.html#downloading-v-dem-data",
    "href": "slides/week-6.1.html#downloading-v-dem-data",
    "title": "The Grammar of Data Wrangling",
    "section": "Downloading V-Dem Data",
    "text": "Downloading V-Dem Data\n\nThe vdem function from vdemdata just downloads all of the data. Try running this code chunk. What do you see in democracy?\n\n\nlibrary(vdemdata) # load the V-Dem package\n\ndemocracy &lt;- vdem() # download the V-Dem dataset\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/week-6.1.html#filter",
    "href": "slides/week-6.1.html#filter",
    "title": "The Grammar of Data Wrangling",
    "section": "filter()",
    "text": "filter()\n\nRun this code. What do you see?\nTry changing the year\nFor one year, use == instead of &gt;=\nOr try &lt;= and see what happens\n\n\n\ndemocracy &lt;- vdem |&gt; # download the V-Dem dataset\n  filter(year &gt;= 1990) # filter out years less than 1990\n  \nglimpse(democracy)  \n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/week-6.1.html#versus",
    "href": "slides/week-6.1.html#versus",
    "title": "The Grammar of Data Wrangling",
    "section": "= versus ==",
    "text": "= versus ==\n\n\n= is used to assign values to variables, just like &lt;-\n== is used to test if two values are equal to each other\nSo filter(year == 1990) will give you just the observations for 1990"
  },
  {
    "objectID": "slides/week-6.1.html#and",
    "href": "slides/week-6.1.html#and",
    "title": "The Grammar of Data Wrangling",
    "section": ">= and <=",
    "text": "&gt;= and &lt;=\n\n&gt;= is used to test if a value is greater than or equal to another value\n&lt;= is used to test if a value is less than or equal to another value\nSo filter(year &gt;= 1990) will give you the observations for 1990 and later\nAnd filter(year &lt;= 1990) will give you the observations for 1990 and earlier"
  },
  {
    "objectID": "slides/week-6.1.html#select",
    "href": "slides/week-6.1.html#select",
    "title": "The Grammar of Data Wrangling",
    "section": "select()",
    "text": "select()\n\nRun this code. What do you see?\nNow try v2x_libdem instead of v2x_polyarchy\nChoose more from the codebook\n\n\ndemocracy &lt;- vdem |&gt; # download the V-Dem dataset\n  select(                  # select (and rename) these variables\n    country = country_name,     # before the = sign is new name  \n    vdem_ctry_id = country_id,  # after the = sign is the old name\n    year, \n    polyarchy = v2x_polyarchy\n  )\n  \nglimpse(democracy)  \n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/week-6.1.html#mutate",
    "href": "slides/week-6.1.html#mutate",
    "title": "The Grammar of Data Wrangling",
    "section": "mutate()",
    "text": "mutate()\n\nModify the code to create new variable that is three times the value of polyarchy\nHow about polyarchy squared?\n\n\ndemocracy &lt;- vdem |&gt; # download the V-Dem dataset\n  filter(year == 2015) |&gt; # keep only observations from 2015\n  select(                  # select (and rename) these variables\n    country = country_name,     # name before the = sign is new name  \n    vdem_ctry_id = country_id,  # name after the = sign is old name\n    year, \n    polyarchy = v2x_polyarchy \n    ) |&gt;\n  mutate(\n    polyarchy_dbl = polyarchy * 2 # create variable 2X polyarchy\n  )\n  \nglimpse(democracy)  \n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/week-6.1.html#some-common-arithmetic-operators",
    "href": "slides/week-6.1.html#some-common-arithmetic-operators",
    "title": "The Grammar of Data Wrangling",
    "section": "Some Common Arithmetic Operators",
    "text": "Some Common Arithmetic Operators\n\n\n+ addition\n- subtraction\n* multiplication\n/ division\n^ exponentiation (also **)"
  },
  {
    "objectID": "slides/week-6.1.html#vdemdata-example",
    "href": "slides/week-6.1.html#vdemdata-example",
    "title": "The Grammar of Data Wrangling",
    "section": "vdemdata Example",
    "text": "vdemdata Example\n\n\n# Load packages\nlibrary(vdemdata) # to download V-Dem data\nlibrary(dplyr)\n\n# Download the data\ndemocracy &lt;- vdem |&gt; # download the V-Dem dataset\n  filter(year == 2015)  |&gt; # filter year, keep 2015\n  select(                  # select (and rename) these variables\n    country = country_name,     # the name before the = sign is the new name  \n    vdem_ctry_id = country_id,  # the name after the = sign is the old name\n    year, \n    polyarchy = v2x_polyarchy, \n    gdp_pc = e_gdppc, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, # replace the values in region with country names\n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n  )\n\n# View the data\nglimpse(democracy)"
  },
  {
    "objectID": "slides/week-6.1.html#section",
    "href": "slides/week-6.1.html#section",
    "title": "The Grammar of Data Wrangling",
    "section": "",
    "text": "Use filter() to select years…\n\n\n# Download the data\ndemocracy &lt;- vdem |&gt; \n  filter(year == 2015)  |&gt; # keep 2015\n  select(                 \n    country = country_name,       \n    vdem_ctry_id = country_id,  \n    year, \n    polyarchy = v2x_polyarchy, \n    gdp_pc = e_gdppc, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region,\n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n  )"
  },
  {
    "objectID": "slides/week-6.1.html#section-1",
    "href": "slides/week-6.1.html#section-1",
    "title": "The Grammar of Data Wrangling",
    "section": "",
    "text": "Use select() to choose variables…\n\n\n# Download the data\ndemocracy &lt;- vdem |&gt; \n  filter(year == 2015)  |&gt; \n  select(                  # select (and rename) these variables\n    country = country_name,     # the name before the = sign is the new name  \n    vdem_ctry_id = country_id,  # the name after the = sign is the old name\n    year, \n    polyarchy = v2x_polyarchy, \n    gdp_pc = e_gdppc, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, \n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n  )"
  },
  {
    "objectID": "slides/week-6.1.html#section-2",
    "href": "slides/week-6.1.html#section-2",
    "title": "The Grammar of Data Wrangling",
    "section": "",
    "text": "Use mutate with case_match() to Recode Region….\n\n\n# Download the data\ndemocracy &lt;- vdem |&gt;\n  filter(year == 2015)  |&gt; \n  select(                  \n    country = country_name,     \n    vdem_ctry_id = country_id,  \n    year, \n    polyarchy = v2x_polyarchy, \n    gdp_pc = e_gdppc, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, # replace the values in region with country names\n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\")\n                    # number on the left of the ~ is the V-Dem region code\n                    # we are changing the number to the country name on the right\n                    # of the equals sign\n  )"
  },
  {
    "objectID": "slides/week-6.1.html#your-turn",
    "href": "slides/week-6.1.html#your-turn",
    "title": "The Grammar of Data Wrangling",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nHave a look at the V-Dem codebook\n\n\nfilter the data for the year 2010\nselect a different democracy indicator\nuse a different region variable (e.g., e_regionpol_7C)"
  },
  {
    "objectID": "slides/week-6.1.html#group-summarize-and-arrange",
    "href": "slides/week-6.1.html#group-summarize-and-arrange",
    "title": "The Grammar of Data Wrangling",
    "section": "Group, Summarize and Arrange",
    "text": "Group, Summarize and Arrange\n\n\ngroup_by(), summarize(), arrange()\nA very common sequence in data science:\n\nTake an average or some other statistic for a group\nRank from high to low values of summary value"
  },
  {
    "objectID": "slides/week-6.1.html#example",
    "href": "slides/week-6.1.html#example",
    "title": "The Grammar of Data Wrangling",
    "section": "Example",
    "text": "Example\n\n\n# group_by(), summarize() and arrange()\ndemocracy |&gt; # save result as new object\n  group_by(region)  |&gt; # group data by region\n  summarize(           # summarize following vars (by region)\n    polyarchy_mean = mean(polyarchy, na.rm = TRUE), # calculate mean after remove NAs\n    libdem_median = median(libdem, na.rm = TRUE), # calculate median after remove NAs\n    gender = sd(gender, na.rm = TRUE), # calculate std. dev after remove NAs\n    gdp_pc = min(gdp_pc, na.rm = TRUE) # calculate minimum flfp after remove NAs\n  ) |&gt; \n  arrange(desc(polyarchy_mean)) # arrange in descending order by polyarchy score"
  },
  {
    "objectID": "slides/week-6.1.html#section-3",
    "href": "slides/week-6.1.html#section-3",
    "title": "The Grammar of Data Wrangling",
    "section": "",
    "text": "Use across() to Apply Same Function to Multiple Columns\n\n\ndem_women |&gt;\n  group_by(region) |&gt; \n  summarize(\n    across(c(polyarchy, libdem, women_rep, flfp), # apply to these columns \n           mean, # take the mean of the columns\n           na.rm = TRUE,  # remove NAs\n           .names = \"mean_{col}\") # change the suffix of the vars\n  ) |&gt; \n  arrange(desc(mean_polyarchy))"
  },
  {
    "objectID": "slides/week-6.1.html#try-it-yourself",
    "href": "slides/week-6.1.html#try-it-yourself",
    "title": "The Grammar of Data Wrangling",
    "section": "Try it Yourself",
    "text": "Try it Yourself\nNow try grouping by country instead of region and filter for years &gt;= 2000.\n\nWhat is the median value of polyarchy for Sweden?\nWhat is the max value of libdem New Zealand?\nWhat is the standard deviation of gender for Norway?\nWhat is the min of gdp_pc for Germany?\n\nTry using across() to calculate the mean of polyarchy, libdem, gender1 and gdp_pc for each country."
  },
  {
    "objectID": "slides/week-4.1.html#line-chart-from-last-class",
    "href": "slides/week-4.1.html#line-chart-from-last-class",
    "title": "Data Viz Best Practices",
    "section": "Line Chart from Last Class",
    "text": "Line Chart from Last Class"
  },
  {
    "objectID": "slides/week-4.1.html#problem",
    "href": "slides/week-4.1.html#problem",
    "title": "Data Viz Best Practices",
    "section": "Problem",
    "text": "Problem"
  },
  {
    "objectID": "slides/week-4.1.html#color-blindness",
    "href": "slides/week-4.1.html#color-blindness",
    "title": "Data Viz Best Practices",
    "section": "Color Blindness",
    "text": "Color Blindness\n\n\nColor Vision Deficiency (CVD) or color blindness affects 8 percent of men and 1 in 200 women\nThere are different types of CVD but most common is red-green color blindness\nTherefore, don’t include red and green in the same chart!\nLook for color blind safe palettes"
  },
  {
    "objectID": "slides/week-4.1.html#section",
    "href": "slides/week-4.1.html#section",
    "title": "Data Viz Best Practices",
    "section": "",
    "text": "Solution: Use a colorblind safe color scheme like viridis…"
  },
  {
    "objectID": "slides/week-4.1.html#section-1",
    "href": "slides/week-4.1.html#section-1",
    "title": "Data Viz Best Practices",
    "section": "",
    "text": "Use scale_color_viridis_d() in this case to specify the viridis color scheme…\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  ) +\n  scale_color_viridis_d(option = \"mako\", end = .8) # use viridis color palette"
  },
  {
    "objectID": "slides/week-4.1.html#better",
    "href": "slides/week-4.1.html#better",
    "title": "Data Viz Best Practices",
    "section": "Better!",
    "text": "Better!"
  },
  {
    "objectID": "slides/week-4.1.html#your-turn",
    "href": "slides/week-4.1.html#your-turn",
    "title": "Data Viz Best Practices",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nSee this reference to view different palettes and options\nAlso check out the paletteer package for easy access to many more palettes\nUse scale_color_viridis_d() to specify a viridis color scheme\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-4.1.html#scatter-plot-setup",
    "href": "slides/week-4.1.html#scatter-plot-setup",
    "title": "Data Viz Best Practices",
    "section": "Scatter Plot Setup",
    "text": "Scatter Plot Setup\n\n\ndem_summary_ctry &lt;-\n  read_csv(\"data/dem_women.csv\") |&gt;\n  group_by(country, region) |&gt; # group by country, keep region\n  summarize(\n    polyarchy = mean(polyarchy, na.rm = TRUE),\n    gdp_pc = mean(gdp_pc, na.rm = TRUE), \n    flfp = mean(flfp, na.rm = TRUE), \n    women_rep = mean(women_rep, na.rm = TRUE)\n  )"
  },
  {
    "objectID": "slides/week-4.1.html#scatter-plot-example",
    "href": "slides/week-4.1.html#scatter-plot-example",
    "title": "Data Viz Best Practices",
    "section": "Scatter Plot Example",
    "text": "Scatter Plot Example"
  },
  {
    "objectID": "slides/week-4.1.html#scatter-plot-example-1",
    "href": "slides/week-4.1.html#scatter-plot-example-1",
    "title": "Data Viz Best Practices",
    "section": "Scatter Plot Example",
    "text": "Scatter Plot Example\n\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    ) +\n  scale_color_viridis_d(option = \"mako\", end = .8)"
  },
  {
    "objectID": "slides/week-4.1.html#continuous-color-scales",
    "href": "slides/week-4.1.html#continuous-color-scales",
    "title": "Data Viz Best Practices",
    "section": "Continuous Color Scales",
    "text": "Continuous Color Scales"
  },
  {
    "objectID": "slides/week-4.1.html#use-scale_color_viridis_c",
    "href": "slides/week-4.1.html#use-scale_color_viridis_c",
    "title": "Data Viz Best Practices",
    "section": "Use scale_color_viridis_c()",
    "text": "Use scale_color_viridis_c()\n\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = flfp)) + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"FLFP\"\n    ) +\n  scale_color_viridis_c(option = \"mako\", end = .8)"
  },
  {
    "objectID": "slides/week-4.1.html#fill-vs.-color",
    "href": "slides/week-4.1.html#fill-vs.-color",
    "title": "Data Viz Best Practices",
    "section": "Fill vs. Color",
    "text": "Fill vs. Color\n\n\nUse color (e.g. color = or scale_color_*) to modify the color of points, lines, or text.\nCommonly applied to:\n\nScatter plots\nLine charts\nText elements"
  },
  {
    "objectID": "slides/week-4.1.html#fill-vs.-color-1",
    "href": "slides/week-4.1.html#fill-vs.-color-1",
    "title": "Data Viz Best Practices",
    "section": "Fill vs. Color",
    "text": "Fill vs. Color\n\n\nUse fill (e.g. fill = or scale_fill_*) to modify the fill color of shapes like bars, boxes, or polygons.\nCommonly applied to:\n\nBar charts\nBox plots\nHistograms"
  },
  {
    "objectID": "slides/week-4.1.html#our-column-chart-from-previous-class",
    "href": "slides/week-4.1.html#our-column-chart-from-previous-class",
    "title": "Data Viz Best Practices",
    "section": "Our Column Chart from Previous Class",
    "text": "Our Column Chart from Previous Class"
  },
  {
    "objectID": "slides/week-4.1.html#section-2",
    "href": "slides/week-4.1.html#section-2",
    "title": "Data Viz Best Practices",
    "section": "",
    "text": "Here we used the fill argument to color the bars in our column chart.\n\n\nggplot(dem_summary, aes(x = reorder(region, -polyarchy), y = polyarchy)) +\n  geom_col(fill = \"steelblue\") + \n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    ) + \n  theme_minimal() +\n  scale_fill_viridis_d(option = \"mako\", end = .8)"
  },
  {
    "objectID": "slides/week-4.1.html#section-3",
    "href": "slides/week-4.1.html#section-3",
    "title": "Data Viz Best Practices",
    "section": "",
    "text": "Should we do this instead?\n\n\nggplot(dem_summary, aes(x = reorder(region, -polyarchy), y = polyarchy, fill = region)) +\n  geom_col() + \n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    ) + \n  theme_minimal() +\n  scale_fill_viridis_d(option = \"mako\", end = .8)"
  },
  {
    "objectID": "slides/week-4.1.html#section-4",
    "href": "slides/week-4.1.html#section-4",
    "title": "Data Viz Best Practices",
    "section": "",
    "text": "Should we do this instead?"
  },
  {
    "objectID": "slides/week-4.1.html#when-to-use-a-color-scheme",
    "href": "slides/week-4.1.html#when-to-use-a-color-scheme",
    "title": "Data Viz Best Practices",
    "section": "When to Use a Color Scheme",
    "text": "When to Use a Color Scheme\n\n\nUse a color scheme when you want to differentiate between categories or groups\nBut only when the color adds value to the visualization\nIn this case, region is already differentiated by column height"
  },
  {
    "objectID": "slides/week-4.1.html#your-turn-1",
    "href": "slides/week-4.1.html#your-turn-1",
    "title": "Data Viz Best Practices",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nPractice applying color schemes to a scatter plot\nFirst, try a color scheme for a discrete variable\nThen, try a color scheme for a continuous variable\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-4.1.html#facet-wrapping",
    "href": "slides/week-4.1.html#facet-wrapping",
    "title": "Data Viz Best Practices",
    "section": "Facet Wrapping",
    "text": "Facet Wrapping"
  },
  {
    "objectID": "slides/week-4.1.html#facet-wrapping-1",
    "href": "slides/week-4.1.html#facet-wrapping-1",
    "title": "Data Viz Best Practices",
    "section": "Facet Wrapping",
    "text": "Facet Wrapping\n\nUse facet_wrap() with ~ before variable you want to wrap on…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  facet_wrap(~ region) +\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )"
  },
  {
    "objectID": "slides/week-4.1.html#facet-wrapping-2",
    "href": "slides/week-4.1.html#facet-wrapping-2",
    "title": "Data Viz Best Practices",
    "section": "Facet Wrapping",
    "text": "Facet Wrapping\n\nWhat else changes? Back down to two dimensions…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  facet_wrap(~ region) +\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )"
  },
  {
    "objectID": "slides/week-4.1.html#facet-wrapping-3",
    "href": "slides/week-4.1.html#facet-wrapping-3",
    "title": "Data Viz Best Practices",
    "section": "Facet Wrapping",
    "text": "Facet Wrapping\n\nDon’t forget to take the legend title out of the captions…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  facet_wrap(~ region) +\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )"
  },
  {
    "objectID": "slides/week-4.1.html#your-turn-2",
    "href": "slides/week-4.1.html#your-turn-2",
    "title": "Data Viz Best Practices",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nFacet wrap your scatter plot\nUse scales = \"free\" in facet_wrap call to fix the West\n\nfacet_wrap(~ region, scales = \"free\")\n\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-4.1.html#labeling-points",
    "href": "slides/week-4.1.html#labeling-points",
    "title": "Data Viz Best Practices",
    "section": "Labeling Points",
    "text": "Labeling Points"
  },
  {
    "objectID": "slides/week-4.1.html#labeling-points-1",
    "href": "slides/week-4.1.html#labeling-points-1",
    "title": "Data Viz Best Practices",
    "section": "Labeling Points",
    "text": "Labeling Points\n\nFilter for Asia, add labels with geom_text()…\n\ndem_summary_ctry |&gt; \n  filter(region == \"Asia\") |&gt;\n  ggplot(aes(x = gdp_pc, y = polyarchy)) + \n    geom_point() + \n    geom_text(aes(label = country), size = 2, vjust = 2) +\n    geom_smooth(method = \"lm\", linewidth = 1) +\n    scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n      labs(\n        x= \"GDP Per Capita\", \n        y = \"Polyarchy Score\",\n        title = \"Wealth and democracy in Asia, 1990 - present\", \n        caption = \"Source: V-Dem Institute\"\n        )"
  },
  {
    "objectID": "slides/week-4.1.html#your-turn-3",
    "href": "slides/week-4.1.html#your-turn-3",
    "title": "Data Viz Best Practices",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nFilter for Asia or another region\nUse geom_text() to add labels to your points\nPlay with size and vjust paramters\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-4.1.html#make-it-interactive",
    "href": "slides/week-4.1.html#make-it-interactive",
    "title": "Data Viz Best Practices",
    "section": "Make it Interactive",
    "text": "Make it Interactive"
  },
  {
    "objectID": "slides/week-4.1.html#section-5",
    "href": "slides/week-4.1.html#section-5",
    "title": "Data Viz Best Practices",
    "section": "",
    "text": "Use plotly to make any plot interactive…\n\nlibrary(plotly)\n\nmodernization_plot &lt;- ggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  aes(label = country) +\n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    ) +\n  scale_color_viridis_d(option = \"inferno\", end = .8)\n\nggplotly(modernization_plot, tooltip = c(\"country\", \"gdp_pc\", \"polyarchy\"))"
  },
  {
    "objectID": "slides/week-4.1.html#your-turn-4",
    "href": "slides/week-4.1.html#your-turn-4",
    "title": "Data Viz Best Practices",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nMake your scatter plot interactive with plotly\nUse tooltip argument to show more information on hover\nMake sure to include an additional aesthetic in aes() for the label\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-4.1.html#messages-warnings-and-errors",
    "href": "slides/week-4.1.html#messages-warnings-and-errors",
    "title": "Data Viz Best Practices",
    "section": "Messages, Warnings and Errors",
    "text": "Messages, Warnings and Errors\n\n\nMessages tell you what R is doing\nWarnings tell you that something might be wrong\nErrors tell you that something is definitely wrong\n\nLocate the error line number in the console and check your code\nError line tells you about where the error occurred, not exact\nErrors are normal, don’t freak out!\nIn fact, you should practice making errors to learn how to fix them"
  },
  {
    "objectID": "slides/week-2.2.html#a-data-science-workflow",
    "href": "slides/week-2.2.html#a-data-science-workflow",
    "title": "Intro to the Tidyverse",
    "section": "A Data Science Workflow",
    "text": "A Data Science Workflow"
  },
  {
    "objectID": "slides/week-2.2.html#the-tidyverse",
    "href": "slides/week-2.2.html#the-tidyverse",
    "title": "Intro to the Tidyverse",
    "section": "The Tidyverse",
    "text": "The Tidyverse\n\nThe Tidyverse is a collection of data science packages\nIt is also considered a dialect of R\nIn this class, we will be using many Tidyverse packages\n\nreadr for reading data\n\ntidyr for data tidying\ndplyr for data manipulation\nggplot2 for data visualization\n\nClick here for a full list"
  },
  {
    "objectID": "slides/week-2.2.html#working-with-tidyverse-packages",
    "href": "slides/week-2.2.html#working-with-tidyverse-packages",
    "title": "Intro to the Tidyverse",
    "section": "Working with Tidyverse packages",
    "text": "Working with Tidyverse packages\n\n\nAt first we will load the packages independently, e.g. library(ggplot2)\nLater we will load them all at once with library(tidyverse)\nAnother way to call a package is with ::, e.g. ggplot2::ggplot()"
  },
  {
    "objectID": "slides/week-2.2.html#reading-data-into-r",
    "href": "slides/week-2.2.html#reading-data-into-r",
    "title": "Intro to the Tidyverse",
    "section": "Reading Data into R",
    "text": "Reading Data into R\n\n\nLet’s use the readr package to read in a dataset\n\n\nlibrary(readr)\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")"
  },
  {
    "objectID": "slides/week-2.2.html#lets-look-at-the-data",
    "href": "slides/week-2.2.html#lets-look-at-the-data",
    "title": "Intro to the Tidyverse",
    "section": "Let’s Look at the Data",
    "text": "Let’s Look at the Data\n\nOne way to do this is with the base R head() function\n\nhead(dem_summary)\n\n# A tibble: 6 × 5\n  region         polyarchy gdp_pc  flfp women_rep\n  &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 The West           0.871  37.9   53.0      28.1\n2 Latin America      0.637   9.61  48.1      21.3\n3 Eastern Europe     0.539  12.2   50.5      18.0\n4 Asia               0.408   9.75  50.3      14.5\n5 Africa             0.393   4.41  56.7      17.4\n6 Middle East        0.246  21.1   26.6      10.2"
  },
  {
    "objectID": "slides/week-2.2.html#use-view",
    "href": "slides/week-2.2.html#use-view",
    "title": "Intro to the Tidyverse",
    "section": "Use View()",
    "text": "Use View()\n\nAnother way to look at the data is with View(). Or click on the name of the data frame in the Environment pane.\n\nView(dem_summary)"
  },
  {
    "objectID": "slides/week-2.2.html#using-glimpse-from-dplyr",
    "href": "slides/week-2.2.html#using-glimpse-from-dplyr",
    "title": "Intro to the Tidyverse",
    "section": "Using glimpse() from dplyr",
    "text": "Using glimpse() from dplyr\n\nAnother way to look at the data is with glimpse() from the dplyr package.\n\nlibrary(dplyr)\n\nglimpse(dem_summary)\n\nRows: 6\nColumns: 5\n$ region    &lt;chr&gt; \"The West\", \"Latin America\", \"Eastern Europe\", \"Asia\", \"Afri…\n$ polyarchy &lt;dbl&gt; 0.8709230, 0.6371358, 0.5387451, 0.4076602, 0.3934166, 0.245…\n$ gdp_pc    &lt;dbl&gt; 37.913054, 9.610284, 12.176554, 9.746391, 4.410484, 21.134319\n$ flfp      &lt;dbl&gt; 52.99082, 48.12645, 50.45894, 50.32171, 56.69530, 26.57872\n$ women_rep &lt;dbl&gt; 28.12921, 21.32548, 17.99728, 14.45225, 17.44296, 10.21568"
  },
  {
    "objectID": "slides/week-2.2.html#your-turn",
    "href": "slides/week-2.2.html#your-turn",
    "title": "Intro to the Tidyverse",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nRead in the dem_summary.csv file\nUse the three methods we discussed to view the data\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-2.2.html#a-few-more-basic-dplyr-functions",
    "href": "slides/week-2.2.html#a-few-more-basic-dplyr-functions",
    "title": "Intro to the Tidyverse",
    "section": "A Few More Basic dplyr Functions",
    "text": "A Few More Basic dplyr Functions\n\nUse select() to choose columns.\n\ndem_summary_abbr &lt;- dem_summary |&gt;\n  select(polyarchy, gdp_pc)\n\nglimpse(dem_summary_abbr)\n\nRows: 6\nColumns: 2\n$ polyarchy &lt;dbl&gt; 0.8709230, 0.6371358, 0.5387451, 0.4076602, 0.3934166, 0.245…\n$ gdp_pc    &lt;dbl&gt; 37.913054, 9.610284, 12.176554, 9.746391, 4.410484, 21.134319"
  },
  {
    "objectID": "slides/week-2.2.html#a-few-more-basic-dplyr-functions-1",
    "href": "slides/week-2.2.html#a-few-more-basic-dplyr-functions-1",
    "title": "Intro to the Tidyverse",
    "section": "A Few More Basic dplyr Functions",
    "text": "A Few More Basic dplyr Functions\nUse filter() to choose rows.\n\ndem_summary_abbr &lt;- dem_summary |&gt;\n  filter(gdp_pc &gt; 10)\n\nglimpse(dem_summary_abbr)\n\nRows: 3\nColumns: 5\n$ region    &lt;chr&gt; \"The West\", \"Eastern Europe\", \"Middle East\"\n$ polyarchy &lt;dbl&gt; 0.8709230, 0.5387451, 0.2458892\n$ gdp_pc    &lt;dbl&gt; 37.91305, 12.17655, 21.13432\n$ flfp      &lt;dbl&gt; 52.99082, 50.45894, 26.57872\n$ women_rep &lt;dbl&gt; 28.12921, 17.99728, 10.21568\n\n\n\n\n\n\n\n\nNote\n\n\nUsing the same name for the data frame results in overwriting the original data frame. If you want to keep the original data frame, use a different name."
  },
  {
    "objectID": "slides/week-2.2.html#a-few-more-basic-dplyr-functions-2",
    "href": "slides/week-2.2.html#a-few-more-basic-dplyr-functions-2",
    "title": "Intro to the Tidyverse",
    "section": "A Few More Basic dplyr Functions",
    "text": "A Few More Basic dplyr Functions\nUse mutate() to create new columns.\n\ndem_summary_abbr &lt;- dem_summary |&gt;\n  mutate(gdp_pc_thousands = gdp_pc * 1000)\n\nglimpse(dem_summary_abbr)\n\nRows: 6\nColumns: 6\n$ region           &lt;chr&gt; \"The West\", \"Latin America\", \"Eastern Europe\", \"Asia\"…\n$ polyarchy        &lt;dbl&gt; 0.8709230, 0.6371358, 0.5387451, 0.4076602, 0.3934166…\n$ gdp_pc           &lt;dbl&gt; 37.913054, 9.610284, 12.176554, 9.746391, 4.410484, 2…\n$ flfp             &lt;dbl&gt; 52.99082, 48.12645, 50.45894, 50.32171, 56.69530, 26.…\n$ women_rep        &lt;dbl&gt; 28.12921, 21.32548, 17.99728, 14.45225, 17.44296, 10.…\n$ gdp_pc_thousands &lt;dbl&gt; 37913.054, 9610.284, 12176.554, 9746.391, 4410.484, 2…"
  },
  {
    "objectID": "slides/week-2.2.html#your-turn-1",
    "href": "slides/week-2.2.html#your-turn-1",
    "title": "Intro to the Tidyverse",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nUse your new dplyr verbs to manipulate the data\nSelect columns, filter rows, and create new columns\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-2.2.html#basic-data-viz-with-ggplot2",
    "href": "slides/week-2.2.html#basic-data-viz-with-ggplot2",
    "title": "Intro to the Tidyverse",
    "section": "Basic Data Viz with ggplot2",
    "text": "Basic Data Viz with ggplot2\n\n\nggplot2 is a powerful data visualization package\nIt is based on the grammar of graphics\nWe will talk about this more in depth later"
  },
  {
    "objectID": "slides/week-2.2.html#basic-data-viz-with-ggplot2-1",
    "href": "slides/week-2.2.html#basic-data-viz-with-ggplot2-1",
    "title": "Intro to the Tidyverse",
    "section": "Basic Data Viz with ggplot2",
    "text": "Basic Data Viz with ggplot2\n\nFor now, let’s make a simple column chart\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(data = dem_summary, aes(x = region, y = polyarchy)) +\n  geom_col(fill = \"dodgerblue\")"
  },
  {
    "objectID": "slides/week-2.2.html#your-turn-2",
    "href": "slides/week-2.2.html#your-turn-2",
    "title": "Intro to the Tidyverse",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nUse ggplot2 to make a simple column chart\nChoose a different variable to plot\nChange the color of the bars\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-2.1.html#what-can-r-do",
    "href": "slides/week-2.1.html#what-can-r-do",
    "title": "R Coding Basics",
    "section": "What Can R Do?",
    "text": "What Can R Do?\n\n\nR is a powerful language for data analysis and visualization\nIt is also a general-purpose programming language\nDoes everything from web development to machine learning\nIt is open-source and has a large community of users and developers"
  },
  {
    "objectID": "slides/week-2.1.html#r-as-a-calculator",
    "href": "slides/week-2.1.html#r-as-a-calculator",
    "title": "R Coding Basics",
    "section": "R as a Calculator",
    "text": "R as a Calculator\n\n\nR can be used as a simple calculator\nYou can perform arithmetic operations on numbers\n\n\n# Addi a number and store it to a value\nsum_of_2plus2 &lt;- 2 + 2\n\n\nsum_of_2plus2\n\n[1] 4"
  },
  {
    "objectID": "slides/week-2.1.html#some-common-arithmetic-operators",
    "href": "slides/week-2.1.html#some-common-arithmetic-operators",
    "title": "R Coding Basics",
    "section": "Some Common Arithmetic Operators",
    "text": "Some Common Arithmetic Operators\n\n\n+ addition\n- subtraction\n* multiplication\n/ division\n^ exponentiation (also **)"
  },
  {
    "objectID": "slides/week-2.1.html#your-turn",
    "href": "slides/week-2.1.html#your-turn",
    "title": "R Coding Basics",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nTry some basic calculations\nTry them in a code chunk in the Quarto doc\nThen try some in the console\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-2.1.html#what-is-an-object",
    "href": "slides/week-2.1.html#what-is-an-object",
    "title": "R Coding Basics",
    "section": "What is an Object?",
    "text": "What is an Object?\n\nAn object in R is a data structure used to store data\nIt can vary from simple scalar types to more complex data structures like vectors, lists, or data frames\nObjects hold not only data but also information about the type of data and the operations that can be performed on them\nEvery entity in R is considered an object, making R a language based around the manipulation of objects"
  },
  {
    "objectID": "slides/week-2.1.html#how-to-store-data",
    "href": "slides/week-2.1.html#how-to-store-data",
    "title": "R Coding Basics",
    "section": "How to Store Data",
    "text": "How to Store Data\n\nIn R, you can store data in objects using the assignment operator &lt;-\nThe object name is on the left of &lt;-, and the data or value you wish to assign to the object is on the right\nThen you can print the object to the console using the object name\n\n\n# Store the value 42 in the object my_number\nmy_number &lt;- 42\n\n# Print the value of my_number\nmy_number \n\n[1] 42"
  },
  {
    "objectID": "slides/week-2.1.html#storing-a-vector",
    "href": "slides/week-2.1.html#storing-a-vector",
    "title": "R Coding Basics",
    "section": "Storing a Vector",
    "text": "Storing a Vector\n\n\nSometimes you want to store more than one number\nIn this case you can store a vector\nA vector is a collection of numbers or characters\n\n\nmy_numbers &lt;- c(1, 2, 3, 4, 5)\n\nmy_numbers\n\n[1] 1 2 3 4 5"
  },
  {
    "objectID": "slides/week-2.1.html#printing-objects",
    "href": "slides/week-2.1.html#printing-objects",
    "title": "R Coding Basics",
    "section": "“Printing” objects",
    "text": "“Printing” objects\n\n\nSometimes you will see print() used to display the contents of objects\nThis is not typically necessary\nSometimes you need it (like when printing inside of a function)\nBut usually you can just type the name of the object"
  },
  {
    "objectID": "slides/week-2.1.html#your-turn-1",
    "href": "slides/week-2.1.html#your-turn-1",
    "title": "R Coding Basics",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nStore a number in an object\nCreate a vector of numbers and store it in an object\n“Print” the objects by typing the object names\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-2.1.html#functions-1",
    "href": "slides/week-2.1.html#functions-1",
    "title": "R Coding Basics",
    "section": "Functions",
    "text": "Functions\n\nA function is a set of instructions that produces some output\nIn R, you can use built-in functions to perform specific tasks\nFor example, you can use the mean() function to calculate the average of a set of numbers\nTo do this you have to use the combine function c() to create a vector of numbers"
  },
  {
    "objectID": "slides/week-2.1.html#section",
    "href": "slides/week-2.1.html#section",
    "title": "R Coding Basics",
    "section": "",
    "text": "Create a vector of numbers and take the mean…\n\n\n# Create a vector of numbers\nnumbers &lt;- c(1, 2, 3, 4, 5)\n\n# Calculate the mean of the numbers\nmean(numbers)\n\n[1] 3"
  },
  {
    "objectID": "slides/week-2.1.html#some-common-base-r-functions",
    "href": "slides/week-2.1.html#some-common-base-r-functions",
    "title": "R Coding Basics",
    "section": "Some Common Base R Functions",
    "text": "Some Common Base R Functions\n\nmean() calculates the mean of a set of numbers\nmedian() calculates the median of a set of numbers\nsd() calculates the standard deviation of a set of numbers\nsum() calculates the sum of a set of numbers\nlength() calculates the length of a vector\nmax() and min() calculate the maximum and minimum values of a vector\nround() rounds a number to a specified number of decimal places\nsqrt() calculates the square root of a number\nlog() calculates the natural logarithm of a number\nexp() calculates the exponential of a number\nabs() calculates the absolute value of a number"
  },
  {
    "objectID": "slides/week-2.1.html#your-turn-2",
    "href": "slides/week-2.1.html#your-turn-2",
    "title": "R Coding Basics",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nCreate a vector of numbers\nStore as an object\nApply a function to the object\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-2.1.html#from-functions-to-packages",
    "href": "slides/week-2.1.html#from-functions-to-packages",
    "title": "R Coding Basics",
    "section": "From Functions to Packages",
    "text": "From Functions to Packages\n\n\nA function is a set of instructions\n\nread_csv() is a function\nggplot() is a function\n\nA package is a collection of functions\n\nreadr is a package that contains the read_csv() function\nggplot2 is a package that contains the ggplot() function\n\nUse install.packages() to install packages\nUse library() to load packages\nYou can install packages from CRAN"
  },
  {
    "objectID": "slides/week-2.1.html#installing-packages",
    "href": "slides/week-2.1.html#installing-packages",
    "title": "R Coding Basics",
    "section": "Installing Packages",
    "text": "Installing Packages\n\n\nYou can install packages from CRAN (Comprehensive R Archive Network)\nUse the install.packages() function to install packages\nFor example, to install the tidyverse package, you would run install.packages(\"tidyverse\")"
  },
  {
    "objectID": "slides/week-2.1.html#installing-packages-1",
    "href": "slides/week-2.1.html#installing-packages-1",
    "title": "R Coding Basics",
    "section": "Installing Packages",
    "text": "Installing Packages\n\n\nAnother way to install packages is from a GitHub repository\nWe will use the pak package to install packages from GitHub\nTo install pak, run install.packages(\"pak\")\nThen you can use pak::pkg_install() to install packages from GitHub\nFor example, to install the vdemlite package, you would run pak::pkg_install(\"eteitelbaum/vdemlite\")"
  },
  {
    "objectID": "slides/week-2.1.html#installing-packages-2",
    "href": "slides/week-2.1.html#installing-packages-2",
    "title": "R Coding Basics",
    "section": "Installing Packages",
    "text": "Installing Packages\n\n\nYou only need to install a package once\nAfter you install a package, you can load it with the library() function\nDo not put install.packages() in your R script or Quarto document"
  },
  {
    "objectID": "slides/week-2.1.html#your-turn-3",
    "href": "slides/week-2.1.html#your-turn-3",
    "title": "R Coding Basics",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nInstall the tidyverse package in your environment\nLoad the tidyverse package\nInstall the pak package\nInstall vdemlite using pak::pkg_install()\nLoad the vdemlite package\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-2.1.html#installing-r-and-rstudio",
    "href": "slides/week-2.1.html#installing-r-and-rstudio",
    "title": "R Coding Basics",
    "section": "Installing R and RStudio",
    "text": "Installing R and RStudio\n\n\nYou can install R and RStudio on your local machine\nVisit Posit to download R and RStudio\nInstall R first, then RStudio\nYou download R from CRAN and RStudio from the RStudio website"
  },
  {
    "objectID": "slides/week-2.1.html#try-it",
    "href": "slides/week-2.1.html#try-it",
    "title": "R Coding Basics",
    "section": "Try It!",
    "text": "Try It!\n\n\nInstall R and RStudio on your local machine\nOpen RStudio\nOpen a Quarto document\nTry some of the code we have used today\nInstall the tidyverse package"
  },
  {
    "objectID": "slides/week-7.1.html#what-is-tyding-data",
    "href": "slides/week-7.1.html#what-is-tyding-data",
    "title": "Tidying Data",
    "section": "What is Tyding Data?",
    "text": "What is Tyding Data?\n\nEarlier we talked about the concept of “tidy data”\n\nEach variable forms a column\nEach observation is in a row\nEach cell has a single value\n\nThe process of tidying data involves reshaping (or pivoting) data into a tidy format\nWe want to use the pivot_longer() or pivot_wider() functions from tidyr to do this"
  },
  {
    "objectID": "slides/week-7.1.html#section",
    "href": "slides/week-7.1.html#section",
    "title": "Tidying Data",
    "section": "",
    "text": "Query: Are these data in a tidy format?\n\n\nlibrary(tidyr)\n\nsmiths\n\n# A tibble: 2 × 5\n  subject     time   age weight height\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 John Smith     1    33     90   1.87\n2 Mary Smith     1    NA     NA   1.54\n\n\n\n\n\n\n\n\n\nTip\n\n\nTo get a list of data frames available in a package use the data() function, e.g. data(package = \"tidyr\")."
  },
  {
    "objectID": "slides/week-7.1.html#section-1",
    "href": "slides/week-7.1.html#section-1",
    "title": "Tidying Data",
    "section": "",
    "text": "How about these data?\n\n\nworld_bank_pop\n\n# A tibble: 1,064 × 20\n   country indicator      `2000`  `2001`  `2002`  `2003`  `2004`  `2005`  `2006`\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 ABW     SP.URB.TOTL    4.16e4 4.20e+4 4.22e+4 4.23e+4 4.23e+4 4.24e+4 4.26e+4\n 2 ABW     SP.URB.GROW    1.66e0 9.56e-1 4.01e-1 1.97e-1 9.46e-2 1.94e-1 3.67e-1\n 3 ABW     SP.POP.TOTL    8.91e4 9.07e+4 9.18e+4 9.27e+4 9.35e+4 9.45e+4 9.56e+4\n 4 ABW     SP.POP.GROW    2.54e0 1.77e+0 1.19e+0 9.97e-1 9.01e-1 1.00e+0 1.18e+0\n 5 AFE     SP.URB.TOTL    1.16e8 1.20e+8 1.24e+8 1.29e+8 1.34e+8 1.39e+8 1.44e+8\n 6 AFE     SP.URB.GROW    3.60e0 3.66e+0 3.72e+0 3.71e+0 3.74e+0 3.81e+0 3.81e+0\n 7 AFE     SP.POP.TOTL    4.02e8 4.12e+8 4.23e+8 4.34e+8 4.45e+8 4.57e+8 4.70e+8\n 8 AFE     SP.POP.GROW    2.58e0 2.59e+0 2.61e+0 2.62e+0 2.64e+0 2.67e+0 2.70e+0\n 9 AFG     SP.URB.TOTL    4.31e6 4.36e+6 4.67e+6 5.06e+6 5.30e+6 5.54e+6 5.83e+6\n10 AFG     SP.URB.GROW    1.86e0 1.15e+0 6.86e+0 7.95e+0 4.59e+0 4.47e+0 5.03e+0\n# ℹ 1,054 more rows\n# ℹ 11 more variables: `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;,\n#   `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;"
  },
  {
    "objectID": "slides/week-7.1.html#pivot-longer-1",
    "href": "slides/week-7.1.html#pivot-longer-1",
    "title": "Tidying Data",
    "section": "Pivot Longer",
    "text": "Pivot Longer\n\npivot_longer() takes three arguments:\n\ncols - which columns you want to pivot\nnames_to - the name of the column where the old column names are going to (identifier)\nvalues_to - the name of the column where the values are going to"
  },
  {
    "objectID": "slides/week-7.1.html#example-wb-population-data",
    "href": "slides/week-7.1.html#example-wb-population-data",
    "title": "Tidying Data",
    "section": "Example: WB Population Data",
    "text": "Example: WB Population Data\n\n\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Pivot using pivot_longer\n\nlong_pop_data &lt;- world_bank_pop |&gt;\n  pivot_longer(\n    cols = `2000`:`2017`,   # The columns you want to pivot (years)\n    names_to = \"year\",      # New column name for the years\n    values_to = \"pop\"       # New column name for the values\n  ) \n\n# View the tidied data\nlong_pop_data\n\n# A tibble: 19,152 × 4\n   country indicator   year    pop\n   &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 ABW     SP.URB.TOTL 2000  41625\n 2 ABW     SP.URB.TOTL 2001  42025\n 3 ABW     SP.URB.TOTL 2002  42194\n 4 ABW     SP.URB.TOTL 2003  42277\n 5 ABW     SP.URB.TOTL 2004  42317\n 6 ABW     SP.URB.TOTL 2005  42399\n 7 ABW     SP.URB.TOTL 2006  42555\n 8 ABW     SP.URB.TOTL 2007  42729\n 9 ABW     SP.URB.TOTL 2008  42906\n10 ABW     SP.URB.TOTL 2009  43079\n# ℹ 19,142 more rows"
  },
  {
    "objectID": "slides/week-7.1.html#section-2",
    "href": "slides/week-7.1.html#section-2",
    "title": "Tidying Data",
    "section": "",
    "text": "This is better, usable even, but are we done if we want a tidy data frame?\n\n\n\n# A tibble: 19,152 × 4\n   country indicator   year    pop\n   &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 ABW     SP.URB.TOTL 2000  41625\n 2 ABW     SP.URB.TOTL 2001  42025\n 3 ABW     SP.URB.TOTL 2002  42194\n 4 ABW     SP.URB.TOTL 2003  42277\n 5 ABW     SP.URB.TOTL 2004  42317\n 6 ABW     SP.URB.TOTL 2005  42399\n 7 ABW     SP.URB.TOTL 2006  42555\n 8 ABW     SP.URB.TOTL 2007  42729\n 9 ABW     SP.URB.TOTL 2008  42906\n10 ABW     SP.URB.TOTL 2009  43079\n# ℹ 19,142 more rows"
  },
  {
    "objectID": "slides/week-7.1.html#section-3",
    "href": "slides/week-7.1.html#section-3",
    "title": "Tidying Data",
    "section": "",
    "text": "Issue is that the data are in long form (which is OK for some purposes), but we want to make it wider. Wider, but tidy…\n\n\n\n# A tibble: 19,152 × 4\n   country indicator   year    pop\n   &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;\n 1 ABW     SP.URB.TOTL 2000  41625\n 2 ABW     SP.URB.TOTL 2001  42025\n 3 ABW     SP.URB.TOTL 2002  42194\n 4 ABW     SP.URB.TOTL 2003  42277\n 5 ABW     SP.URB.TOTL 2004  42317\n 6 ABW     SP.URB.TOTL 2005  42399\n 7 ABW     SP.URB.TOTL 2006  42555\n 8 ABW     SP.URB.TOTL 2007  42729\n 9 ABW     SP.URB.TOTL 2008  42906\n10 ABW     SP.URB.TOTL 2009  43079\n# ℹ 19,142 more rows"
  },
  {
    "objectID": "slides/week-7.1.html#pivot-wider-1",
    "href": "slides/week-7.1.html#pivot-wider-1",
    "title": "Tidying Data",
    "section": "Pivot Wider",
    "text": "Pivot Wider\n\npivot_wider() takes three main arguments:\n\nnames_from - the column whose values will become new column names (identifier)\nvalues_from - the column containing the values that will fill the new columns\nvalues_fill (optional) - specifies what to use for missing values (e.g., NA, 0)"
  },
  {
    "objectID": "slides/week-7.1.html#pivotwider-the-wb-data",
    "href": "slides/week-7.1.html#pivotwider-the-wb-data",
    "title": "Tidying Data",
    "section": "pivotwider() the WB Data",
    "text": "pivotwider() the WB Data\n\n\n# pivot wider\ntidy_pop_data &lt;- long_pop_data |&gt;\n  pivot_wider(\n    names_from = indicator, \n    values_from = pop\n  )\n\n# view the data\ntidy_pop_data\n\n# A tibble: 4,788 × 6\n   country year  SP.URB.TOTL SP.URB.GROW SP.POP.TOTL SP.POP.GROW\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 ABW     2000        41625      1.66         89101       2.54 \n 2 ABW     2001        42025      0.956        90691       1.77 \n 3 ABW     2002        42194      0.401        91781       1.19 \n 4 ABW     2003        42277      0.197        92701       0.997\n 5 ABW     2004        42317      0.0946       93540       0.901\n 6 ABW     2005        42399      0.194        94483       1.00 \n 7 ABW     2006        42555      0.367        95606       1.18 \n 8 ABW     2007        42729      0.408        96787       1.23 \n 9 ABW     2008        42906      0.413        97996       1.24 \n10 ABW     2009        43079      0.402        99212       1.23 \n# ℹ 4,778 more rows"
  },
  {
    "objectID": "slides/week-7.1.html#pivotwider-the-wb-data-1",
    "href": "slides/week-7.1.html#pivotwider-the-wb-data-1",
    "title": "Tidying Data",
    "section": "pivotwider() the WB Data",
    "text": "pivotwider() the WB Data\n\n\n\n# A tibble: 4,788 × 6\n   country year  SP.URB.TOTL SP.URB.GROW SP.POP.TOTL SP.POP.GROW\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 ABW     2000        41625      1.66         89101       2.54 \n 2 ABW     2001        42025      0.956        90691       1.77 \n 3 ABW     2002        42194      0.401        91781       1.19 \n 4 ABW     2003        42277      0.197        92701       0.997\n 5 ABW     2004        42317      0.0946       93540       0.901\n 6 ABW     2005        42399      0.194        94483       1.00 \n 7 ABW     2006        42555      0.367        95606       1.18 \n 8 ABW     2007        42729      0.408        96787       1.23 \n 9 ABW     2008        42906      0.413        97996       1.24 \n10 ABW     2009        43079      0.402        99212       1.23 \n# ℹ 4,778 more rows"
  },
  {
    "objectID": "slides/week-7.1.html#download-some-messy-data",
    "href": "slides/week-7.1.html#download-some-messy-data",
    "title": "Tidying Data",
    "section": "Download Some Messy Data",
    "text": "Download Some Messy Data\n\n\nDownload some messy WB data\nMake it multiple variables\nDownload as a CSV file\nSave in your project /data folder"
  },
  {
    "objectID": "slides/week-7.1.html#messy-data-example",
    "href": "slides/week-7.1.html#messy-data-example",
    "title": "Tidying Data",
    "section": "Messy Data Example",
    "text": "Messy Data Example"
  },
  {
    "objectID": "slides/week-7.1.html#read-data",
    "href": "slides/week-7.1.html#read-data",
    "title": "Tidying Data",
    "section": "Read Data",
    "text": "Read Data\n\n\n# Load packages\nlibrary(readr) \nlibrary(dplyr) \n\n# Read data from csv file into an object called \"wb_data_messy\"\nwb_data_messy &lt;- read_csv(\"data/your_file_name.csv\")\n\n# View the data\nglimpse(wb_data_messy)"
  },
  {
    "objectID": "slides/week-7.1.html#pivot-the-data",
    "href": "slides/week-7.1.html#pivot-the-data",
    "title": "Tidying Data",
    "section": "Pivot the Data",
    "text": "Pivot the Data\n\n\nUse pivot_longer() to get the data in long form\nUse pivot_wider() to get the series in the columns"
  },
  {
    "objectID": "slides/week-7.1.html#special-considerations",
    "href": "slides/week-7.1.html#special-considerations",
    "title": "Tidying Data",
    "section": "Special Considerations",
    "text": "Special Considerations\n\n\n# Load tidyr\nlibrary(tidyr)\n\n# Reshape the data\nwb_data &lt;- wb_data_messy |&gt; \n  pivot_longer(             \n    cols = `1973 [YR1973]`: `2022 [YR2022]`, # columns to pivot\n    names_to = \"year\", # name the identifier column \"year\"\n    values_to = \"values\" # name the numeric var column \n  ) \n\n# View the data\nglimpse(wb_data)"
  },
  {
    "objectID": "slides/week-7.1.html#special-considerations-1",
    "href": "slides/week-7.1.html#special-considerations-1",
    "title": "Tidying Data",
    "section": "Special Considerations",
    "text": "Special Considerations\n\n\nwb_data &lt;- wb_data |&gt;\n  select(-`Series Name`)\n\n# Reshape the data\ntidy_data &lt;- wb_data |&gt; \n  pivot_wider(             \n    names_from = `Series Code`,\n    values_from = values\n  ) \n\n# View the data\nglimpse(wb_data)"
  },
  {
    "objectID": "slides/week-7.1.html#give-it-a-shot",
    "href": "slides/week-7.1.html#give-it-a-shot",
    "title": "Tidying Data",
    "section": "Give it a Shot!",
    "text": "Give it a Shot!\n\n\nTry downloading some messy WB data and make it tidy\nRefer to previous slides for help\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-7.1.html#mutating-variables",
    "href": "slides/week-7.1.html#mutating-variables",
    "title": "Tidying Data",
    "section": "Mutating Variables",
    "text": "Mutating Variables\n\n\nAnytime we want to change a variable, we are going to use the dplyr verbs mutate() or mutate_at()\nmutate() is if you want to change on variable\nUse across() for multiple variables"
  },
  {
    "objectID": "slides/week-7.1.html#lets-fix-our-variables",
    "href": "slides/week-7.1.html#lets-fix-our-variables",
    "title": "Tidying Data",
    "section": "Let’s Fix Our Variables",
    "text": "Let’s Fix Our Variables\n\n\n# Fix year and flfp\nwb_data &lt;- wb_data |&gt; \n  mutate(year = substring(year, 1, 4)) |&gt;  # truncate year\n  mutate(across(c(\"year\", \"flfp\"), as.numeric))  # convert year and flfp to numeric\n\n# View the data\nglimpse(wb_data)"
  },
  {
    "objectID": "slides/week-7.1.html#now-try-it-with-multiple-varaibles",
    "href": "slides/week-7.1.html#now-try-it-with-multiple-varaibles",
    "title": "Tidying Data",
    "section": "Now Try it With Multiple Varaibles",
    "text": "Now Try it With Multiple Varaibles\n\n\nGo to the data frame with multiple variables that you created earlier\nHow would you modify this code to make sure the variables are in the right format?"
  },
  {
    "objectID": "slides/week-7.1.html#clean-variable-names",
    "href": "slides/week-7.1.html#clean-variable-names",
    "title": "Tidying Data",
    "section": "Clean Variable Names",
    "text": "Clean Variable Names\n\nVery simple: use the janitor package!\n\n# Load janitor\nlibrary(janitor)\n\n# Apply clean_names() to wb_data, store in new data frame called wb_data_clean\nwb_data_clean &lt;- wb_data |&gt;  \n  clean_names() \n\n# Write wb_data_clean to a csv file\nwrite_csv(wb_data_clean, \"data/wb_data_clean.csv\")\n\n# View the data\nglimpse(wb_data_clean)"
  },
  {
    "objectID": "slides/week-7.1.html#column-specifications",
    "href": "slides/week-7.1.html#column-specifications",
    "title": "Tidying Data",
    "section": "Column Specifications",
    "text": "Column Specifications\n\n\nCan clean columns on backend\nBut can also specify data types in read_csv() or read_excel()\n\n\nspec(wb_data_messy) # show column specifications (from `readr`)\n\nwb_data_messy &lt;- read_csv(\"data/your_file_name.csv\", \n                          col_types = cols(\n                           col_double(`1990 [YR1990]`) \n                          ))"
  },
  {
    "objectID": "slides/week-13.2.html#what-is-reveal.js",
    "href": "slides/week-13.2.html#what-is-reveal.js",
    "title": "Revealjs Slides",
    "section": "What is Reveal.js?",
    "text": "What is Reveal.js?\n\nWeb-based presentation framework that enables the creation of interactive presentations using HTML\nSupports dynamic transitions, embedded media, and interactive elements\nWorks across devices and platforms, ensuring your presentation looks great everywhere\nOffers a wide range of plugins and extensions to enhance your presentations, from analytics to themes\nShare your presentations as a URL, making it accessible to anyone, anywhere"
  },
  {
    "objectID": "slides/week-13.2.html#why-revealjs",
    "href": "slides/week-13.2.html#why-revealjs",
    "title": "Revealjs Slides",
    "section": "Why Revealjs?",
    "text": "Why Revealjs?\n\nYou can also make PowerPoint and Beamer (pdf) slides with Quarto\nRevealjs is more fun and has some key advantages\n\nIt’s web-based and interactive\nIt’s more accessible\nIt’s easy to share\n\nBut sometimes other formats can be useful\n\nCan you think of some scenarios where you might want to use PowerPoint or Beamer?"
  },
  {
    "objectID": "slides/week-13.2.html#setup",
    "href": "slides/week-13.2.html#setup",
    "title": "Revealjs Slides",
    "section": "Setup",
    "text": "Setup\n\nSetting up a reveal.js presentation in Quarto is easy. You just specify the revealjs format. From there you can use the usual YAML arguments like title:, subtitle:, etc. that you would use in any Quarto document. Here’s a simple example:\n\n---\ntitle: \"Revealjs Presentation\"\nsubtitle: \"For Demonstration Purposes\"\nauthor: \"Your Name\"\ndate: today\nformat: revealjs\n---"
  },
  {
    "objectID": "slides/week-13.2.html#creating-slides",
    "href": "slides/week-13.2.html#creating-slides",
    "title": "Revealjs Slides",
    "section": "Creating Slides",
    "text": "Creating Slides\n\nSlides are created using the standard markdown syntax. # gives you a new section, while ## gives you a new slide. Then you can write text and use - for bullet points.\n# Section Header\n\n## Slide 1\n\nThis is the first slide.\n\n- Bullet 1\n- Bullet 2\n- Bullet 3"
  },
  {
    "objectID": "slides/week-13.2.html#spacing",
    "href": "slides/week-13.2.html#spacing",
    "title": "Revealjs Slides",
    "section": "Spacing",
    "text": "Spacing\n\nSometimes you want to have more spacing than the default spacing between lines on each slide.\nThe easiest way to handle this is to insert &lt;br&gt; tags where you want the extra space.\n\n## Slide 2\n\n&lt;br&gt;\n\nThis is the content for my second slide. It is going to have this line and then a line break and then some code. \n\n&lt;br&gt;\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n[1] 4\n\n\n:::\n:::"
  },
  {
    "objectID": "slides/week-13.2.html#section",
    "href": "slides/week-13.2.html#section",
    "title": "Revealjs Slides",
    "section": "",
    "text": "If perchance you don’t want a title slide, just eliminate title, subtitle, etc. from the YAML header.\n\n---\nformat: revealjs\n---"
  },
  {
    "objectID": "slides/week-13.2.html#incremental-lists",
    "href": "slides/week-13.2.html#incremental-lists",
    "title": "Revealjs Slides",
    "section": "Incremental Lists",
    "text": "Incremental Lists\n\nThere are two ways to get incremental lists. The first is to specify incremental: true in the YAML header.\n\n---\nformat: \n  revealjs:\n    incremental: true\n---"
  },
  {
    "objectID": "slides/week-13.2.html#incremental-lists-1",
    "href": "slides/week-13.2.html#incremental-lists-1",
    "title": "Revealjs Slides",
    "section": "Incremental Lists",
    "text": "Incremental Lists\n\nThe other is to surround the relevant bullet points in a div with the class incremental.\n\n::: {.incremental}\n- Bullet 1\n- Bullet 2\n- Bullet 3\n:::"
  },
  {
    "objectID": "slides/week-13.2.html#incremental-lists-2",
    "href": "slides/week-13.2.html#incremental-lists-2",
    "title": "Revealjs Slides",
    "section": "Incremental Lists",
    "text": "Incremental Lists\n\nOr, let’s say you have incremental: true in the YAML header, but you want to turn it off for a particular slide. In this case, you can use nonincremental.\n::: {.nonincremental}\n- Bullet 1\n- Bullet 2\n- Bullet 3\n:::"
  },
  {
    "objectID": "slides/week-13.2.html#your-turn",
    "href": "slides/week-13.2.html#your-turn",
    "title": "Revealjs Slides",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nStart a new Quarto project\nCreate a new Quarto document in your project folder\nAdd the YAML header and specify the revealjs format\nAdd slides and sections\nUse the incremental class to create incremental lists\nNow try setting incremental: true in the YAML header\nUse nonincremental to turn off incremental lists for a particular slide\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-13.2.html#content-overflow",
    "href": "slides/week-13.2.html#content-overflow",
    "title": "Revealjs Slides",
    "section": "Content Overflow",
    "text": "Content Overflow\n\nSometimes you have too much material to fit on a slide. In this case, you can use the .smaller or .scrollable class. You can use curly braces to add these to a particular slide…\n\n## Slide Title {.smaller}\n\n## Slide Title {.scrollable}"
  },
  {
    "objectID": "slides/week-13.2.html#content-overflow-1",
    "href": "slides/week-13.2.html#content-overflow-1",
    "title": "Revealjs Slides",
    "section": "Content Overflow",
    "text": "Content Overflow\n\nOr you can add them to the YAML header to apply them to the entire presentation…\n\n---\nformat:\n  revealjs:\n    smaller: true\n    scrollable: true\n---"
  },
  {
    "objectID": "slides/week-13.2.html#adding-images",
    "href": "slides/week-13.2.html#adding-images",
    "title": "Revealjs Slides",
    "section": "Adding Images",
    "text": "Adding Images\n\nTo add an image, you can use the standard markdown syntax.\n\n![](images/your-image.png)"
  },
  {
    "objectID": "slides/week-13.2.html#adding-images-1",
    "href": "slides/week-13.2.html#adding-images-1",
    "title": "Revealjs Slides",
    "section": "Adding Images",
    "text": "Adding Images\n\nTo control the width of the image, you can use the width attribute.\n\n![](images/your-image.png){width=50%}"
  },
  {
    "objectID": "slides/week-13.2.html#columns",
    "href": "slides/week-13.2.html#columns",
    "title": "Revealjs Slides",
    "section": "Columns",
    "text": "Columns\n\nTo put content in columns, you can create a div with the columns class.\n:::: {.columns}\n\n::: {.column width=\"40%\"}\nLeft column\n:::\n\n::: {.column width=\"60%\"}\nRight column\n:::\n\n::::"
  },
  {
    "objectID": "slides/week-13.2.html#your-turn-1",
    "href": "slides/week-13.2.html#your-turn-1",
    "title": "Revealjs Slides",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nTry making a long slide with the .smaller class\nNow use the .scrollable class instead\nAdd an image to a slide\n\nGo to Wikimedia Commons and download an image\nAdd the image to your project folder\n\nMake a slide with two columns\n\nOn the left, write a few bullet points\nOn the right, add an image\nUse the width attribute to control the size of the image\n\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-13.2.html#code-blocks",
    "href": "slides/week-13.2.html#code-blocks",
    "title": "Revealjs Slides",
    "section": "Code Blocks",
    "text": "Code Blocks\n\n\nTo add a code block, use the standard markdown syntax\nYou can specify the language for syntax highlighting\nJust like a normal HTML document, you can add chunk options"
  },
  {
    "objectID": "slides/week-13.2.html#code-blocks-1",
    "href": "slides/week-13.2.html#code-blocks-1",
    "title": "Revealjs Slides",
    "section": "Code Blocks",
    "text": "Code Blocks\n\n\n```{r}\n#| label: leaflet_map1\n#| eval: false\n\nlibrary(leaflet)\nleaflet() %&gt;% \n  addTiles() %&gt;%   # Add default OpenStreetMap map tiles\n  addMarkers(lat = 38.90243843683386, lng =  -77.0443814477152, \n             label = \"Elliott School of International Affairs\")\n```"
  },
  {
    "objectID": "slides/week-13.2.html#section-1",
    "href": "slides/week-13.2.html#section-1",
    "title": "Revealjs Slides",
    "section": "",
    "text": "If your echo: is set to false, then you will just see the output.\n\n\n\n\n\n\n\n\nNote how the output of the leaflet is interactive!"
  },
  {
    "objectID": "slides/week-13.2.html#section-2",
    "href": "slides/week-13.2.html#section-2",
    "title": "Revealjs Slides",
    "section": "",
    "text": "But if echo: is set to true, then you will see the code and the output…\n\nlibrary(leaflet)\nleaflet() %&gt;% \n  addTiles() %&gt;%   # Add default OpenStreetMap map tiles\n  addMarkers(lat = 38.90243843683386, lng =  -77.0443814477152, \n             label = \"Elliott School of International Affairs\")"
  },
  {
    "objectID": "slides/week-13.2.html#section-3",
    "href": "slides/week-13.2.html#section-3",
    "title": "Revealjs Slides",
    "section": "",
    "text": "And just like in a normal HTML document, you can also set these options in the YAML header.\n\nexecute:\n  echo: false\n  message: false\n  warning: false"
  },
  {
    "objectID": "slides/week-13.2.html#section-4",
    "href": "slides/week-13.2.html#section-4",
    "title": "Revealjs Slides",
    "section": "",
    "text": "For presentation purposes, you may oly want to show specific lines of code.\n\n\nlibrary(leaflet)\nleaflet() %&gt;% \n  addTiles() %&gt;%   # Add default OpenStreetMap map tiles\n  addMarkers(lat = 38.90243843683386, lng =  -77.0443814477152, \n             label = \"Elliott School of International Affairs\")"
  },
  {
    "objectID": "slides/week-13.2.html#section-5",
    "href": "slides/week-13.2.html#section-5",
    "title": "Revealjs Slides",
    "section": "",
    "text": "To do this, you would use the code-line-numbers option in the YAML header.\n\n\nlibrary(leaflet)\nleaflet() %&gt;% \n  addTiles() %&gt;%   # Add default OpenStreetMap map tiles\n  addMarkers(lat = 38.90243843683386, lng =  -77.0443814477152, \n             label = \"Elliott School of International Affairs\")"
  },
  {
    "objectID": "slides/week-13.2.html#your-turn-2",
    "href": "slides/week-13.2.html#your-turn-2",
    "title": "Revealjs Slides",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nStart a new slide\nAdd an R code chunk to it\nAdd a leaflet map to it\nRender the slide\nTry different chunk options\n\nDisplay only the output\nDisplay code and output\nDisplay just the code\nDisplay only particular lines\n\nTry adjusting evaluate options in the YAML header\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-13.2.html#themes",
    "href": "slides/week-13.2.html#themes",
    "title": "Revealjs Slides",
    "section": "Themes",
    "text": "Themes\n\nYou can customize the look of your slides by using a different theme, e.g. \n\ntheme: dark\n\n\nSee the Reveal documentation for the full list of availalbe themes."
  },
  {
    "objectID": "slides/week-13.2.html#themes-1",
    "href": "slides/week-13.2.html#themes-1",
    "title": "Revealjs Slides",
    "section": "Themes",
    "text": "Themes\n\nYou can also add a custom SCSS file to tweak an existing them or create your own:\n/*-- scss:defaults --*/\n\n$body-bg: #191919;\n$body-color: #fff;\n$link-color: #42affa;\n\n/*-- scss:rules --*/\n\n.reveal .slide blockquote {\n  border-left: 3px solid $text-muted;\n  padding-left: 0.5em;\n}"
  },
  {
    "objectID": "slides/week-13.2.html#syntax-highlighting",
    "href": "slides/week-13.2.html#syntax-highlighting",
    "title": "Revealjs Slides",
    "section": "Syntax Highlighting",
    "text": "Syntax Highlighting\n\nQuarto offers 20 different syntax highlighting themes. Click here to see the available themes.\n\nYou can select your preferred theme by adding highlight-style to the YAML header, e.g. \n\nhighlight-style: github"
  },
  {
    "objectID": "slides/week-13.2.html#background-styling",
    "href": "slides/week-13.2.html#background-styling",
    "title": "Revealjs Slides",
    "section": "Background Styling",
    "text": "Background Styling\n\nYou can change the color of your background by adding the background-color attribute to a slide.\n\n## Slide Title {background-color=\"aquamarine\"}"
  },
  {
    "objectID": "slides/week-13.2.html#background-styling-1",
    "href": "slides/week-13.2.html#background-styling-1",
    "title": "Revealjs Slides",
    "section": "Background Styling",
    "text": "Background Styling\n\nYou can change the color of your background by adding the background-color attribute to a slide.\n\n## Slide Title {background-color=\"aquamarine\"}"
  },
  {
    "objectID": "slides/week-13.2.html#background-styling-2",
    "href": "slides/week-13.2.html#background-styling-2",
    "title": "Revealjs Slides",
    "section": "Background Styling",
    "text": "Background Styling\n\nSimilarly, you can add a background image to your slide by adding the background-image attribute.\n\n## Slide Title {background-image=\"/images/drr6502-img.png\" data-background-size=\"contain\" data-background-opacity=\"0.5\"}"
  },
  {
    "objectID": "slides/week-13.2.html#background-styling-3",
    "href": "slides/week-13.2.html#background-styling-3",
    "title": "Revealjs Slides",
    "section": "Background Styling",
    "text": "Background Styling\n\nSimilarly, you can add a background image to your slide by adding the background-image attribute.\n\n## Slide Title {background-image=\"/images/drr6502-img.png\" data-background-size=\"contain\" data-background-opacity=\"0.5\"}"
  },
  {
    "objectID": "slides/week-13.2.html#background-styling-4",
    "href": "slides/week-13.2.html#background-styling-4",
    "title": "Revealjs Slides",
    "section": "Background Styling",
    "text": "Background Styling\n\nAnd you can add a background image to the title slide by adding the title-slide-attributes attribute to the YAML header.\n\n\n---\ntitle: My Slide Show\ntitle-slide-attributes:\n    data-background-image: /path/to/title_image.png\n    data-background-size: contain\n    data-background-opacity: \"0.5\"\n---\n\nSee here for an example."
  },
  {
    "objectID": "slides/week-13.2.html#a-few-more-tricks-and-tips",
    "href": "slides/week-13.2.html#a-few-more-tricks-and-tips",
    "title": "Revealjs Slides",
    "section": "A Few More Tricks and Tips",
    "text": "A Few More Tricks and Tips\n\nFade your slide transitions with transition: fade\nAdd slide numbers with slide-number: true\nUse footer:to add a footer your slides\nUse logo: to add a logo to your slides\nYou can add a chalkboard to your slides by adding chalkboard: true to the YAML header\nYou can add speaker notes by creating a div and adding the notes attribute to it.\n\nThen you can view them by running the presentation in speaker mode\n\nYou can use multiplex: true to advance slides for your audience\nAnd much much more! Check out the guide for details\n\n\nThese are my speaker notes!"
  },
  {
    "objectID": "slides/week-13.2.html#my-yaml-for-this-presentation",
    "href": "slides/week-13.2.html#my-yaml-for-this-presentation",
    "title": "Revealjs Slides",
    "section": "My YAML for This Presentation",
    "text": "My YAML for This Presentation\n ---\ntitle: \"Revealjs Slides\"\nsubtitle: \"Session 4--Visualizing Data\"\nfooter: \"[DRR Website](https://quarto.training)\"\nlogo: images/drr6502-logo.png\nformat:\n  revealjs:\n    theme: [simple, custom.scss]\n    transition: fade\n    slide-number: true\n    chalkboard: true\nexecute:\n  echo: false\n  message: false\n  warning: false\n  freeze: auto\n---"
  },
  {
    "objectID": "slides/week-13.2.html#your-turn-3",
    "href": "slides/week-13.2.html#your-turn-3",
    "title": "Revealjs Slides",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nAdd a theme to your presentation\nTry a different type of syntax highlighting\nChange the background color of a slide\nAdd a background image to a slide\nAdd a background image to the title slide\nTry using SCSS to modify the theme style\nTry a cool trick like chalkboard or multiplex\nUpload your presentation to Quarto Pub\n\nquarto publish quarto-pub mydocument.qmd"
  },
  {
    "objectID": "slides/week-3.1.html#types-of-visualizations",
    "href": "slides/week-3.1.html#types-of-visualizations",
    "title": "Grammar of Graphics",
    "section": "Types of Visualizations",
    "text": "Types of Visualizations\n\n\nColumn charts (bar charts)\n\nUse to compare values across categories\n\nHistograms\n\nUse to show distribution of a single variable\n\nLine charts\n\nUse to show trends over time\nCan use column charts but not as effective\n\nScatter plots\n\nUse to show relationships between two variables\nX-axis is usually explanatory variable, Y-axis is outcome variable"
  },
  {
    "objectID": "slides/week-3.1.html#the-grammar-of-graphics",
    "href": "slides/week-3.1.html#the-grammar-of-graphics",
    "title": "Grammar of Graphics",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\nData viz has a language with its own grammar\nBasic components include:\n\nData we are trying to visualize\nAesthetics (dimensions)\nGeom (e.g. bar, line, scatter plot)\nColor scales\nThemes\nAnnotations"
  },
  {
    "objectID": "slides/week-3.1.html#section",
    "href": "slides/week-3.1.html#section",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Let’s start with the first two, the data and the aesthetic, with a column chart example…\n\n\nlibrary(readr)\nlibrary(ggplot2)\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")\n\nggplot(dem_summary, aes(x = region, y = polyarchy))"
  },
  {
    "objectID": "slides/week-3.1.html#section-1",
    "href": "slides/week-3.1.html#section-1",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "This gives us the axes without any visualization:"
  },
  {
    "objectID": "slides/week-3.1.html#section-2",
    "href": "slides/week-3.1.html#section-2",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Now let’s add a geom. In this case we want a column chart so we add geom_col().\n\n\nggplot(dem_summary, aes(x = region, y = polyarchy)) + \n  geom_col()"
  },
  {
    "objectID": "slides/week-3.1.html#section-3",
    "href": "slides/week-3.1.html#section-3",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "That gets the idea across but looks a little depressing, so…"
  },
  {
    "objectID": "slides/week-3.1.html#section-4",
    "href": "slides/week-3.1.html#section-4",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "…let’s change the color of the columns by specifying fill = \"steelblue\".\n\n\nggplot(dem_summary, aes(x = region, y = polyarchy)) + \n  geom_col(fill = \"steelblue\")\n\n\n\n\n\n\n\n\nTip\n\n\nSee here for more available ggplot2 colors."
  },
  {
    "objectID": "slides/week-3.1.html#section-5",
    "href": "slides/week-3.1.html#section-5",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Note how color of original columns is simply overwritten:"
  },
  {
    "objectID": "slides/week-3.1.html#section-6",
    "href": "slides/week-3.1.html#section-6",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Now let’s add some labels with the labs() function:\n\n\nggplot(dem_summary, aes(x = region, y = polyarchy)) + \n  geom_col(fill = \"steelblue\") +\n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )"
  },
  {
    "objectID": "slides/week-3.1.html#section-7",
    "href": "slides/week-3.1.html#section-7",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "And that gives us…"
  },
  {
    "objectID": "slides/week-3.1.html#section-8",
    "href": "slides/week-3.1.html#section-8",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Next, we reorder the bars with fct_reorder() from the forcats package.\n\n\nlibrary(forcats)\n\nggplot(dem_summary, aes(x = fct_reorder(region, -polyarchy), y = polyarchy)) +\n  geom_col(fill = \"steelblue\") + \n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )\n\n\nNote that we could also use the base R reorder() function here."
  },
  {
    "objectID": "slides/week-3.1.html#section-9",
    "href": "slides/week-3.1.html#section-9",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "This way, we get a nice, visually appealing ordering of the bars according to levels of democracy…"
  },
  {
    "objectID": "slides/week-3.1.html#section-10",
    "href": "slides/week-3.1.html#section-10",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Now let’s change the theme to theme_minimal().\n\n\nggplot(dem_summary, aes(x = reorder(region, -polyarchy), y = polyarchy)) +\n  geom_col(fill = \"steelblue\") + \n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    ) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nTip\n\n\nSee here for available ggplot2 themes."
  },
  {
    "objectID": "slides/week-3.1.html#section-11",
    "href": "slides/week-3.1.html#section-11",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Gives us a clean, elegant look."
  },
  {
    "objectID": "slides/week-3.1.html#section-12",
    "href": "slides/week-3.1.html#section-12",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Note that you can also save your plot as an object to modify later.\n\n\ndem_bar_chart &lt;- ggplot(dem_summary, aes(x = reorder(region, -polyarchy), y = polyarchy)) +\n  geom_col(fill = \"steelblue\")"
  },
  {
    "objectID": "slides/week-3.1.html#section-13",
    "href": "slides/week-3.1.html#section-13",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Which gives us…\n\ndem_bar_chart"
  },
  {
    "objectID": "slides/week-3.1.html#section-14",
    "href": "slides/week-3.1.html#section-14",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Now let’s add back our labels…\n\n\ndem_bar_chart &lt;- dem_bar_chart +\n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )"
  },
  {
    "objectID": "slides/week-3.1.html#section-15",
    "href": "slides/week-3.1.html#section-15",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "So now we have…\n\ndem_bar_chart"
  },
  {
    "objectID": "slides/week-3.1.html#section-16",
    "href": "slides/week-3.1.html#section-16",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "And now we’ll add back our theme…\n\n\ndem_bar_chart &lt;- dem_bar_chart + theme_minimal()"
  },
  {
    "objectID": "slides/week-3.1.html#section-17",
    "href": "slides/week-3.1.html#section-17",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Voila!\n\ndem_bar_chart"
  },
  {
    "objectID": "slides/week-3.1.html#section-18",
    "href": "slides/week-3.1.html#section-18",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Change the theme. There are many themes to choose from.\n\ndem_bar_chart + theme_bw()"
  },
  {
    "objectID": "slides/week-3.1.html#your-turn",
    "href": "slides/week-3.1.html#your-turn",
    "title": "Grammar of Graphics",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nglimpse() the data\nFind a new variable to visualize1\nMake a bar chart with it\nChange the color of the bars\nOrder the bars\nAdd labels\nAdd a theme\nTry saving your plot as an object\nThen change the labels and/or theme\n\n\n\n\n−+\n10:00\n\n\n\nData for GDP per capita spotty after 2015"
  },
  {
    "objectID": "slides/week-3.1.html#purpose-of-histograms",
    "href": "slides/week-3.1.html#purpose-of-histograms",
    "title": "Grammar of Graphics",
    "section": "Purpose of Histograms",
    "text": "Purpose of Histograms\n\n\nHistograms are used to visualize the distribution of a single variable\nx-axis represents value of variable of interest\ny-axis represents the frequency of that value"
  },
  {
    "objectID": "slides/week-3.1.html#purpose-of-histograms-1",
    "href": "slides/week-3.1.html#purpose-of-histograms-1",
    "title": "Grammar of Graphics",
    "section": "Purpose of Histograms",
    "text": "Purpose of Histograms\n\n\nThey are generally used for continuous variables (e.g., income, age, etc.)\n\nA continuous variable is one that can take on any value within a range (e.g., 0.5, 1.2, 3.7, etc.)\nA discrete variable is one that can only take on certain values (e.g., 1, 2, 3, etc.)\n\nTypically, the height of the bar represents the number of observations which fall in that bin"
  },
  {
    "objectID": "slides/week-3.1.html#example",
    "href": "slides/week-3.1.html#example",
    "title": "Grammar of Graphics",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/week-3.1.html#histogram-code",
    "href": "slides/week-3.1.html#histogram-code",
    "title": "Grammar of Graphics",
    "section": "Histogram Code",
    "text": "Histogram Code\n\n\n# load dplyr\n\nlibrary(dplyr)\n\n# load data\ndem_women &lt;- read_csv(\"data/dem_women.csv\")\n\n# filter to 2022\ndem_women_2022 &lt;- dem_women |&gt;\n  filter(year == 2022) \n\n# create histogram\nggplot(dem_women_2022, aes(x = flfp)) +\n  geom_histogram(fill = \"steelblue\") + \n  labs(\n    x = \"Percentage of Working Aged Women in Labor Force\",\n    y = \"Number of Countries\",\n    title = \"Female labor force participation rates, 2022\",\n    caption = \"Source: World Bank\"\n    ) + theme_minimal()"
  },
  {
    "objectID": "slides/week-3.1.html#histogram-code-1",
    "href": "slides/week-3.1.html#histogram-code-1",
    "title": "Grammar of Graphics",
    "section": "Histogram Code",
    "text": "Histogram Code\n\nNote that you only need to specify the x axis variable in the aes() function. ggplot2 will automatically visualize the y-axis for a histogram.\n\n\nggplot(dem_women_2022, aes(x = flfp)) +\n  geom_histogram(bins = 50, fill = \"steelblue\") + \n  labs(\n    x = \"Percentage of Working Aged Women in Labor Force\",\n    y = \"Number of Countries\",\n    title = \"Female labor force participation rates, 2022\",\n    caption = \"Source: World Bank\"\n    ) + theme_minimal()"
  },
  {
    "objectID": "slides/week-3.1.html#change-number-of-bins",
    "href": "slides/week-3.1.html#change-number-of-bins",
    "title": "Grammar of Graphics",
    "section": "Change Number of Bins",
    "text": "Change Number of Bins\n\nChange number of bins (bars) using bins or binwidth arguments (default number of bins = 30):\n\n\nggplot(dem_women_2022, aes(x = flfp)) +\n  geom_histogram(bins = 50, fill = \"steelblue\") + \n  labs(\n    x = \"Percentage of Working Aged Women in Labor Force\",\n    y = \"Number of Countries\",\n    title = \"Female labor force participation rates, 2022\",\n    caption = \"Source: World Bank\"\n    ) + theme_minimal()"
  },
  {
    "objectID": "slides/week-3.1.html#section-19",
    "href": "slides/week-3.1.html#section-19",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "At 50 bins…"
  },
  {
    "objectID": "slides/week-3.1.html#section-20",
    "href": "slides/week-3.1.html#section-20",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "At 100 bins…probably too many!"
  },
  {
    "objectID": "slides/week-3.1.html#section-21",
    "href": "slides/week-3.1.html#section-21",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Using binwidth instead of bins…\n\n\nggplot(dem_women_2022, aes(x = flfp)) +\n  geom_histogram(binwidth = 2, fill = \"steelblue\") + \n  labs(\n    x = \"Percentage of Working Aged Women in Labor Force\",\n    y = \"Number of Countries\",\n    title = \"Female labor force participation rates, 2022\",\n    caption = \"Source: World Bank\"\n    ) + theme_minimal()"
  },
  {
    "objectID": "slides/week-3.1.html#section-22",
    "href": "slides/week-3.1.html#section-22",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Setting binwidth to 2…"
  },
  {
    "objectID": "slides/week-3.1.html#change-from-count-to-density",
    "href": "slides/week-3.1.html#change-from-count-to-density",
    "title": "Grammar of Graphics",
    "section": "Change from Count to Density",
    "text": "Change from Count to Density\n\n\nggplot(dem_women_2022, aes(after_stat(density), x = flfp)) +\n  geom_histogram(fill = \"steelblue\") + \n  labs(\n    x = \"Percentage of Working Aged Women in Labor Force\",\n    y = \"Density\",\n    title = \"Female labor force participation rates, 2022\",\n    caption = \"Source: World Bank\"\n    ) + theme_minimal()\n\n\nFor densities, the total area sums to 1. The height of a bar represents the probability of observations in that bin (rather than the number of observations)."
  },
  {
    "objectID": "slides/week-3.1.html#section-23",
    "href": "slides/week-3.1.html#section-23",
    "title": "Grammar of Graphics",
    "section": "",
    "text": "Which gives us…"
  },
  {
    "objectID": "slides/week-3.1.html#your-turn-1",
    "href": "slides/week-3.1.html#your-turn-1",
    "title": "Grammar of Graphics",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nPick a variable that you want to explore the distribution of1\nMake a histogram\n\nOnly specify x = in aes()\nSpecify geom as geom_histogram\n\nChoose color for bars\nChoose appropriate labels\nChange number of bins\nChange from count to density\n\n\n\n\n−+\n10:00\n\n\n\n\n\nData for GDP per capita spotty after 2015"
  },
  {
    "objectID": "slides/week-13.1.html#hypotheses",
    "href": "slides/week-13.1.html#hypotheses",
    "title": "Hypothesis Testing 2",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n\nNull hypothesis: there is no relationship between treatment and outcome, the difference is due to chance\nAlternative hypothesis: there is a relationship, the difference is not due to chance"
  },
  {
    "objectID": "slides/week-13.1.html#approach",
    "href": "slides/week-13.1.html#approach",
    "title": "Hypothesis Testing 2",
    "section": "Approach",
    "text": "Approach\n\n\nUnder the null hypothesis, treatment has NO impact on y (the outcome)\nThis means that if we were to change the values of the treatment variable, the values on y would stay the same"
  },
  {
    "objectID": "slides/week-13.1.html#approach-1",
    "href": "slides/week-13.1.html#approach-1",
    "title": "Hypothesis Testing 2",
    "section": "Approach",
    "text": "Approach\n\nSo…we can simulate the null distribution by:\n\nReshuffling the treatment variable\nCalculating the treatment effect\nRepeating many times\n\n\n\n\nThen we can ask: how likely would we be to observe the treatment effect in our data, if there is no effect of the treatment?"
  },
  {
    "objectID": "slides/week-13.1.html#résumé-experiment-example",
    "href": "slides/week-13.1.html#résumé-experiment-example",
    "title": "Hypothesis Testing 2",
    "section": "Résumé Experiment Example",
    "text": "Résumé Experiment Example\nBertrand and Mullainathan studied racial discrimination in responses to job applications in Chicago and Boston. They sent 4,870 résumés, randomly assigning names associated with different racial groups. - Data are in openintro package as an object called resume - I will save as myDat\n\nlibrary(openintro)\nmyDat &lt;- resume"
  },
  {
    "objectID": "slides/week-13.1.html#callbacks-by-race",
    "href": "slides/week-13.1.html#callbacks-by-race",
    "title": "Hypothesis Testing 2",
    "section": "Callbacks by Race",
    "text": "Callbacks by Race\n\nRemember, race of applicant is randomly assigned.\n\nlibrary(tidyverse)\n\nmns &lt;- myDat |&gt;\n  group_by(race) |&gt; \n  summarize(calls = mean(received_callback))\nmns\n\n# A tibble: 2 × 2\n  race   calls\n  &lt;chr&gt;  &lt;dbl&gt;\n1 black 0.0645\n2 white 0.0965"
  },
  {
    "objectID": "slides/week-13.1.html#section",
    "href": "slides/week-13.1.html#section",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "Let’s save the means for white and black applicants.\n\n\nmean_white = mns$calls[2]\nmean_black = mns$calls[1]"
  },
  {
    "objectID": "slides/week-13.1.html#section-1",
    "href": "slides/week-13.1.html#section-1",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "And calculate the treatment effect. The treatment effect is the difference in means.\n\n\nteffect &lt;- mean_white - mean_black\nteffect\n\n[1] 0.03203285"
  },
  {
    "objectID": "slides/week-13.1.html#section-2",
    "href": "slides/week-13.1.html#section-2",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "Before formal tests, let’s look at the data–the estimates and the confidence intervals…"
  },
  {
    "objectID": "slides/week-13.1.html#section-3",
    "href": "slides/week-13.1.html#section-3",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "First, let’s make the CIs for the white applicants.\n\n\nlibrary(tidymodels)\nboot_df_white &lt;- myDat |&gt;\n  filter(race == \"white\") |&gt; \n  specify(response = received_callback) |&gt;  \n  generate(reps = 15000, type = \"bootstrap\") |&gt; \n  calculate(stat = \"mean\")\nlower_bound_white &lt;- boot_df_white |&gt; summarize(lower_bound_white = quantile(stat, 0.025)) |&gt; pull() \nupper_bound_white &lt;- boot_df_white |&gt; summarize(upper_bound_white = quantile(stat, 0.975)) |&gt; pull()"
  },
  {
    "objectID": "slides/week-13.1.html#section-4",
    "href": "slides/week-13.1.html#section-4",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "Now, let’s create the CIs for black applicants.\n\n\nboot_df_black &lt;- myDat |&gt;\n  filter(race == \"black\") |&gt; \n  specify(response = received_callback) |&gt;  \n  generate(reps = 15000, type = \"bootstrap\") |&gt; \n  calculate(stat = \"mean\")\nlower_bound_black &lt;- boot_df_black |&gt; summarize(lower_bound_black = quantile(stat, 0.025)) |&gt; pull() \nupper_bound_black &lt;- boot_df_black |&gt; summarize(upper_bound_black = quantile(stat, 0.975)) |&gt; pull()"
  },
  {
    "objectID": "slides/week-13.1.html#section-5",
    "href": "slides/week-13.1.html#section-5",
    "title": "Hypothesis Testing 2",
    "section": "",
    "text": "Now, let’s tidy the data for plotting.\n\n\nplotData &lt;- tibble(\n  race = c(\"Black\", \"White\"),\n  meanCalls = c(mean_black, mean_white),\n  lower95 = c(lower_bound_black, lower_bound_white),\n  upper95 = c(upper_bound_black, upper_bound_white)\n)\nplotData\n\n# A tibble: 2 × 4\n  race  meanCalls lower95 upper95\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Black    0.0645  0.0550  0.0743\n2 White    0.0965  0.0846  0.108"
  },
  {
    "objectID": "slides/week-13.1.html#plot",
    "href": "slides/week-13.1.html#plot",
    "title": "Hypothesis Testing 2",
    "section": "Plot",
    "text": "Plot"
  },
  {
    "objectID": "slides/week-13.1.html#plot-1",
    "href": "slides/week-13.1.html#plot-1",
    "title": "Hypothesis Testing 2",
    "section": "Plot",
    "text": "Plot\n\n\nggplot(plotData, aes(y = meanCalls, x = race, ymin = lower95, ymax = upper95)) +\n  geom_col(fill = \"steelblue4\") +\n  geom_errorbar(width = .05) +\n  theme_bw()  +\n  ylim(0, .15) +\n  labs(x = \"Race of Applicant\",\n       y = \"Call Back Rate\")"
  },
  {
    "objectID": "slides/week-13.1.html#is-this-evidence-of-racial-discrimination",
    "href": "slides/week-13.1.html#is-this-evidence-of-racial-discrimination",
    "title": "Hypothesis Testing 2",
    "section": "Is this evidence of racial discrimination?",
    "text": "Is this evidence of racial discrimination?\n\n\nWhat is the null hypothesis?\n\n\n\nWhat is the alternative hypothesis?\n\n\n\n\nHow can we formally test the null hypothesis to decide whether to reject it?"
  },
  {
    "objectID": "slides/week-13.1.html#formal-hypothesis-test",
    "href": "slides/week-13.1.html#formal-hypothesis-test",
    "title": "Hypothesis Testing 2",
    "section": "Formal Hypothesis Test",
    "text": "Formal Hypothesis Test\n\n\nCalculate the difference in means (White - Black)\nShuffle the race variable\nCalculate the difference in means for the shuffled data\nRepeat many times\nSimulates the null distribution of differences in callbacks"
  },
  {
    "objectID": "slides/week-13.1.html#hypothetical-original-data",
    "href": "slides/week-13.1.html#hypothetical-original-data",
    "title": "Hypothesis Testing 2",
    "section": "Hypothetical Original Data",
    "text": "Hypothetical Original Data\n\n\n\n\nApplicant\nRace\nCallback\n\n\n\n\nA\nBlack\nYes\n\n\nB\nBlack\nNo\n\n\nC\nBlack\nNo\n\n\nD\nWhite\nYes\n\n\nE\nWhite\nNo\n\n\nF\nWhite\nNo"
  },
  {
    "objectID": "slides/week-13.1.html#step-1-calculate-original-difference-in-callback-rates",
    "href": "slides/week-13.1.html#step-1-calculate-original-difference-in-callback-rates",
    "title": "Hypothesis Testing 2",
    "section": "Step 1: Calculate Original Difference in Callback Rates",
    "text": "Step 1: Calculate Original Difference in Callback Rates\n\n\nObjective: Understand initial association between race and callback rates"
  },
  {
    "objectID": "slides/week-13.1.html#step-2-shuffle-permute-the-race-variable",
    "href": "slides/week-13.1.html#step-2-shuffle-permute-the-race-variable",
    "title": "Hypothesis Testing 2",
    "section": "Step 2: Shuffle (Permute) the Race Variable",
    "text": "Step 2: Shuffle (Permute) the Race Variable\n\n\nMethod: Randomly reassign race labels, keeping callback outcomes fixed"
  },
  {
    "objectID": "slides/week-13.1.html#hypothetical-shuffled-data",
    "href": "slides/week-13.1.html#hypothetical-shuffled-data",
    "title": "Hypothesis Testing 2",
    "section": "Hypothetical Shuffled Data",
    "text": "Hypothetical Shuffled Data\n\n\n\n\nApplicant\nRace (Shuffled)\nCallback\n\n\n\n\nA\nWhite\nYes\n\n\nB\nBlack\nNo\n\n\nC\nWhite\nNo\n\n\nD\nWhite\nYes\n\n\nE\nBlack\nNo\n\n\nF\nBlack\nNo"
  },
  {
    "objectID": "slides/week-13.1.html#step-3-calculate-difference-in-callback-rates-again",
    "href": "slides/week-13.1.html#step-3-calculate-difference-in-callback-rates-again",
    "title": "Hypothesis Testing 2",
    "section": "Step 3: Calculate Difference in Callback Rates Again",
    "text": "Step 3: Calculate Difference in Callback Rates Again\n\n\nAfter Shuffling: Calculate the difference in callback rates between Black and White groups\nPurpose: Determine if observed difference is due to chance"
  },
  {
    "objectID": "slides/week-13.1.html#repeat-many-times",
    "href": "slides/week-13.1.html#repeat-many-times",
    "title": "Hypothesis Testing 2",
    "section": "Repeat Many Times",
    "text": "Repeat Many Times\n\n\nRepeat shuffling 5000 times to generate a distribution of differences by chance\nTest: Compare observed difference to null distribution to assess effect of race on callbacks\nIf observed difference is extreme (p-value is low), reject the null hypothesis"
  },
  {
    "objectID": "slides/week-13.1.html#simulating-with-tidymodels",
    "href": "slides/week-13.1.html#simulating-with-tidymodels",
    "title": "Hypothesis Testing 2",
    "section": "Simulating with tidymodels",
    "text": "Simulating with tidymodels\n\nIn real life we are going to use the tidymodels package to do the simulation for us.\n\nnull_dist &lt;- myDat |&gt;\n  specify(response = received_callback, explanatory = race) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(5000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in means\", \n            order = c(\"white\", \"black\")) #"
  },
  {
    "objectID": "slides/week-13.1.html#get-the-p-value",
    "href": "slides/week-13.1.html#get-the-p-value",
    "title": "Hypothesis Testing 2",
    "section": "Get the p-value",
    "text": "Get the p-value\n\nLet’s get the p-value with get_pvalue from the infer package.\n\n\nget_p_value(null_dist, obs_stat = teffect, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0"
  },
  {
    "objectID": "slides/week-13.1.html#visualize-the-null-distribution",
    "href": "slides/week-13.1.html#visualize-the-null-distribution",
    "title": "Hypothesis Testing 2",
    "section": "Visualize the Null Distribution",
    "text": "Visualize the Null Distribution\n\n\nvisualize(null_dist) +\n  shade_p_value(obs_stat = teffect, direction = \"greater\") +\n  labs(\n    x = \"Estimated Difference under the Null\",\n    y = \"Count\"\n  ) + \n  theme_bw()"
  },
  {
    "objectID": "slides/week-13.1.html#what-should-we-conclude",
    "href": "slides/week-13.1.html#what-should-we-conclude",
    "title": "Hypothesis Testing 2",
    "section": "What should we conclude?",
    "text": "What should we conclude?\n\n\nThe p-value is very small (below .05 threshold)\nTherefore, we reject the null hypothesis: the racial gap is extremely unlikely to have occurred due to chance alone\nThis is evidence of racial discrimination"
  },
  {
    "objectID": "slides/week-13.1.html#your-turn",
    "href": "slides/week-13.1.html#your-turn",
    "title": "Hypothesis Testing 2",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nUse the gender variable in the resume data to assess whether there is gender discrimination in call backs\nPlot means and 95% confidence intervals for the call back rate for men and women\nWrite the null and alternative hypotheses\nSimulate the null distribution\nVisualize the null distribution and the gender gap\nCalculate the p-value\nWhat do you conclude from your test?"
  },
  {
    "objectID": "slides/week-3.2.html#line-charts",
    "href": "slides/week-3.2.html#line-charts",
    "title": "Which Visualization Should I Use?",
    "section": "Line Charts",
    "text": "Line Charts\n\n\nLine charts are used to show trends over time\nYou especially want to use a line chart when you have multiple cases or categories that you want to compare over time"
  },
  {
    "objectID": "slides/week-3.2.html#huntingtons-three-waves",
    "href": "slides/week-3.2.html#huntingtons-three-waves",
    "title": "Which Visualization Should I Use?",
    "section": "Huntington’s Three Waves",
    "text": "Huntington’s Three Waves"
  },
  {
    "objectID": "slides/week-3.2.html#line-chart-example",
    "href": "slides/week-3.2.html#line-chart-example",
    "title": "Which Visualization Should I Use?",
    "section": "Line Chart Example",
    "text": "Line Chart Example"
  },
  {
    "objectID": "slides/week-3.2.html#section",
    "href": "slides/week-3.2.html#section",
    "title": "Which Visualization Should I Use?",
    "section": "",
    "text": "Here is the code…\n\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  )"
  },
  {
    "objectID": "slides/week-3.2.html#section-1",
    "href": "slides/week-3.2.html#section-1",
    "title": "Which Visualization Should I Use?",
    "section": "",
    "text": "Use geom_line() to specify a line chart…\n\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  )"
  },
  {
    "objectID": "slides/week-3.2.html#section-2",
    "href": "slides/week-3.2.html#section-2",
    "title": "Which Visualization Should I Use?",
    "section": "",
    "text": "Add third dimension to the aes() call for line color…\n\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  )"
  },
  {
    "objectID": "slides/week-3.2.html#section-3",
    "href": "slides/week-3.2.html#section-3",
    "title": "Which Visualization Should I Use?",
    "section": "",
    "text": "Modify the legend title…\n\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  )"
  },
  {
    "objectID": "slides/week-3.2.html#your-turn",
    "href": "slides/week-3.2.html#your-turn",
    "title": "Which Visualization Should I Use?",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nSee table three of this article\nSelect three countries to visualize\nAdjust setup code to filter data on those countries\nVisualize with geom_line()\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-3.2.html#scatter-plots",
    "href": "slides/week-3.2.html#scatter-plots",
    "title": "Which Visualization Should I Use?",
    "section": "Scatter Plots",
    "text": "Scatter Plots\n\n\nScatter plots are used to show the relationship between two variables\nFrequently the outcome variable is on the y-axis and the predictor variable is on the x-axis\nIn addition to the points, you can use color, size, and shape to add more information to the plot"
  },
  {
    "objectID": "slides/week-3.2.html#scatter-plot-setup",
    "href": "slides/week-3.2.html#scatter-plot-setup",
    "title": "Which Visualization Should I Use?",
    "section": "Scatter Plot Setup",
    "text": "Scatter Plot Setup\n\n\ndem_summary_ctry &lt;- read_csv(\"data/dem_women.csv\") |&gt;\n  group_by(country, region) |&gt; # group by country, keep region\n  summarize(\n    polyarchy = mean(polyarchy, na.rm = TRUE),\n    gdp_pc = mean(gdp_pc, na.rm = TRUE), \n    flfp = mean(flfp, na.rm = TRUE), \n    women_rep = mean(women_rep, na.rm = TRUE)\n  )"
  },
  {
    "objectID": "slides/week-3.2.html#scatter-plot",
    "href": "slides/week-3.2.html#scatter-plot",
    "title": "Which Visualization Should I Use?",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\n\nCode\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy, color = region, size = women_rep)) + \n  geom_point() + # use geom_point() for scatter plots\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\",\n    size = \"Women Reps\"\n    )"
  },
  {
    "objectID": "slides/week-3.2.html#scatter-plot-1",
    "href": "slides/week-3.2.html#scatter-plot-1",
    "title": "Which Visualization Should I Use?",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nUse geom_point()…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy, color = region, size = women_rep)) +\n  geom_point() + # use geom_point() for scatter plots\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\",\n    size = \"Women Reps\"\n    )"
  },
  {
    "objectID": "slides/week-3.2.html#scatter-plot-2",
    "href": "slides/week-3.2.html#scatter-plot-2",
    "title": "Which Visualization Should I Use?",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nFour dimensions…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy, color = region, size = women_rep)) + \n  geom_point() + # use geom_point() for scatter plots\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\",\n    size = \"Women Reps\"\n    )"
  },
  {
    "objectID": "slides/week-3.2.html#scatter-plot-3",
    "href": "slides/week-3.2.html#scatter-plot-3",
    "title": "Which Visualization Should I Use?",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nStretch axis on log scale and use scales package to adjust labels…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy, color = region, size = women_rep)) + \n  geom_point() + # use geom_point() for scatter plots\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\",\n    size = \"Women Reps\"\n    )"
  },
  {
    "objectID": "slides/week-3.2.html#scatter-plot-4",
    "href": "slides/week-3.2.html#scatter-plot-4",
    "title": "Which Visualization Should I Use?",
    "section": "Scatter Plot",
    "text": "Scatter Plot\n\nChange legend titles…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy, color = region, size = women_rep)) + \n  geom_point() + # use geom_point() for scatter plots\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\",\n    size = \"Women Reps\"\n    )"
  },
  {
    "objectID": "slides/week-3.2.html#your-turn-1",
    "href": "slides/week-3.2.html#your-turn-1",
    "title": "Which Visualization Should I Use?",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nThere are four variables in dem_summary_ctry\nPick one related to women’s empowerment\nVisualize it on the y-axis with gdp_pc or polyarchy on the x-axis\nChange labels and legend titles to match your visualization\nInterpret your plot\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-3.2.html#add-a-trend-line",
    "href": "slides/week-3.2.html#add-a-trend-line",
    "title": "Which Visualization Should I Use?",
    "section": "Add a Trend Line",
    "text": "Add a Trend Line"
  },
  {
    "objectID": "slides/week-3.2.html#add-a-trend-line-1",
    "href": "slides/week-3.2.html#add-a-trend-line-1",
    "title": "Which Visualization Should I Use?",
    "section": "Add a Trend Line",
    "text": "Add a Trend Line\n\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    )"
  },
  {
    "objectID": "slides/week-3.2.html#add-a-trend-line-2",
    "href": "slides/week-3.2.html#add-a-trend-line-2",
    "title": "Which Visualization Should I Use?",
    "section": "Add a Trend Line",
    "text": "Add a Trend Line\n\nTaking out size and adding color to geom_point() call…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    )"
  },
  {
    "objectID": "slides/week-3.2.html#add-a-trend-line-3",
    "href": "slides/week-3.2.html#add-a-trend-line-3",
    "title": "Which Visualization Should I Use?",
    "section": "Add a Trend Line",
    "text": "Add a Trend Line\n\nChanging legend titles…\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    )"
  },
  {
    "objectID": "slides/week-3.2.html#your-turn-2",
    "href": "slides/week-3.2.html#your-turn-2",
    "title": "Which Visualization Should I Use?",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nAdd a trendline to your plot\nChange the labels accordingly\nTry using method = \"loess\" instead of a “lm”\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-3.2.html#which-plot-should-you-use",
    "href": "slides/week-3.2.html#which-plot-should-you-use",
    "title": "Which Visualization Should I Use?",
    "section": "Which Plot Should You Use?",
    "text": "Which Plot Should You Use?\n\n\nTrends in stock process over time\nDistribution of income in a country\nComparison of FLFP across MENA countries\nRelationship between poverty and inequality (cross-nationally)"
  },
  {
    "objectID": "slides/week-3.2.html#which-geom-would-you-use",
    "href": "slides/week-3.2.html#which-geom-would-you-use",
    "title": "Which Visualization Should I Use?",
    "section": "Which Geom Would You Use?",
    "text": "Which Geom Would You Use?\n\n\nColumn chart\nHistogram\nLine chart\nScatter plot"
  },
  {
    "objectID": "slides/week-3.2.html#other-plots-and-geometries",
    "href": "slides/week-3.2.html#other-plots-and-geometries",
    "title": "Which Visualization Should I Use?",
    "section": "Other Plots and Geometries",
    "text": "Other Plots and Geometries\n\n\n\n\nBox Plot\ngeom_boxplot()\nViolin Plot\ngeom_violin()\nDensity Plot\ngeom_density()\nBar Plot (Categorical)\ngeom_bar()\nHeatmap\ngeom_tile()\n\n\n\nArea Plot\ngeom_area()\nDot Plot\ngeom_dotplot()\nPie Chart\n(usually a bar plot with coord_polar())\nRidgeline Plot\nggridges::geom_density_ridges()\nMap Plot (Choropleth)\ngeom_polygon()"
  },
  {
    "objectID": "instructor.html",
    "href": "instructor.html",
    "title": "Instructor",
    "section": "",
    "text": "Emmanuel Teitelbaum is an associate professor of political science and international affairs at the The George Washington University His research and writing explore how class conflict and compromise intersect with democracy and development. He also has a strong interest in labor standards and understanding how labor unions, nonprofit organizations, consumers and corporations can help to promote them.\nAt GW, Professor Teitelbaum teaches courses on comparative politics, comparative political economy and data science. He is on faculty in the Department of Political Science and the Elliott School of International Affairs and is affiliated with the Sigur Center for Asian Studies as well as the Institute for International Economic Policy.",
    "crumbs": [
      "Course information",
      "Instructor"
    ]
  },
  {
    "objectID": "weeks/week-14.html",
    "href": "weeks/week-14.html",
    "title": "Week 14",
    "section": "",
    "text": "📖 Aklin and Matto, “Prisoners of the Wrong Dilemma”"
  },
  {
    "objectID": "weeks/week-14.html#readings",
    "href": "weeks/week-14.html#readings",
    "title": "Week 14",
    "section": "",
    "text": "📖 Aklin and Matto, “Prisoners of the Wrong Dilemma”"
  },
  {
    "objectID": "weeks/week-14.html#slides",
    "href": "weeks/week-14.html#slides",
    "title": "Week 14",
    "section": "Slides",
    "text": "Slides\n🖥️ Climate change and collective action\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important\n\n\n\nExam 2 is due on Sunday, March 19."
  },
  {
    "objectID": "weeks/week-8.html#readings",
    "href": "weeks/week-8.html#readings",
    "title": "Week 8",
    "section": "Readings",
    "text": "Readings\n📖 James Putzel, “The ‘Populist’ Right Challenge to Neoliberalism”\n📖 Mary Cullen, “How to Write and Format a White Paper”"
  },
  {
    "objectID": "weeks/week-8.html#slides",
    "href": "weeks/week-8.html#slides",
    "title": "Week 8",
    "section": "Slides",
    "text": "Slides\n🖥️ Populism and social policy\n🖥️ Exam 2 review"
  },
  {
    "objectID": "weeks/week-8.html#assignments",
    "href": "weeks/week-8.html#assignments",
    "title": "Week 8",
    "section": "Assignments",
    "text": "Assignments\n📘 Exam 2\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Important\n\n\n\n“Project Assignment 2: White Paper” is due on Sunday, April 9"
  },
  {
    "objectID": "weeks/week-12.html#readings",
    "href": "weeks/week-12.html#readings",
    "title": "Week 12",
    "section": "Readings",
    "text": "Readings\n📖 Autor, Dorn and Hanson, “The China Shock”\n📖 Joseph Stiglitz, People, Power, and Profits, Chapter 3"
  },
  {
    "objectID": "weeks/week-12.html#slides",
    "href": "weeks/week-12.html#slides",
    "title": "Week 12",
    "section": "Slides",
    "text": "Slides\n🖥️ Globalization\n🖥️ Inequality"
  },
  {
    "objectID": "weeks/week-12.html#assignments",
    "href": "weeks/week-12.html#assignments",
    "title": "Week 12",
    "section": "Assignments",
    "text": "Assignments\n✍️ Project Assignment 2: White Paper\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "📖 Gumbrell-McCormick and Hyman, “Democracy in Trade Unions, Democracy Through Trade Unions?”\n📖 Hall and Soskice, Varities of Capitalism, Chapter 1"
  },
  {
    "objectID": "weeks/week-11.html#readings",
    "href": "weeks/week-11.html#readings",
    "title": "Week 11",
    "section": "",
    "text": "📖 Gumbrell-McCormick and Hyman, “Democracy in Trade Unions, Democracy Through Trade Unions?”\n📖 Hall and Soskice, Varities of Capitalism, Chapter 1"
  },
  {
    "objectID": "weeks/week-11.html#slides",
    "href": "weeks/week-11.html#slides",
    "title": "Week 11",
    "section": "Slides",
    "text": "Slides\n🖥️ Unions and collective bargaining\n🖥️ Varieties of capitalism\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "📖 HOPWR, The Very Basics\n📖 r4ds, Introduction",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#readings",
    "href": "weeks/week-2.html#readings",
    "title": "Week 2",
    "section": "",
    "text": "📖 HOPWR, The Very Basics\n📖 r4ds, Introduction",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#videos",
    "href": "weeks/week-2.html#videos",
    "title": "Week 2",
    "section": "Videos",
    "text": "Videos\n📺 Using Functions and Obejcts in R\n📺 Explore the Data Frame\n📺 Maintaining the House the Tidyverse Built",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#slides",
    "href": "weeks/week-2.html#slides",
    "title": "Week 2",
    "section": "Slides",
    "text": "Slides\n🖥 R Coding Basics\n🖥 Intro to the Tidyverse\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "📖 Review the course syllabus and support resources\n📖 HOPWR, Appendix A and B\n📖 Hello, Quarto\n📖 Markdown Guide",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#readings",
    "href": "weeks/week-1.html#readings",
    "title": "Week 1",
    "section": "",
    "text": "📖 Review the course syllabus and support resources\n📖 HOPWR, Appendix A and B\n📖 Hello, Quarto\n📖 Markdown Guide",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#videos",
    "href": "weeks/week-1.html#videos",
    "title": "Week 1",
    "section": "Videos",
    "text": "Videos\n📺 Getting Started with R and RStudio\n📺 Getting Started with Quarto\n📺 Markdown Crash Course",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#slides",
    "href": "weeks/week-1.html#slides",
    "title": "Week 1",
    "section": "Slides",
    "text": "Slides\n🖥 Coding Examples\n🖥 Meet ouR Tech Stack\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "📖 Stokes et. al., Brokers, Voters and Clientelism, Chapter 1\n📖 Aspinall and Berenschot, Democracy for Sale, Chapter 5"
  },
  {
    "objectID": "weeks/week-5.html#readings",
    "href": "weeks/week-5.html#readings",
    "title": "Week 5",
    "section": "",
    "text": "📖 Stokes et. al., Brokers, Voters and Clientelism, Chapter 1\n📖 Aspinall and Berenschot, Democracy for Sale, Chapter 5"
  },
  {
    "objectID": "weeks/week-5.html#slides",
    "href": "weeks/week-5.html#slides",
    "title": "Week 5",
    "section": "Slides",
    "text": "Slides\n🖥️ Programmatic politics vs. clientelism\n🖥️ Vote buying\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "📖 Edward Miguel, “Tribe or Nation?”\n📖 Holland and Ben Ross Schneider, “Easy and Hard Redistribution”"
  },
  {
    "objectID": "weeks/week-6.html#readings",
    "href": "weeks/week-6.html#readings",
    "title": "Week 6",
    "section": "",
    "text": "📖 Edward Miguel, “Tribe or Nation?”\n📖 Holland and Ben Ross Schneider, “Easy and Hard Redistribution”"
  },
  {
    "objectID": "weeks/week-6.html#slides",
    "href": "weeks/week-6.html#slides",
    "title": "Week 6",
    "section": "Slides",
    "text": "Slides\n🖥️ Ethnicity and public goods\n🖥️ Democracy and redistribution"
  },
  {
    "objectID": "weeks/week-6.html#videos",
    "href": "weeks/week-6.html#videos",
    "title": "Week 6",
    "section": "Videos",
    "text": "Videos\n📺 Bar chart video\n📺 Line chart video\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "assignments/coding-assignment-1.html",
    "href": "assignments/coding-assignment-1.html",
    "title": "Homework 1",
    "section": "",
    "text": "For this assignment, you are going to evaluate modernization theory as laid out in Seymour Martin Lipset’s classic article entitled “Some Social Requisites of Democracy: Economic Development and Political Legitimacy.” How classic is this article? According to Google Scholar, this piece has been cited more than 11.5 thousand times!\nWe are going to use data from V-Dem and modern data viz tools to explore Lipset’s hypothesis that economic modernization is highly correlated with democracy. We have already done this to some extent by looking at the relationship between wealth and the polyarchy score. But we are going to broaden things out by looking at other measures of democracy contained in the V-Dem dataset. Specifically, you will get to choose between the following four measures:\n\nliberal democracy (libdem)\nparticipatory democracy (partipdem)\ndeliberative democracy (delibdem)\negalitarian democracy (egaldem).\n\nFor measuring modernization we are going to stick with GDP per capita (gdp_pc).\nStart by running this code chunk to import all of the packages you will need for this exercise. Then start working through the code and questions below. Feel free to grab relevant code chunks from the slides or the classwork QMD files that we have used in class so far.\n\nlibrary(tidyverse)",
    "crumbs": [
      "Coding Asignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-1.html#overview",
    "href": "assignments/coding-assignment-1.html#overview",
    "title": "Homework 1",
    "section": "",
    "text": "For this assignment, you are going to evaluate modernization theory as laid out in Seymour Martin Lipset’s classic article entitled “Some Social Requisites of Democracy: Economic Development and Political Legitimacy.” How classic is this article? According to Google Scholar, this piece has been cited more than 11.5 thousand times!\nWe are going to use data from V-Dem and modern data viz tools to explore Lipset’s hypothesis that economic modernization is highly correlated with democracy. We have already done this to some extent by looking at the relationship between wealth and the polyarchy score. But we are going to broaden things out by looking at other measures of democracy contained in the V-Dem dataset. Specifically, you will get to choose between the following four measures:\n\nliberal democracy (libdem)\nparticipatory democracy (partipdem)\ndeliberative democracy (delibdem)\negalitarian democracy (egaldem).\n\nFor measuring modernization we are going to stick with GDP per capita (gdp_pc).\nStart by running this code chunk to import all of the packages you will need for this exercise. Then start working through the code and questions below. Feel free to grab relevant code chunks from the slides or the classwork QMD files that we have used in class so far.\n\nlibrary(tidyverse)",
    "crumbs": [
      "Coding Asignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-1.html#step-1-make-a-line-chart-showing-country-trends-20-pts",
    "href": "assignments/coding-assignment-1.html#step-1-make-a-line-chart-showing-country-trends-20-pts",
    "title": "Homework 1",
    "section": "Step 1: Make a line chart showing country trends (20 pts)",
    "text": "Step 1: Make a line chart showing country trends (20 pts)\na) Read in the data from line_chart.csv using read_csv() in an object called line_chart_dta.\nb) Now use glimpse() to inspect the data. How many rows and columns are in the data frame? Write your answer below the code chunk\nc) Now click on the data frame in the Environment tab or use View() to scroll through the data frame. What countries, years and variables are in the data frame? What are the measures of democracy?\nd) Now choose one of the democracy indicators and visualize it with a line chart using ggplot2. Be sure to specify x =, y = and color = in your aes() call and use geom_line() to create the chart.Add appropriate axis labels, a title and a caption. Now add a colorblind-friendly color map using viridis and a theme.\ne) In a few sentences below, interpret your chart. Describe the levels and trends in democracy scores. Put your answer right below this line in markdown text (DO NOT write your comments in a code chunk!!).",
    "crumbs": [
      "Coding Asignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-1.html#step-2-make-a-column-chart-comparing-regional-levels-20-pts",
    "href": "assignments/coding-assignment-1.html#step-2-make-a-column-chart-comparing-regional-levels-20-pts",
    "title": "Homework 1",
    "section": "Step 2: Make a column chart comparing regional levels (20 pts)",
    "text": "Step 2: Make a column chart comparing regional levels (20 pts)\na) Read in the data from column_chart.csv using read_csv() in an object called column_chart_dta. You should see average democracy scores for the countries that you made a line chart for in Step 1.\nb) View the data frame. Describe briefly what you see, e.g. number of rows, columns and how the countries rank in terms of one of the democracy scores.\nc) Use ggplot() and geom_col() to visualize one of the democracy measures with a column chart (you can use the same measure as in Step 1 or a different one). Use reorder() to arrange the columns in order of the y-axis values. Make sure to add appropriate axis labels, a title and a caption. Change the fill color and add a theme to spruce it up a bit. Remember that these are averages over the same number of years as the line chart you made in Step 1\nd) Interpret your column chart. Does the evidence in the column chart roughly match what you saw in the line chart above?",
    "crumbs": [
      "Coding Asignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-1.html#step-3-make-a-scatter-plot-20-pts",
    "href": "assignments/coding-assignment-1.html#step-3-make-a-scatter-plot-20-pts",
    "title": "Homework 1",
    "section": "Step 3: Make a scatter plot (20 pts)",
    "text": "Step 3: Make a scatter plot (20 pts)\na) Load the scatter plot data with read_csv()from scatter_plot.csv, storing the data in an object called scatter_plot_dta. Then glimpse or view the data and describe briefly what you see. Note that the values you see are country averages for the variables over the period 2010 - 2019.\nb) Now build a scatter plot with ggplot2 using these data. Put GDP on the x-axis and one of the measures of democracy on the y-axis and color the points by region. Stretch the x-axis on a log scale and use the scales package to add a prefix and suffix to the x-axis numbers to indicate that they are dollar figures. Add appropriate labels and, optionally, a viridis color map and add your favorite theme.\n\n\n\n\n\n\nNote\n\n\n\nNote that you will get some warnings about a few missing data points, so try setting warning: false at the top of the code chunk with the hash pipe (#|) operator.\n\n\nc) Next add a trend line with geom_smooth() preferably with a linear model (method = “lm”). Note that you will get some warnings about missing data points and a message about the linear model. Try setting warning: false at the top of the code chunk with the hash pipe (#|) operator.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-1.html#step-4-additional-steps-20-pts",
    "href": "assignments/coding-assignment-1.html#step-4-additional-steps-20-pts",
    "title": "Homework 1",
    "section": "Step 4: Additional Steps (20 pts)",
    "text": "Step 4: Additional Steps (20 pts)\na) Facet wrap your scatter plot by region.\nb) Remove the facet_wrap() call and display the relationship for one region and use geom_text() to label your points.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-1.html#step-5-interpretation-10-pts",
    "href": "assignments/coding-assignment-1.html#step-5-interpretation-10-pts",
    "title": "Homework 1",
    "section": "Step 5: Interpretation (10 pts)",
    "text": "Step 5: Interpretation (10 pts)\nInterpret your results from your visualizations, including the scatter plot, line chart and column chart. Is there an obvious relationship between development and democracy? Do the data generally support Lipset’s theory?",
    "crumbs": [
      "Coding Asignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-1.html#step-6-rendering-10-pts",
    "href": "assignments/coding-assignment-1.html#step-6-rendering-10-pts",
    "title": "Homework 1",
    "section": "Step 6: Rendering (10 pts)",
    "text": "Step 6: Rendering (10 pts)\nPress the render button to create your HTML document. If you get any errors go back and fix them and try again until the document renders.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-1.html#submission",
    "href": "assignments/coding-assignment-1.html#submission",
    "title": "Homework 1",
    "section": "Submission",
    "text": "Submission\nAfter rendering your document, export your project folder and submit it on Blackboard. You will find the link to the Coding Assignment one submission portal under the Assignments link. There are instructions along with screenshots there to assist you with the process of exporting your project as a ZIP file from Posit Cloud.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-2.html",
    "href": "assignments/coding-assignment-2.html",
    "title": "Coding Assignment 2",
    "section": "",
    "text": "For this assignment, you are going to download some variables using the wbstats and vdemlite packages. Then you are going to wrangle these data and merge the two data sets into one and analyze how they relate to women’s representation in national parliaments. Do your work for each step in the code chunks provided.\nHere is a setup code chunk. You can load all of your packages here or as you go along in the code chunks below using the library() function. However, note that you really only need to load a library once per document.\n\n\n\n\n\n\nWarning\n\n\n\nPlease refrain from installing any packages in your code chunks because this will also install them every time the code chunk is run or your document is rendered. This may not be desirable for you or whoever is looking at or running your code (e.g. me).",
    "crumbs": [
      "Coding Asignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-2.html#overview",
    "href": "assignments/coding-assignment-2.html#overview",
    "title": "Coding Assignment 2",
    "section": "",
    "text": "For this assignment, you are going to download some variables using the wbstats and vdemlite packages. Then you are going to wrangle these data and merge the two data sets into one and analyze how they relate to women’s representation in national parliaments. Do your work for each step in the code chunks provided.\nHere is a setup code chunk. You can load all of your packages here or as you go along in the code chunks below using the library() function. However, note that you really only need to load a library once per document.\n\n\n\n\n\n\nWarning\n\n\n\nPlease refrain from installing any packages in your code chunks because this will also install them every time the code chunk is run or your document is rendered. This may not be desirable for you or whoever is looking at or running your code (e.g. me).",
    "crumbs": [
      "Coding Asignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-2.html#step-1-download-data-from-v-dem-10pts",
    "href": "assignments/coding-assignment-2.html#step-1-download-data-from-v-dem-10pts",
    "title": "Coding Assignment 2",
    "section": "Step 1: Download data from V-Dem (10pts)",
    "text": "Step 1: Download data from V-Dem (10pts)\nUse the V-Dem codebook or the searchdem() function in vdemlite to identify at least two measures of women’s empowerment (make at least one something that we did not use in class). Also retrieve the six-fold regional classification that we used in class and a general measure of democracy (like the polyarchy or libdem score). Now use fetchdem() to download the most recent 25 years or so of data for your analysis for all countries in the dataset. Make sure to load the packages that you need and glimpse() the data or View() it to make sure that it downloaded properly.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-2.html#step-2-download-data-from-the-world-bank-10-pts",
    "href": "assignments/coding-assignment-2.html#step-2-download-data-from-the-world-bank-10-pts",
    "title": "Coding Assignment 2",
    "section": "Step 2: Download data from the World Bank (10 pts)",
    "text": "Step 2: Download data from the World Bank (10 pts)\nNext, using the WDI package, download the variable on women’s representation in parliament (“SG.GEN.PARL.ZS”), at least one additional measure related to women’s empowerment and a measure of wealth (GDP per capita). Go to the WDI site the WDIsearch() function to identify relevant variables. Download data for the same time period as you did for the V-Dem data for all countries. Ensure that at least one of the variables you select is different from the ones we explored in class.\n\n\n\n\n\n\nImportant\n\n\n\nTry to make sure you download indicators with enough data to conduct your analysis. You won’t get great results if there are too many NAs.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-2.html#step-3-clean-the-data-20-pts",
    "href": "assignments/coding-assignment-2.html#step-3-clean-the-data-20-pts",
    "title": "Coding Assignment 2",
    "section": "Step 3: Clean the data (20 pts)",
    "text": "Step 3: Clean the data (20 pts)\nNow clean the data. Drop any columns that you will not need for the analysis. Filter out any years that do not match between the two data frames. Transform the region codes into actual region names. Etc.\n\n# clean v-dem data\n\n\n# clean wb data",
    "crumbs": [
      "Coding Asignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-2.html#step-4-merge-the-data-20-pts",
    "href": "assignments/coding-assignment-2.html#step-4-merge-the-data-20-pts",
    "title": "Coding Assignment 2",
    "section": "Step 4: Merge the data (20 pts)",
    "text": "Step 4: Merge the data (20 pts)\nNow add country codes using the countrycode package and merge the data using left_join().",
    "crumbs": [
      "Coding Asignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-2.html#step-5-summarize-your-combined-data-set-20-pts",
    "href": "assignments/coding-assignment-2.html#step-5-summarize-your-combined-data-set-20-pts",
    "title": "Coding Assignment 2",
    "section": "Step 5: Summarize your combined data set (20 pts)",
    "text": "Step 5: Summarize your combined data set (20 pts)\nUse group_by(), summarize() and arrange() to glean preliminary insights about your data. For example, how do regions compare on mean values of women’s representation and how do these values relate to the values of other variables in your data set? Which countries stand out in terms of women’s representation?\n\n# Regional summary\n\n\n# Country summary\n\n\n\n\n\n\n\nWarning\n\n\n\nDon’t write your paragraphs or any interpretation or analysis in a code chunk. Use markdown text. Quarto is designed to be a literate programming tool, so you can write your analysis in markdown text and include code chunks when necessary to support your argument.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-2.html#step-6-visualize-and-interpret-your-data-20-pts",
    "href": "assignments/coding-assignment-2.html#step-6-visualize-and-interpret-your-data-20-pts",
    "title": "Coding Assignment 2",
    "section": "Step 6: Visualize and interpret your data (20 pts)",
    "text": "Step 6: Visualize and interpret your data (20 pts)\nUsing the data from step 5, create an appropriate visualization that either shows differences in levels of women’s empowerment across countries or regions or demonstrates the relationship between women’s empowerment and a potential explanatory variable like welath or democracy. Then write a few lines about the patterns that your visualization uncovers.\n\n\n\n\n\n\nNote\n\n\n\nYou may relate your analysis to the assigned Norris reading that explores reasons for variations in women’s empowerment but this is not strictly required.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-2.html#submission",
    "href": "assignments/coding-assignment-2.html#submission",
    "title": "Coding Assignment 2",
    "section": "Submission",
    "text": "Submission\nAfter rendering your document, export your project folder and submit it on Blackboard. You will find the link to the Coding Assignment one submission portal under the Assignments link. There is a screen capture video in the Discord server that will help you understand how to do this.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org."
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "Lecture Recordings\n🔗 on Blackboard\n\n\nProf T’s Zoom Office\n🔗 on Zoom\n\n\nOffice Hours Appointments\n🔗 on Calendly",
    "crumbs": [
      "Course information",
      "Useful links"
    ]
  },
  {
    "objectID": "course-description.html",
    "href": "course-description.html",
    "title": "Course Description",
    "section": "",
    "text": "This course provides a comprehensive introduction to data science, blending theoretical foundations with practical applications. Students will begin with data visualization, learning the essential tools and principles for creating informative and impactful visual representations of data. The course then progresses to data wrangling, where students will clean, manipulate, and prepare data for analysis.\nAs the semester unfolds, students will explore web scraping techniques to gather data from online sources, followed by an examination of the ethical considerations surrounding data collection, privacy, and algorithmic bias. The course then delves into statistical modeling, introducing both linear and logistic regression, and other core methods used in predictive analytics.\nThroughout the course, various special topics, such as bootstrapping, randomization tests, or interactive web applications, will be covered, offering students a chance to explore current trends and tools in data science. The curriculum is designed to equip students with a versatile toolkit, enabling them to tackle real-world data challenges and communicate their findings effectively.",
    "crumbs": [
      "Course information",
      "Description"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html",
    "href": "assignments/coding-assignment-3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Community Driven Development (CDD) programs have become increasingly popular in international development over the past several decades. CDD programs directly involve impacted communities in the selection, design, and implementation of development projects. Usually, this also involves building upon or creating community level organizations (or committees) that are involved with the projects, with the goal of creating democratic and inclusive decision-making processes. See this 3ie working paper for discussion of some common features of CDD.\nGiven these design features, proponents of CDD argue that these programs should (a) improve the quality and impact of development projects (e.g., public goods) AND (b) improve social cohesion, inclusion, and local governance. For this reason, CDD has been implemented in a number of post-conflict settings, where the hope is that CDD can both improve development outcomes and contribute to rebuilding social cohesion and governance (which are often harmed by conflict).\nDoes CDD improve development outcomes? Does it improve social cohesion after conflict? Your goal in this assignment is to answer these questions, using data from an evaluation of a CDD program in post-conflict Sierra Leone.\nThe relevant files for this assignment are available in the Posit Cloud folder.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html#overview",
    "href": "assignments/coding-assignment-3.html#overview",
    "title": "Homework 3",
    "section": "",
    "text": "Community Driven Development (CDD) programs have become increasingly popular in international development over the past several decades. CDD programs directly involve impacted communities in the selection, design, and implementation of development projects. Usually, this also involves building upon or creating community level organizations (or committees) that are involved with the projects, with the goal of creating democratic and inclusive decision-making processes. See this 3ie working paper for discussion of some common features of CDD.\nGiven these design features, proponents of CDD argue that these programs should (a) improve the quality and impact of development projects (e.g., public goods) AND (b) improve social cohesion, inclusion, and local governance. For this reason, CDD has been implemented in a number of post-conflict settings, where the hope is that CDD can both improve development outcomes and contribute to rebuilding social cohesion and governance (which are often harmed by conflict).\nDoes CDD improve development outcomes? Does it improve social cohesion after conflict? Your goal in this assignment is to answer these questions, using data from an evaluation of a CDD program in post-conflict Sierra Leone.\nThe relevant files for this assignment are available in the Posit Cloud folder.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html#the-gobifo-program-in-sierra-leone",
    "href": "assignments/coding-assignment-3.html#the-gobifo-program-in-sierra-leone",
    "title": "Homework 3",
    "section": "The GoBifo Program in Sierra Leone",
    "text": "The GoBifo Program in Sierra Leone\nSierra Leone was devastated by a civil war lasting from 1991 to 2002. In the post-conflict context, the Ministry of Local Government and Community Development implemented the GoBifo (“forge ahead”) CDD project in Sierra Leone from 2005-2009 (with support from the World Bank and other international donors). See this report for a detailed summary of the program.\nThe program had two key components:\n\nBlock grants of about 4,667 dollars (about 100 per household in each community) that communities could allocate to local development projects, skills trainings, and small-business investment.\n“Technical assistance that promoted democratic decision-making, the participation of socially marginalized women and youth in local politics, and transparent budgeting practices” [@casey2012]\n\nThe GiBifo program also included an impact evaluation, conducted by economists Katherine Casey, Rachel Glennerster, and Edward Miguel. The academic paper generated from the project is in the References section [@casey2012].\nImportantly, the impact evaluation included randomization: communities in the study were randomly assigned to receive the GoBifo program, or not. In this sense, the impact evaluation was similar to a clinical drug trial, where some subjects are randomly assigned to receive the medicine while others do not receive it. This aspect of the design makes it easier to assess the causal impact of this project: we can compare outcomes in the treatment (GoBifo program) and control groups, and test for whether any differences we observe are likely to be due to chance.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html#data",
    "href": "assignments/coding-assignment-3.html#data",
    "title": "Homework 3",
    "section": "Data",
    "text": "Data\n“gobifo_data.csv” provides a condensed version of the data collected by @casey2012 for their evaluation of the project. The study team gathered literally hundreds of outcome variables, and the paper includes an interesting discussion of how they analyze the data and interpret the results given the large number of outcomes they measured.\nThe unit of analysis (or row in the dataset) is the village.\nFor our purposes, we will focus on a smaller set of variables, summarized below:\n\nTreatment variable (named t in the data): this equals treatment if the community was randomly assigned to be in the program, and control if not. As a character variable, tidymodels will automatically recognize it as a factor. But be sure to use quotation marks when working with the two categories, e.g. filter(t == \"treatment\").\nCommunity Decision Making Infrastructure: Is there a village development committee in the community? (vdc)\nPublic Goods: Is there a functioning primary school in the community? (f_psch)\nSanitation: Is there a functioning latrine in the community (f_latrine)\nInclusion, Gender equality in participatory decision making (role_wmn).\n\n“Enumerator account of how actively women participated in the deliberation compared to men, ranging from 5 = no difference between women and men to 1 = women not active at all compared to men”\n\nSocial Capital: Trust in other in the community (trust_own)\nConflict: Percentage of respondents in the village that reported they had NO conflicts/disputes with others that required outside intervention (no_conflict)",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html#part-1-setup-and-load-data-10-points",
    "href": "assignments/coding-assignment-3.html#part-1-setup-and-load-data-10-points",
    "title": "Homework 3",
    "section": "Part 1: Setup and Load Data (10 points)",
    "text": "Part 1: Setup and Load Data (10 points)\n\nCreate a folder and an RProj file for this assignment. Save “gobifo_data.csv” in that folder. Create a new Quarto document where you will complete the work for this assignment. Make sure to include a title, your name, and the date. (Note that these steps are done for you if you are working in Posit Cloud)\nCreate a code chunk that loads the packages you will need: tidyverse and tidymodels\nUse read_csv() to read the “gobifo_data.csv” data in to RStudio. Save the dataset as an object called “myData”\nExamine the data. How many observations (villages) are there in the study? You can use glimpse() to determine the number of rows. How many were in the treatment (t) group (in the program) and how many were in control? You can use summarize() or count() for this calculation.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html#part-2-calculate-the-treatment-effect-of-a-variable-16-points",
    "href": "assignments/coding-assignment-3.html#part-2-calculate-the-treatment-effect-of-a-variable-16-points",
    "title": "Homework 3",
    "section": "Part 2: Calculate the Treatment Effect of a Variable (16 points)",
    "text": "Part 2: Calculate the Treatment Effect of a Variable (16 points)\nChoose one variables. Calculate the means and treatment effect of your chosen variable.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html#part-3-state-your-hypotheses-16-points",
    "href": "assignments/coding-assignment-3.html#part-3-state-your-hypotheses-16-points",
    "title": "Homework 3",
    "section": "Part 3: State Your Hypotheses (16 points)",
    "text": "Part 3: State Your Hypotheses (16 points)\nWhat is your null hypothesis for the effect of your chosen variable? What is your alternative hypothesis?",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html#part-4-generate-the-null-distribution-16-points",
    "href": "assignments/coding-assignment-3.html#part-4-generate-the-null-distribution-16-points",
    "title": "Homework 3",
    "section": "Part 4: Generate the Null Distribution (16 points)",
    "text": "Part 4: Generate the Null Distribution (16 points)\nUsing tidymodels, generate a null distribution for the treatment effect of your chosen variable.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html#part-5-get-the-p-value-16-points",
    "href": "assignments/coding-assignment-3.html#part-5-get-the-p-value-16-points",
    "title": "Homework 3",
    "section": "Part 5: Get the p-value (16 points)",
    "text": "Part 5: Get the p-value (16 points)\nUse the get_p_value() function from the infer package to calculate the p-value for your chosen variable.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html#part-6-visualize-the-treatment-effect-16-points",
    "href": "assignments/coding-assignment-3.html#part-6-visualize-the-treatment-effect-16-points",
    "title": "Homework 3",
    "section": "Part 6: Visualize the Treatment Effect (16 points)",
    "text": "Part 6: Visualize the Treatment Effect (16 points)\nUse the visualize() function from the infer package to visualize the treatment effect of your chosen variable relative to the null distribution.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html#part-7-interpret-the-results-10-points",
    "href": "assignments/coding-assignment-3.html#part-7-interpret-the-results-10-points",
    "title": "Homework 3",
    "section": "Part 7: Interpret the Results (10 points)",
    "text": "Part 7: Interpret the Results (10 points)\nWrite one or two paragraphs that summarize the results of your analysis and accurately interpret them. What is the big picture takeaway of your analysis? What does your analysis teach us about the impact of this CDD program based on the variable you chose?",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-3.html#extra-credit",
    "href": "assignments/coding-assignment-3.html#extra-credit",
    "title": "Homework 3",
    "section": "Extra Credit",
    "text": "Extra Credit\nThere are addtional outcome variables in the study that you did not select: get the p-value and visualize the treatment effect relative to the null distribution for these variables (one point per variable).\n\n\n\n\n\n\nSubmission Instructions\n\n\n\nAfter rendering your document, export your project folder and submit it on Blackboard. You will find the link to the Coding Assignment one submission portal under the Assignments link. There is a screen capture video in the Discord server that will help you understand how to do this.",
    "crumbs": [
      "Coding Asignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignments/coding-assignment-4.1.html",
    "href": "assignments/coding-assignment-4.1.html",
    "title": "Homework 3",
    "section": "",
    "text": "In this homework assignment, we are once again going to be exploring the correlates of democracy with V-Dem. But this time we are going to do so in the context of regression analysis."
  },
  {
    "objectID": "assignments/coding-assignment-4.1.html#step-1-load-the-data-8-pts.",
    "href": "assignments/coding-assignment-4.1.html#step-1-load-the-data-8-pts.",
    "title": "Homework 3",
    "section": "Step 1: Load the data (8 pts.)",
    "text": "Step 1: Load the data (8 pts.)\nUsing fetchdem from vdemlite, select the v2x_regime variable, a continuous measure of democracy like v2x_polyarchy, and at least three variables that you believe correlate with democracy for all countries and one year of your choosing. Use mutate() to transform v2x_regime into a binary variable that indicates whether a country is a democracy or not (I have provided this part of the code for you).\n\n# Uncomment this code to start working on it.\n\n#library(vdemlite)\n#library(tidyverse)\n\n#vdem_data &lt;- fetchdem(indicators = c(???),\n#                      start_year = ???, end_year = ???) |&gt;\n#  mutate(regime_dummy = ifelse(v2x_regime &gt;= 2, 1, 0)\n\n#glimpse(vdem_data)"
  },
  {
    "objectID": "assignments/coding-assignment-4.1.html#step-2-hypotheses-8-pts.",
    "href": "assignments/coding-assignment-4.1.html#step-2-hypotheses-8-pts.",
    "title": "Homework 3",
    "section": "Step 2: Hypotheses (8 pts.)",
    "text": "Step 2: Hypotheses (8 pts.)\nTalk about why you chose the indicators you did and why you think your independent variables should be related to democracy."
  },
  {
    "objectID": "assignments/coding-assignment-4.1.html#step-3-run-a-bivariate-linear-regression-model-12-pts.",
    "href": "assignments/coding-assignment-4.1.html#step-3-run-a-bivariate-linear-regression-model-12-pts.",
    "title": "Homework 3",
    "section": "Step 3: Run a bivariate linear regression model (12 pts.)",
    "text": "Step 3: Run a bivariate linear regression model (12 pts.)\nRun a bivariate linear regression model with your selected continuous measure of democracy as the dependent variable and one of your selected predictors as the independent variable. Use the lm() function to run the model and the summary() function to view the results."
  },
  {
    "objectID": "assignments/coding-assignment-4.1.html#step-4-interpret-the-results-12-pts.",
    "href": "assignments/coding-assignment-4.1.html#step-4-interpret-the-results-12-pts.",
    "title": "Homework 3",
    "section": "Step 4: Interpret the results (12 pts.)",
    "text": "Step 4: Interpret the results (12 pts.)\nInterpret the results of the regression model. What is the coefficient on the predictor variable? What does this coefficient tell you about the relationship between the predictor and democracy? Is the relationship stastically significant? How about the constant (intercept term)?"
  },
  {
    "objectID": "assignments/coding-assignment-4.1.html#step-5-run-a-multiple-linear-regression-model-12-pts.",
    "href": "assignments/coding-assignment-4.1.html#step-5-run-a-multiple-linear-regression-model-12-pts.",
    "title": "Homework 3",
    "section": "Step 5: Run a Multiple Linear Regression Model (12 pts.)",
    "text": "Step 5: Run a Multiple Linear Regression Model (12 pts.)\nRun a multiple linear regression model with your selected continous measure of democracy as the dependent variable and at least three of your selected predictors as the independent variables. Use the lm() function to run the model and the summary() function to view the results."
  },
  {
    "objectID": "assignments/coding-assignment-4.1.html#step-6-interpret-the-results-of-your-multiple-regression-12-pts.",
    "href": "assignments/coding-assignment-4.1.html#step-6-interpret-the-results-of-your-multiple-regression-12-pts.",
    "title": "Homework 3",
    "section": "Step 6: Interpret the Results of Your Multiple Regression (12 pts.)",
    "text": "Step 6: Interpret the Results of Your Multiple Regression (12 pts.)\nInterpret the results of the multiple regression model. What is the coefficient on each of the predictor variables? What does this coefficient tell you about the relationship between the predictors and democracy? Are the relationships statistically significant? How would you interpret the constant (intercept term)?"
  },
  {
    "objectID": "assignments/coding-assignment-4.1.html#step-7-run-a-multiple-logistic-regression-12-pts.",
    "href": "assignments/coding-assignment-4.1.html#step-7-run-a-multiple-logistic-regression-12-pts.",
    "title": "Homework 3",
    "section": "Step 7: Run a Multiple Logistic Regression (12 pts.)",
    "text": "Step 7: Run a Multiple Logistic Regression (12 pts.)\nUsing your dichotomous measure of regime type (democratic or not) as the dependent variable, run a multiple logistic regression model with your three predictors. Use the glm() function to run the model and the summary() function to view the results."
  },
  {
    "objectID": "assignments/coding-assignment-4.1.html#step-8-interpret-the-results-of-your-logistic-regression-12-pts.",
    "href": "assignments/coding-assignment-4.1.html#step-8-interpret-the-results-of-your-logistic-regression-12-pts.",
    "title": "Homework 3",
    "section": "Step 8: Interpret the Results of Your Logistic Regression (12 pts.)",
    "text": "Step 8: Interpret the Results of Your Logistic Regression (12 pts.)\nInterpret the results of the logistic regression model. Interpret the direction and significance of each predictor. Exponentiate the coefficient of one of the predictors and interpret the substantive effects."
  },
  {
    "objectID": "assignments/coding-assignment-4.1.html#step-9-calculate-predicted-probabilities-12-pts.",
    "href": "assignments/coding-assignment-4.1.html#step-9-calculate-predicted-probabilities-12-pts.",
    "title": "Homework 3",
    "section": "Step 9: Calculate predicted probabilities (12 pts.)",
    "text": "Step 9: Calculate predicted probabilities (12 pts.)\nUse the marginaleffects package to give the predicted probabilities of regime transition (e.g. becoming a democracy) for three countries in your dataset. Display the results using the tidy() function from the broom package."
  },
  {
    "objectID": "assignments/coding-assignment-4.1.html#extra-credit-3-pts",
    "href": "assignments/coding-assignment-4.1.html#extra-credit-3-pts",
    "title": "Homework 3",
    "section": "Extra Credit (3 pts)",
    "text": "Extra Credit (3 pts)\nUse ggplot2 to visualize your predictions with a coefficient plot.\n\n\n\n\n\n\nSubmission Instructions\n\n\n\nAfter rendering your document, export your project folder and submit it on Blackboard. You will find the link to the Coding Assignment one submission portal under the Assignments link. There is a screen capture video in the Discord server that will help you understand how to do this."
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Week 7",
    "section": "",
    "text": "Important\n\n\n\n“Data Assignment 2: Cleaning & Visualizing Data in Excel” is due on Sunday, March 5"
  },
  {
    "objectID": "weeks/week-7.html#readings",
    "href": "weeks/week-7.html#readings",
    "title": "Week 7",
    "section": "Readings",
    "text": "Readings\nArchibong et. al., “Washington Consensus Reforms and Economic Performance in Sub-Saharan Africa”\nBenería et. al., “Markets, Globalization and Gender”"
  },
  {
    "objectID": "weeks/week-7.html#slides",
    "href": "weeks/week-7.html#slides",
    "title": "Week 7",
    "section": "Slides",
    "text": "Slides\n🖥️ Washington consensus reforms\n🖥️ The gendered legacy of reform"
  },
  {
    "objectID": "weeks/week-7.html#videos",
    "href": "weeks/week-7.html#videos",
    "title": "Week 7",
    "section": "Videos",
    "text": "Videos\n📺 Scatter plot video\n📺 Map video"
  },
  {
    "objectID": "weeks/week-7.html#assignments",
    "href": "weeks/week-7.html#assignments",
    "title": "Week 7",
    "section": "Assignments",
    "text": "Assignments\n🧮 Data Assignment 2: Cleaning & Visualizing Data in Excel\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Important\n\n\n\nCoding Assignment 1 is due on Monday, February 10",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#readings",
    "href": "weeks/week-4.html#readings",
    "title": "Week 4",
    "section": "Readings",
    "text": "Readings\n📖 r4ds, Communication\n📖 Lipset, Some Social Requisites of Democracy",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#slides",
    "href": "weeks/week-4.html#slides",
    "title": "Week 4",
    "section": "Slides",
    "text": "Slides\n🖥 Visualization Best Practices\n🖥 Red green colourblindess, do you have it?",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#assignments",
    "href": "weeks/week-4.html#assignments",
    "title": "Week 4",
    "section": "Assignments",
    "text": "Assignments\n🧮 Coding Assignment 1\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "📖 r4ds, Data Import\n📖 r4ds, Data Visualization\n📖 r4ds, Layers, sections 9.1-9.4",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#readings",
    "href": "weeks/week-3.html#readings",
    "title": "Week 3",
    "section": "",
    "text": "📖 r4ds, Data Import\n📖 r4ds, Data Visualization\n📖 r4ds, Layers, sections 9.1-9.4",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#videos",
    "href": "weeks/week-3.html#videos",
    "title": "Week 3",
    "section": "Videos",
    "text": "Videos\n📺 Make a Bar Chart with ggplot2\n📺 Make a Histogram with ggplot2\n📺 Make a Line Chart with ggplot2\n📺 Make a Scatterplot with ggplot2",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#slides",
    "href": "weeks/week-3.html#slides",
    "title": "Week 3",
    "section": "Slides",
    "text": "Slides\n🖥 Grammar of Graphics\n🖥 Which Visualization Do I Use?\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important\n\n\n\nProject Assignment 1 is due on Sunday, March 26."
  },
  {
    "objectID": "weeks/week-10.html#readings",
    "href": "weeks/week-10.html#readings",
    "title": "Week 10",
    "section": "Readings",
    "text": "Readings\n📖 Gøsta Esping-Andersen, The Three Worlds of Welfare Capitalism\n📖 Alesina et. al., “Why Doesn’t the United States Have a European-Style Welfare State?”"
  },
  {
    "objectID": "weeks/week-10.html#slides",
    "href": "weeks/week-10.html#slides",
    "title": "Week 10",
    "section": "Slides",
    "text": "Slides\n🖥️ Welfare states in the OECD\n🖥️ U.S. exceptionalism\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-10.html#assignments",
    "href": "weeks/week-10.html#assignments",
    "title": "Week 10",
    "section": "Assignments",
    "text": "Assignments\n✍️ Project Assignment 1: Outline and Sources"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "📖 John Judis, The Politics of Our Time\n\nFor Tuesday, choose one chapter to read from Part I\nFor Thursday, choose one chapter to read from Part II or Part III"
  },
  {
    "objectID": "weeks/week-13.html#readings",
    "href": "weeks/week-13.html#readings",
    "title": "Week 13",
    "section": "",
    "text": "📖 John Judis, The Politics of Our Time\n\nFor Tuesday, choose one chapter to read from Part I\nFor Thursday, choose one chapter to read from Part II or Part III"
  },
  {
    "objectID": "weeks/week-13.html#slides",
    "href": "weeks/week-13.html#slides",
    "title": "Week 13",
    "section": "Slides",
    "text": "Slides\n🖥️ The populist backlash in the West\n🖥️ The rise of nationalism and socialism\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\nExam 2 is due on Sunday, March 19"
  },
  {
    "objectID": "weeks/week-9.html#readings",
    "href": "weeks/week-9.html#readings",
    "title": "Week 9",
    "section": "Readings",
    "text": "Readings\n📖 Mary Cullen, “How to Write and Format a White Paper”"
  },
  {
    "objectID": "weeks/week-9.html#assignments",
    "href": "weeks/week-9.html#assignments",
    "title": "Week 9",
    "section": "Assignments",
    "text": "Assignments\n📘 Exam 2\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-15.html",
    "href": "weeks/week-15.html",
    "title": "Week 15",
    "section": "",
    "text": "Important\n\n\n\nExam 3 is due on Sunday, April 30."
  },
  {
    "objectID": "weeks/week-15.html#slides",
    "href": "weeks/week-15.html#slides",
    "title": "Week 15",
    "section": "Slides",
    "text": "Slides\nExam 3 Review (Factile game)"
  },
  {
    "objectID": "weeks/week-15.html#assignments",
    "href": "weeks/week-15.html#assignments",
    "title": "Week 15",
    "section": "Assignments",
    "text": "Assignments\n📘 Exam 3\n✍️ Project Assignment 3: Tableau Dashboard\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Schedule",
    "section": "",
    "text": "This page displays an outline of the topics, content, and assignments for the semester. Links, to slides, assignments and course resources will go live as the semester progresses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nReading\nVideo\nModule\nAssignment\nDue Date\n\n\n\n\n1\nMon, May 19\nGetting started\n📖 📖\n📺 📺\n📒\n\n\n\n\n\nTue, May 20\nMeet ouR tech stack\n📖\n📺\n📒\n\n\n\n\n\nWed, May 21\nR Coding basics\n📖\n📺 📺\n📒\n📘\nWed, May 21\n\n\n\nThu, May 22\nIntro to the Tidyverse\n📖\n📺\n📒\n\n\n\n\n\nFri, May 23\nGrammar of graphics\n📖 📖\n📺 📺\n📒\n\n\n\n\n\nSat, May 24\nAdvanced data viz techniques\n📖\n📺 📺\n📒\n📘 🧮\nSun, May 25\n\n\n2\nMon, May 26\nWhat is tidy data?\n📖\n📺\n📒\n\n\n\n\n\nTue, May 27\nTransforming data\n📖\n📺\n📒\n\n\n\n\n\nWed, May 28\nSummarizing data\n📖\n📺\n📒\n📘\nWed, May 28\n\n\n\nThu, May 29\nJoining data frames\n📖\n📺\n📒\n\n\n\n\n\nFri, May 30\nTidying data\n📖\n\n📒\n\n\n\n\n\nSat, May 31\nCleaning data\n📖\n📺 📺 📺\n📒\n📘 🧮\nSun, Jun 1\n\n\n3\nMon, Jun 2\nCategorical data\n📖\n📺\n📒\n\n\n\n\n\nTue, Jun 3\nContinuous data\n📖\n📺\n📒\n\n\n\n\n\nWed, Jun 4\nDescribing Distributions\n📖\n\n📒\n📘\nWed, Jun 4\n\n\n\nThu, Jun 5\nSampling and uncertainty\n📖\n📺 📺\n📒\n\n\n\n\n\nFri, Jun 6\nSingle proportion tests\n📖\n\n📒\n\n\n\n\n\nSat, Jun 7\nDifferences between two groups\n📖\n📺\n📒\n📘 🧮\nSun, Jun 8\n\n\n4\nMon, Jun 9\nFitting a line to data\n📖\n📺 📺\n📒\n\n\n\n\n\nTue, Jun 10\nLeast squares regression\n📖\n📺 📺\n📒\n\n\n\n\n\nWed, Jun 11\nDealing with outliers\n📖\n📺\n📒\n📘\nWed, Jun 11\n\n\n\nThu, Jun 12\nMultiple linear regression\n📖\n📺\n📒\n\n\n\n\n\nFri, Jun 13\nModel selection\n📖\n\n📒\n\n\n\n\n\nSat, Jun 14\nAssumptions, EDA and diagnostics\n📖 📖\n\n📒\n📘 🧮\nSun, Jun 15\n\n\n5\nMon, Jun 16\nWhy logistic regression?\n📖\n📺\n📒\n\n\n\n\n\nTue, Jun 17\nThe logistic regression model\n\n📺\n📒\n\n\n\n\n\nWed, Jun 18\nInterpreting results\n\n📺\n📒\n📘\nWed, Jun 18\n\n\n\nThu, Jun 19\nPrediction and overfitting\n\n\n📒\n\n\n\n\n\nFri, Jun 20\nModel evaluation, feature engineering\n\n\n📒\n\n\n\n\n\nSat, Jun 21\nMultiple predictors, interactions and diagnostics\n📖\n\n📒\n📘 🧮\nSun, Jun 22\n\n\n6\nMon, Jun 23\nFinal Project\n\n\n📒\n✍️\nMon, Jun 23\n\n\n\nTue, Jun 24\nFinal Project\n\n\n📒\n\n\n\n\n\nWed, Jun 25\nFinal Project\n\n\n\n✍️\nWed, Jun 25\n\n\n\nThu, Jun 26\nFinal Project\n\n\n\n\n\n\n\n\nFri, Jun 27\nFinal project\n\n\n\n\n\n\n\n\nSat, Jun 28\nFinal project\n\n\n\n✍️\nSat, Jun 28",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "slides/week-11.2.html#electoral-democracy-measure",
    "href": "slides/week-11.2.html#electoral-democracy-measure",
    "title": "Distributions II",
    "section": "Electoral Democracy Measure",
    "text": "Electoral Democracy Measure\n\n\nTo what extent is the ideal of electoral democracy in its fullest sense achieved?\nMeasure runs from 0 (lowest) to 1 (highest)\n0.5 is a cutoff for distinguishing electoral democracy from electoral autocracy\n\n\nThe electoral principle of democracy seeks to embody the core value of making rulers responsive to citizens, achieved through electoral competition for the electorate’s approval under circumstances when suffrage is extensive; political and civil society organizations can operate freely; elections are clean and not marred by fraud or systematic irregularities; and elections affect the composition of the chief executive of the country. In between elections, there is freedom of expression and an independent media capable of presenting alternative views on matters of political relevance. – V-Dem Codebook"
  },
  {
    "objectID": "slides/week-11.2.html#other-high-level-v-dem-measures",
    "href": "slides/week-11.2.html#other-high-level-v-dem-measures",
    "title": "Distributions II",
    "section": "Other High-Level V-Dem Measures",
    "text": "Other High-Level V-Dem Measures\n\n\nLiberal Democracy\nEgalitarian Democracy\nParticipatory Democracy\nDeliberative Democracy\n\nAll continuous measures, ranging from 0 to 1. Let’s take a look at how to summarize data like this!"
  },
  {
    "objectID": "slides/week-11.2.html#data-setup",
    "href": "slides/week-11.2.html#data-setup",
    "title": "Distributions II",
    "section": "Data Setup",
    "text": "Data Setup\n\n\n# Load packages \nlibrary(tidyverse)\nlibrary(vdemlite)\n\n# Create dataset for year 2022, with country name, year, and electoral dem\nvdem2022 &lt;- fetchdem(\n  indicators = c(\"v2x_polyarchy\", \"e_regionpol_6C\",\n                 start_year = 2022, end_year = 2022)\n  ) |&gt;\n  rename(\n    country = country_name,    \n    polyarchy = v2x_polyarchy, \n    region = e_regionpol_6C \n    ) |&gt;\n  mutate(\n    region = case_match(region, \n                        1 ~ \"Eastern Europe\", \n                        2 ~ \"Latin America\",  \n                        3 ~ \"Middle East\",   \n                        4 ~ \"Africa\", \n                        5 ~ \"The West\", \n                        6 ~ \"Asia\")\n    )"
  },
  {
    "objectID": "slides/week-11.2.html#examine-the-data",
    "href": "slides/week-11.2.html#examine-the-data",
    "title": "Distributions II",
    "section": "Examine the Data",
    "text": "Examine the Data\n\n\nglimpse(vdem2022)\n\nRows: 9,170\nColumns: 6\n$ country         &lt;chr&gt; \"Mexico\", \"Mexico\", \"Mexico\", \"Mexico\", \"Mexico\", \"Mex…\n$ country_text_id &lt;chr&gt; \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\", \"MEX\"…\n$ country_id      &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ year            &lt;dbl&gt; 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, …\n$ polyarchy       &lt;dbl&gt; 0.250, 0.248, 0.249, 0.249, 0.251, 0.251, 0.262, 0.276…\n$ region          &lt;chr&gt; \"Latin America\", \"Latin America\", \"Latin America\", \"La…"
  },
  {
    "objectID": "slides/week-11.2.html#section",
    "href": "slides/week-11.2.html#section",
    "title": "Distributions II",
    "section": "",
    "text": "How can we summarize measures of democracy? :thinking:\n\nWe could calculate the mean.\n\nvdem2022 |&gt;\n  summarize(mean_democracy = mean(polyarchy))\n\n  mean_democracy\n1             NA\n\n\nThe mean is the average of the values. Common measure of central tendency but sensitive to outliers."
  },
  {
    "objectID": "slides/week-11.2.html#section-1",
    "href": "slides/week-11.2.html#section-1",
    "title": "Distributions II",
    "section": "",
    "text": "How can we summarize measures of democracy? :thinking:\n\nWe could calculate the median.\n\nvdem2022 |&gt;\n  summarize(median_democracy = median(polyarchy))\n\n  median_democracy\n1               NA\n\n\nThe median is the value that separates the higher half from the lower half of the data."
  },
  {
    "objectID": "slides/week-11.2.html#section-2",
    "href": "slides/week-11.2.html#section-2",
    "title": "Distributions II",
    "section": "",
    "text": "We can also describe the shape of the distribution…\n\nsymmetric (e.g. normal)\nright-skewed\nleft-skewed\nunimodal (one peak)\nbimodal (multiple peaks)"
  },
  {
    "objectID": "slides/week-11.2.html#histograms",
    "href": "slides/week-11.2.html#histograms",
    "title": "Distributions II",
    "section": "Histograms",
    "text": "Histograms\n\nUsed to represent the distribution of a continuous variable\nThe x-axis represents the range of values\nThe y-axis represents the frequency of each value\nThe bars represent the number of observations in each range or “bin”\nThe shape of the histogram can tell us a lot about the distribution of the data"
  },
  {
    "objectID": "slides/week-11.2.html#symmetric-distributions",
    "href": "slides/week-11.2.html#symmetric-distributions",
    "title": "Distributions II",
    "section": "Symmetric Distributions",
    "text": "Symmetric Distributions"
  },
  {
    "objectID": "slides/week-11.2.html#symmetric-distributions-1",
    "href": "slides/week-11.2.html#symmetric-distributions-1",
    "title": "Distributions II",
    "section": "Symmetric Distributions",
    "text": "Symmetric Distributions"
  },
  {
    "objectID": "slides/week-11.2.html#skewed-distributions",
    "href": "slides/week-11.2.html#skewed-distributions",
    "title": "Distributions II",
    "section": "Skewed Distributions",
    "text": "Skewed Distributions"
  },
  {
    "objectID": "slides/week-11.2.html#skewed-distributions-1",
    "href": "slides/week-11.2.html#skewed-distributions-1",
    "title": "Distributions II",
    "section": "Skewed Distributions",
    "text": "Skewed Distributions"
  },
  {
    "objectID": "slides/week-11.2.html#bimodal-distribution",
    "href": "slides/week-11.2.html#bimodal-distribution",
    "title": "Distributions II",
    "section": "Bimodal Distribution",
    "text": "Bimodal Distribution"
  },
  {
    "objectID": "slides/week-11.2.html#when-is-the-mean-useful",
    "href": "slides/week-11.2.html#when-is-the-mean-useful",
    "title": "Distributions II",
    "section": "When is the Mean Useful?",
    "text": "When is the Mean Useful?"
  },
  {
    "objectID": "slides/week-11.2.html#when-is-the-mean-useful-1",
    "href": "slides/week-11.2.html#when-is-the-mean-useful-1",
    "title": "Distributions II",
    "section": "When is the Mean Useful?",
    "text": "When is the Mean Useful?"
  },
  {
    "objectID": "slides/week-11.2.html#when-is-the-mean-useful-2",
    "href": "slides/week-11.2.html#when-is-the-mean-useful-2",
    "title": "Distributions II",
    "section": "When is the Mean Useful?",
    "text": "When is the Mean Useful?"
  },
  {
    "objectID": "slides/week-11.2.html#when-is-the-mean-useful-3",
    "href": "slides/week-11.2.html#when-is-the-mean-useful-3",
    "title": "Distributions II",
    "section": "When is the mean useful?",
    "text": "When is the mean useful?\n\n\nThe Mean works well as a summary statistic when the distribution is relatively symmetric\nNot as well when distributions are skewed or bimodal (or multi-modal)\nWith skewed distributions, the mean is sensitive to extreme values\nThe median is more robust"
  },
  {
    "objectID": "slides/week-11.2.html#lesson",
    "href": "slides/week-11.2.html#lesson",
    "title": "Distributions II",
    "section": "Lesson",
    "text": "Lesson\n\nAlways look at your data!!\nWhen reading or in a presentation, ask yourself:\n\nDoes the mean make sense given the distribution of the measure?\nCould extreme values in a skewed distribution make the mean not as useful?\nHave the analysts shown you the distribution? If not, ask about it!"
  },
  {
    "objectID": "slides/week-11.2.html#visualize-our-measure",
    "href": "slides/week-11.2.html#visualize-our-measure",
    "title": "Distributions II",
    "section": "Visualize Our Measure",
    "text": "Visualize Our Measure"
  },
  {
    "objectID": "slides/week-11.2.html#visualize-our-measure-1",
    "href": "slides/week-11.2.html#visualize-our-measure-1",
    "title": "Distributions II",
    "section": "Visualize Our Measure",
    "text": "Visualize Our Measure\n\n\nmn &lt;- mean(vdem2022$polyarchy, na.rm = TRUE)\nmed &lt;- median(vdem2022$polyarchy, na.rm = TRUE)\n\nggplot(vdem2022, aes(x = polyarchy )) +\n  geom_histogram(binwidth = .05, fill = \"steelblue\") +\n   labs(\n    x = \"Electoral Democracy\", \n    y = \"Frequency\", \n    title = \"Distribution of Electoral Democracy in 2022\", \n    caption = \"Source: V-Dem Institute\"\n  ) + \n  geom_vline(xintercept = mn, linewidth = 2, color = \"darkorange\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-11.2.html#visualize-our-measure-2",
    "href": "slides/week-11.2.html#visualize-our-measure-2",
    "title": "Distributions II",
    "section": "Visualize Our Measure",
    "text": "Visualize Our Measure\n\n\nmn &lt;- mean(vdem2022$polyarchy, na.rm = TRUE)\nmed &lt;- median(vdem2022$polyarchy, na.rm = TRUE)\n\nggplot(vdem2022, aes(x = polyarchy )) +\n  geom_histogram(binwidth = .05, fill = \"steelblue\") +\n   labs(\n    x = \"Electoral Democracy\", \n    y = \"Frequency\", \n    title = \"Distribution of Electoral Democracy in 2022\", \n    caption = \"Source: V-Dem Institute\"\n  ) + \n  geom_vline(xintercept = mn, linewidth = 2, color = \"darkorange\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-11.2.html#your-turn",
    "href": "slides/week-11.2.html#your-turn",
    "title": "Distributions II",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nLook at the V-Dem codebook\nSelect a different high-level measure of democracy\nPreprocess your data to include tha measure in your data frame\nCalculate the mean and median and store as a variable\nVisualize the distribution of the measure\nInclude a vertical line for the mean\nNow try the median\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-11.2.html#recap",
    "href": "slides/week-11.2.html#recap",
    "title": "Distributions II",
    "section": "Recap",
    "text": "Recap\n\n\nWe can use statistics like mean or median to describe the center of a variable\nWe can visualize the entire distribution to charachterize the distribution of the variable\nWe should also say something about the spread of the distribution"
  },
  {
    "objectID": "slides/week-11.2.html#why-measure-and-visualize-spread",
    "href": "slides/week-11.2.html#why-measure-and-visualize-spread",
    "title": "Distributions II",
    "section": "Why Measure and Visualize Spread?",
    "text": "Why Measure and Visualize Spread?"
  },
  {
    "objectID": "slides/week-11.2.html#measures-of-spread-range",
    "href": "slides/week-11.2.html#measures-of-spread-range",
    "title": "Distributions II",
    "section": "Measures of Spread: Range",
    "text": "Measures of Spread: Range\n\n\nRange (min and max values)\nNot ideal b/c does not tell us much about where most of the values are located\n\n\nvdem2022 |&gt;\n  summarize(min = min(polyarchy, na.rm = TRUE),\n            max = max(polyarchy, na.rm = TRUE))\n\n   min   max\n1 0.01 0.922"
  },
  {
    "objectID": "slides/week-11.2.html#measure-of-spread-interquartile-range",
    "href": "slides/week-11.2.html#measure-of-spread-interquartile-range",
    "title": "Distributions II",
    "section": "Measure of Spread: Interquartile Range",
    "text": "Measure of Spread: Interquartile Range\nIQR: 25th percentile - 75th percentile"
  },
  {
    "objectID": "slides/week-11.2.html#interquartile-range",
    "href": "slides/week-11.2.html#interquartile-range",
    "title": "Distributions II",
    "section": "Interquartile Range",
    "text": "Interquartile Range\n\nThe middle 50 percent of the countries in the data lie between 0.262 and 0.747\nThe IQR (0.485) is the difference between the Q3 and Q1 values\n\n\nvdem2022 |&gt;\n  summarize(\n    IQRlow =  quantile(polyarchy, .25, na.rm = TRUE),\n    IQRhigh = quantile(polyarchy, .75, na.rm = TRUE),\n    IQRlength = IQR(polyarchy, na.rm = TRUE)\n          )\n\n  IQRlow IQRhigh IQRlength\n1  0.175   0.712     0.537"
  },
  {
    "objectID": "slides/week-11.2.html#box-plot",
    "href": "slides/week-11.2.html#box-plot",
    "title": "Distributions II",
    "section": "Box Plot",
    "text": "Box Plot\n\nA box plot is a graphical representation of the distribution based on the median and quartiles\nIt is a standardized way of displaying the distribution of data based on a five number summary: minimum, first quartile, median, third quartile, and maximum"
  },
  {
    "objectID": "slides/week-11.2.html#box-plot-1",
    "href": "slides/week-11.2.html#box-plot-1",
    "title": "Distributions II",
    "section": "Box Plot",
    "text": "Box Plot\n\n\nCode\nggplot(vdem2022, aes(x = \"\", y = polyarchy)) +\n  geom_boxplot(fill = \"steelblue\") + \n   labs(\n    x = \"\", \n    y = \"Electoral Democracy\", \n    title = \"Distribution of Electoral Democracy in 2022\", \n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-11.2.html#measure-of-spead-standard-deviation",
    "href": "slides/week-11.2.html#measure-of-spead-standard-deviation",
    "title": "Distributions II",
    "section": "Measure of Spead: Standard Deviation",
    "text": "Measure of Spead: Standard Deviation\n\n\nCan think of it as something like the “average distance” of each data point from the mean\n\n\nvdem2022 |&gt;\n  summarize(mean = mean(polyarchy, na.rm = TRUE),\n            stdDev = sd(polyarchy, na.rm = TRUE))\n\n       mean    stdDev\n1 0.4370313 0.2856311"
  },
  {
    "objectID": "slides/week-11.2.html#standard-deviation",
    "href": "slides/week-11.2.html#standard-deviation",
    "title": "Distributions II",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\n\nA low standard deviation indicates that the values tend to be close to the mean\nA high standard deviation indicates that the values are spread out over a wider range"
  },
  {
    "objectID": "slides/week-11.2.html#starting-with-variance",
    "href": "slides/week-11.2.html#starting-with-variance",
    "title": "Distributions II",
    "section": "Starting with Variance",
    "text": "Starting with Variance\n\n\nVariance is a step towards calculating the standard deviation.\nIt quantifies the average squared deviation of each number from the mean of the data set."
  },
  {
    "objectID": "slides/week-11.2.html#calculating-deviation-from-the-mean",
    "href": "slides/week-11.2.html#calculating-deviation-from-the-mean",
    "title": "Distributions II",
    "section": "Calculating Deviation from the Mean",
    "text": "Calculating Deviation from the Mean\n\nFirst, calculate the mean (\\(\\bar{X}\\)) of the dataset.\nFor each data point (\\(X_i\\)), calculate its deviation from the mean: \\[e_i = X_i - \\bar{X}\\]\n\nExample with a mean of 5:\n\nFor a data point where \\((X_i = 0): (0 - 5 = -5)\\)\nFor a data point where \\((X_i = 10): (10 - 5 = 5)\\)"
  },
  {
    "objectID": "slides/week-11.2.html#squaring-the-deviations",
    "href": "slides/week-11.2.html#squaring-the-deviations",
    "title": "Distributions II",
    "section": "Squaring the Deviations",
    "text": "Squaring the Deviations\n\nSquaring each deviation (\\(e_i\\)) to eliminate negative values: \\[e_i^2 = (X_i - \\bar{X})^2\\]\nSumming up all squared deviations: \\[\\sum_{i=1}^{n} (X_i - \\bar{X})^2\\]\nThis sum represents the total squared deviation from the mean."
  },
  {
    "objectID": "slides/week-11.2.html#calculating-the-variance",
    "href": "slides/week-11.2.html#calculating-the-variance",
    "title": "Distributions II",
    "section": "Calculating the Variance",
    "text": "Calculating the Variance\n\nDivide the total squared deviation by \\((n-1)\\) (to account for the sample variance): \\[\\text{Variance} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2\\]\nUsing \\((n-1)\\) ensures an unbiased estimate of the population variance when calculating from a sample."
  },
  {
    "objectID": "slides/week-11.2.html#deriving-the-standard-deviation",
    "href": "slides/week-11.2.html#deriving-the-standard-deviation",
    "title": "Distributions II",
    "section": "Deriving the Standard Deviation",
    "text": "Deriving the Standard Deviation\n\n\nThe standard deviation is the square root of the variance: \\[s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})^2}\\]\nTaking the square root converts the variance back to the units of the original data."
  },
  {
    "objectID": "slides/week-11.2.html#standard-deviation-simple-example",
    "href": "slides/week-11.2.html#standard-deviation-simple-example",
    "title": "Distributions II",
    "section": "Standard Deviation Simple Example",
    "text": "Standard Deviation Simple Example\n\n\nx = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\ne &lt;- x - mean(x)\ne\n\n [1] -5 -4 -3 -2 -1  0  1  2  3  4  5"
  },
  {
    "objectID": "slides/week-11.2.html#standard-deviation-simple-example-1",
    "href": "slides/week-11.2.html#standard-deviation-simple-example-1",
    "title": "Distributions II",
    "section": "Standard Deviation Simple Example",
    "text": "Standard Deviation Simple Example\n\n\nx = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\ne_squared &lt;- e^2\ne_squared\n\n [1] 25 16  9  4  1  0  1  4  9 16 25"
  },
  {
    "objectID": "slides/week-11.2.html#standard-deviation-simple-example-2",
    "href": "slides/week-11.2.html#standard-deviation-simple-example-2",
    "title": "Distributions II",
    "section": "Standard Deviation Simple Example",
    "text": "Standard Deviation Simple Example\n\n\nx = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nsum_e_squared &lt;- sum(e_squared)\nsum_e_squared\n\n[1] 110"
  },
  {
    "objectID": "slides/week-11.2.html#standard-deviation-simple-example-3",
    "href": "slides/week-11.2.html#standard-deviation-simple-example-3",
    "title": "Distributions II",
    "section": "Standard Deviation Simple Example",
    "text": "Standard Deviation Simple Example\n\n\nvariance &lt;- sum_e_squared/(length(x)-1)\nvariance\n\n[1] 11"
  },
  {
    "objectID": "slides/week-11.2.html#standard-deviation-simple-example-4",
    "href": "slides/week-11.2.html#standard-deviation-simple-example-4",
    "title": "Distributions II",
    "section": "Standard Deviation Simple Example",
    "text": "Standard Deviation Simple Example\n\n\nstandard_dev &lt;- sqrt(variance)\nstandard_dev\n\n[1] 3.316625\n\nsd(x)\n\n[1] 3.316625"
  },
  {
    "objectID": "slides/week-11.2.html#your-turn-1",
    "href": "slides/week-11.2.html#your-turn-1",
    "title": "Distributions II",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nCalculate measures of spread for the polyarchy variable in the V-Dem data (mean, median, IQR, standard deviation)\nHow would you interpret these measures?\nTry a box plot for the polyarchy variable\nTry another variable in the V-Dem data\nHow does it compare to polyarchy?\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-11.2.html#calculating-statistics-by-groups",
    "href": "slides/week-11.2.html#calculating-statistics-by-groups",
    "title": "Distributions II",
    "section": "Calculating Statistics by groups",
    "text": "Calculating Statistics by groups\n\n\nWhat if we want to describe electoral democracy and see how it differs by some different variable? For example, by world region, or by year?\nIn this case we want to combine numerical summaries with categorical variables\nThis brings us back to bar chart"
  },
  {
    "objectID": "slides/week-11.2.html#calculating-statistics-by-groups-1",
    "href": "slides/week-11.2.html#calculating-statistics-by-groups-1",
    "title": "Distributions II",
    "section": "Calculating Statistics by Groups",
    "text": "Calculating Statistics by Groups\n\nLet’s calculate the mean and median of electoral democracy in each world region\nFor this, we add the group_by() to our previous code\n\n\nvdem2022 |&gt;\n  group_by(region) |&gt;\n  summarize(mean_dem = mean(polyarchy, na.rm = TRUE),\n            median_dem = median(polyarchy, na.rm = TRUE))\n\n# A tibble: 6 × 3\n  region         mean_dem median_dem\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;\n1 Africa            0.315      0.254\n2 Asia              0.354      0.328\n3 Eastern Europe    0.475      0.468\n4 Latin America     0.532      0.585\n5 Middle East       0.213      0.166\n6 The West          0.846      0.868"
  },
  {
    "objectID": "slides/week-11.2.html#calculating-statistics-by-groups-2",
    "href": "slides/week-11.2.html#calculating-statistics-by-groups-2",
    "title": "Distributions II",
    "section": "Calculating Statistics by Groups",
    "text": "Calculating Statistics by Groups\n\nLet’s store our statistics as a new data object, democracy_region\n\n\ndemocracy_region &lt;- vdem2022 |&gt; \n  group_by(region) |&gt;\n  summarize(\n    mean_dem = mean(polyarchy, na.rm = TRUE),\n    median_dem = median(polyarchy, na.rm = TRUE)\n  )\n\ndemocracy_region\n\n# A tibble: 6 × 3\n  region         mean_dem median_dem\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;\n1 Africa            0.315      0.254\n2 Asia              0.354      0.328\n3 Eastern Europe    0.475      0.468\n4 Latin America     0.532      0.585\n5 Middle East       0.213      0.166\n6 The West          0.846      0.868"
  },
  {
    "objectID": "slides/week-11.2.html#visualize-using-our-bar-chart-skills",
    "href": "slides/week-11.2.html#visualize-using-our-bar-chart-skills",
    "title": "Distributions II",
    "section": "Visualize using our Bar Chart Skills",
    "text": "Visualize using our Bar Chart Skills\n\n\nCode\nggplot(democracy_region, aes(x = reorder(region, -mean_dem), y = mean_dem)) +\n  geom_col(fill = \"steelblue\") + \n  labs(\n    x = \"Region\", \n    y = \"Mean Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    ) + \n  theme_minimal()"
  },
  {
    "objectID": "slides/week-11.2.html#numerical-variable-by-group",
    "href": "slides/week-11.2.html#numerical-variable-by-group",
    "title": "Distributions II",
    "section": "Numerical Variable by Group",
    "text": "Numerical Variable by Group\nHow should we interpret this plot?\n\n\nCode\nlibrary(ggridges)\n#library(forcats)\n  ggplot(vdem2022, aes(x = polyarchy, y = region, fill = region)) +\n    geom_density_ridges() +\n  labs(\n    x = \"Electoral Democracy\",\n    y = \"Region\",\n    title = \"A Ridge Plot\",\n    caption = \"Source: V-Dem Institute\",\n  ) +\n  scale_fill_viridis_d() +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-11.2.html#your-turn-2",
    "href": "slides/week-11.2.html#your-turn-2",
    "title": "Distributions II",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nMake a bar chart summarizing polyarchy or some other V-Dem variable\nNow try your hand at a ridge plot\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-1.1.html#lets-get-going-.-.-.",
    "href": "slides/week-1.1.html#lets-get-going-.-.-.",
    "title": "Coding Examples",
    "section": "Let’s get going . . .",
    "text": "Let’s get going . . .\n\nLet’s open up the Week 01 module on Posit Cloud…\nAnd work through the examples there."
  },
  {
    "objectID": "slides/week-1.1.html#example-make-a-map",
    "href": "slides/week-1.1.html#example-make-a-map",
    "title": "Coding Examples",
    "section": "Example: Make a map!",
    "text": "Example: Make a map!"
  },
  {
    "objectID": "slides/week-1.1.html#example-make-a-map-1",
    "href": "slides/week-1.1.html#example-make-a-map-1",
    "title": "Coding Examples",
    "section": "Example: Make a map!",
    "text": "Example: Make a map!\n\n\nlibrary(leaflet)\nleaflet() %&gt;% \n  addTiles() %&gt;%   # Add default OpenStreetMap map tiles\n  addMarkers(lat = 38.90243843683386, lng =  -77.0443814477152, \n             label = \"Elliott School of International Affairs\")\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-1.1.html#plotting-democracy-over-time",
    "href": "slides/week-1.1.html#plotting-democracy-over-time",
    "title": "Coding Examples",
    "section": "Plotting Democracy Over Time",
    "text": "Plotting Democracy Over Time"
  },
  {
    "objectID": "slides/week-1.1.html#example-plotting-democracy-over-time",
    "href": "slides/week-1.1.html#example-plotting-democracy-over-time",
    "title": "Coding Examples",
    "section": "Example: Plotting Democracy Over Time",
    "text": "Example: Plotting Democracy Over Time\n\n# Load the packages\nlibrary(vdemlite)\nlibrary(ggplot2)\n\n# Use vdemlite to extract democracy scores for France and INdia\ndem_data &lt;- fetchdem(indicators = \"v2x_polyarchy\",\n                     countries = c(\"FRA\", \"IND\"))\n\n# And now we can plot the data\nggplot(dem_data, aes(y = v2x_polyarchy, x = year, color=country_name)) +\n  geom_line() +\n  theme_minimal() +\n  xlab(\"Year\") +\n  ylab(\"Electoral Democracy Index\") +\n  ggtitle(\"Electoral Democracy, 1970-2022\") +\n  geom_hline(yintercept = .5, linetype = \"dashed\", color = \"grey\") +\n   scale_color_manual(name=\"Country\", values=c(\"#E69F00\", \"#56B4E9\")) +\n  ylim(0, 1)\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-1.1.html#example-un-voting-trends",
    "href": "slides/week-1.1.html#example-un-voting-trends",
    "title": "Coding Examples",
    "section": "Example: UN Voting Trends",
    "text": "Example: UN Voting Trends"
  },
  {
    "objectID": "slides/week-1.1.html#example-un-voting-trends-1",
    "href": "slides/week-1.1.html#example-un-voting-trends-1",
    "title": "Coding Examples",
    "section": "Example: UN Voting Trends",
    "text": "Example: UN Voting Trends\n\nlibrary(tidyverse)\nlibrary(unvotes)\nlibrary(lubridate)\nlibrary(scales)\nlibrary(pacman)\n\nunvotes &lt;- un_votes %&gt;%\n  inner_join(un_roll_calls, by = \"rcid\") %&gt;%\n  inner_join(un_roll_call_issues, by = \"rcid\")\n\nunvotes %&gt;%\n  # then filter out to only include the countries we want\n  filter(country %in% c(\"South Africa\", \"United States\", \"France\")) %&gt;%\n  # then make sure R understands the the year variable is a data\n  mutate(year = year(date)) %&gt;%\n  # Then group the data by country and year\n  group_by(country, year, issue) %&gt;%\n  # then take the average Yes votes for each country_year\n  summarize(percent_yes = mean(vote == \"yes\")) %&gt;%\n  # then make a nice plot\n  ggplot(mapping = aes(x = year, y = percent_yes, color = country)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  facet_wrap(~issue) +\n  scale_y_continuous(labels = percent) +\n  scale_color_manual( values=c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) +\n  labs(\n    title = \"Percentage of 'Yes' votes in the UN General Assembly\",\n    subtitle = \"1946 to 2019\",\n    y = \"% Yes\",\n    x = \"Year\",\n    color = \"Country\"\n  )\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-11.1.html#section",
    "href": "slides/week-11.1.html#section",
    "title": "Distributions I",
    "section": "",
    "text": "What are some ways we can classify data? 😎 💭\n\n\nanecdotal vs. representative\ncensus vs. sample\nobservational vs. experimental\ncategorical vs. numerical\ndiscrete vs. continuous\ncross-sectional vs. time series\nlongitudinal vs. panel\nunstructured vs. structured"
  },
  {
    "objectID": "slides/week-11.1.html#section-1",
    "href": "slides/week-11.1.html#section-1",
    "title": "Distributions I",
    "section": "",
    "text": "What are some ways we can classify data? 😎 💭\n\nanecdotal vs. representative\ncensus vs. sample\nobservational vs. experimental\ncategorical vs. numerical\ndiscrete vs. continuous\ncross-sectional vs. time series\nlongitudinal vs. panel\nunstructured vs. structured"
  },
  {
    "objectID": "slides/week-11.1.html#variable-types",
    "href": "slides/week-11.1.html#variable-types",
    "title": "Distributions I",
    "section": "Variable Types",
    "text": "Variable Types\n\nCategorical\n\nBinary - two categories\nNominal - multiple unordered categories\nOrdinal - multiple ordered categories\n\nNumerical\n\nContinuous - fractional values (measurement data)\nDiscrete - non-negative whole numbers (count data)"
  },
  {
    "objectID": "slides/week-11.1.html#section-2",
    "href": "slides/week-11.1.html#section-2",
    "title": "Distributions I",
    "section": "",
    "text": "What types of variables are these? 🤔\n\nIs a country a democracy? (yes/no)\nPolity (-10 to 10 in 1 unit increments)\nV-Dem Polyarchy (0-1 in 0.01 increments)\nV-Dem Regimes of the World Measure\n\nclosed autocracy | electoral autocracy | electoral democracy | democracy\n\nNumber of protest events\nProtest types (sit in, march, strike, etc.)"
  },
  {
    "objectID": "slides/week-11.1.html#v-dem-regimes-of-the-world-measure",
    "href": "slides/week-11.1.html#v-dem-regimes-of-the-world-measure",
    "title": "Distributions I",
    "section": "V-Dem Regimes of the World Measure",
    "text": "V-Dem Regimes of the World Measure\n\n\nClosed Autocracy\nElectoral Autocracy\nElectoral Democracy\nLiberal Democracy"
  },
  {
    "objectID": "slides/week-11.1.html#data-setup",
    "href": "slides/week-11.1.html#data-setup",
    "title": "Distributions I",
    "section": "Data Setup",
    "text": "Data Setup\n\nlibrary(tidyverse)\nlibrary(vdemlite)\n\nvdem2022 &lt;- fetchdem(indicators = c(\"v2x_regime\", \"e_regionpol_6C\"), \n                     start_year = 2022, end_year = 2022) |&gt;\n  rename(\n    country = country_name, \n    regime = v2x_regime, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, \n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\"),\n    regime = case_match(regime,\n                    0 ~ \"Closed Autocracy\",\n                    1 ~ \"Electoral Autocracy\",\n                    2 ~  \"Electoral Democracy\",\n                    3 ~  \"Liberal Democracy\")\n  )"
  },
  {
    "objectID": "slides/week-11.1.html#examine-the-data",
    "href": "slides/week-11.1.html#examine-the-data",
    "title": "Distributions I",
    "section": "Examine the Data",
    "text": "Examine the Data\n\n\nglimpse(vdem2022)\n\nRows: 179\nColumns: 6\n$ country         &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\"…\n$ country_text_id &lt;chr&gt; \"MEX\", \"SUR\", \"SWE\", \"CHE\", \"GHA\", \"ZAF\", \"JPN\", \"MMR\"…\n$ country_id      &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 19, 2…\n$ year            &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, …\n$ regime          &lt;chr&gt; \"Electoral Democracy\", \"Electoral Democracy\", \"Liberal…\n$ region          &lt;chr&gt; \"Latin America\", \"Latin America\", \"The West\", \"The Wes…"
  },
  {
    "objectID": "slides/week-11.1.html#section-3",
    "href": "slides/week-11.1.html#section-3",
    "title": "Distributions I",
    "section": "",
    "text": "Let’s count the number of regimes by type…\n\n\nvdem2022 |&gt;\n  count(regime)\n\n               regime  n\n1    Closed Autocracy 33\n2 Electoral Autocracy 54\n3 Electoral Democracy 58\n4   Liberal Democracy 34"
  },
  {
    "objectID": "slides/week-11.1.html#section-4",
    "href": "slides/week-11.1.html#section-4",
    "title": "Distributions I",
    "section": "",
    "text": "Now let’s visualize the distribution of regimes with a bar plot…"
  },
  {
    "objectID": "slides/week-11.1.html#section-5",
    "href": "slides/week-11.1.html#section-5",
    "title": "Distributions I",
    "section": "",
    "text": "Now let’s visualize the distribution of regimes with a bar plot…\n\n\nvdem2022 |&gt;\n  ggplot(aes(x = regime)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    x = \"Regime\",\n    y = \"Frequency\",\n    title = \"Regimes of the World in 2022\",\n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-11.1.html#section-6",
    "href": "slides/week-11.1.html#section-6",
    "title": "Distributions I",
    "section": "",
    "text": "Now let’s visualize the distribution of regimes with a bar plot…\n\n\nvdem2022 |&gt;\n  ggplot(aes(x = regime)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    x = \"Regime\",\n    y = \"Frequency\",\n    title = \"Regimes of the World in 2022\",\n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-11.1.html#more-about-geom_bar",
    "href": "slides/week-11.1.html#more-about-geom_bar",
    "title": "Distributions I",
    "section": "More about geom_bar()",
    "text": "More about geom_bar()\n\n\ngeom_bar() is different from geom_col()\nUsed to create bar plots where the height of the bar represents counts or frequencies of categorical variable\nBy default, geom_bar() counts the number of occurrences of each category or group and plots it as the height of the bar\nLike geom_histogram(), geom_bar() only requires the x aesthetic (y is automatically calcualted for you)"
  },
  {
    "objectID": "slides/week-11.1.html#your-turn",
    "href": "slides/week-11.1.html#your-turn",
    "title": "Distributions I",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nExpore the distribution of regimes for a different year\nPreprocess your data to include only the year you are interested in\nVisualize the distribution of regimes using geom_bar()\nUse the labs() function to change title\nWhat is different about the year that you chose relative to 2022?\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-11.1.html#how-do-regimes-vary-by-region",
    "href": "slides/week-11.1.html#how-do-regimes-vary-by-region",
    "title": "Distributions I",
    "section": "How do Regimes Vary by Region?",
    "text": "How do Regimes Vary by Region?\n\n\n\nCode\nvdem2022 |&gt;\n  ggplot(aes(x = region, fill = regime)) +\n      geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Frequency\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-11.1.html#how-do-regimes-vary-by-region-1",
    "href": "slides/week-11.1.html#how-do-regimes-vary-by-region-1",
    "title": "Distributions I",
    "section": "How do Regimes Vary by Region?",
    "text": "How do Regimes Vary by Region?\n\n\nvdem2022 |&gt;\n  ggplot(aes(x = region, fill = regime)) +\n      geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Frequency\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-11.1.html#how-do-regimes-vary-by-region-2",
    "href": "slides/week-11.1.html#how-do-regimes-vary-by-region-2",
    "title": "Distributions I",
    "section": "How do Regimes Vary by Region?",
    "text": "How do Regimes Vary by Region?\n\n\nvdem2022 |&gt;\n  ggplot(aes(x = region, fill = regime)) +\n      geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Frequency\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-11.1.html#what-did-we-just-do",
    "href": "slides/week-11.1.html#what-did-we-just-do",
    "title": "Distributions I",
    "section": "What Did We Just Do?",
    "text": "What Did We Just Do?\n\n\nWe used geom_bar() to visualize the distribution of regimes by region\nEssentially, we used two categorical variables to create a bar plot\nRegime type is…\n\n…an ordered categorical variable\n\nRegion is…\n\n…a nominal categorical variable"
  },
  {
    "objectID": "slides/week-11.1.html#section-7",
    "href": "slides/week-11.1.html#section-7",
    "title": "Distributions I",
    "section": "",
    "text": "Some regions have more countries than others. Why does this create an issue for telling a story with our data here?"
  },
  {
    "objectID": "slides/week-11.1.html#show-proportions-instead",
    "href": "slides/week-11.1.html#show-proportions-instead",
    "title": "Distributions I",
    "section": "Show Proportions Instead",
    "text": "Show Proportions Instead\n\n\n\nCode\nvdem2022 %&gt;%\n  ggplot(., aes(x = region, fill = regime)) +\n      geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Proportion\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-11.1.html#section-8",
    "href": "slides/week-11.1.html#section-8",
    "title": "Distributions I",
    "section": "",
    "text": "We use position = \"fill\" to normalize the data and make the plot more interpretable…\n\n\nvdem2022 %&gt;%\n  ggplot(., aes(x = region, fill = regime)) +\n      geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Proportion\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)"
  },
  {
    "objectID": "slides/week-11.1.html#your-turn-1",
    "href": "slides/week-11.1.html#your-turn-1",
    "title": "Distributions I",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nExplore the distribution of regimes by region for a different year\nPreprocess your data to include only the year you are interested in\nVisualize the distribution of regimes using geom_bar() and position = \"fill\"\nUse the labs() function to change title\nWhat is different about the year that you chose relative to 2022?\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-1.2.html#rstudio",
    "href": "slides/week-1.2.html#rstudio",
    "title": "Meet ouR Tech Stack",
    "section": "RStudio",
    "text": "RStudio\n\nRStudio is an integrated development environment (IDE) for R\nRStudio has four panes\n\nSource-where you write code\nConsole-where you run code\nEnvironment-where you see objects in memory\nFiles/Plots/Packages/Help-where you can navigate files, see plots, install packages, and get help\n\nLet’s open RStudio and have a look around"
  },
  {
    "objectID": "slides/week-1.2.html#quarto",
    "href": "slides/week-1.2.html#quarto",
    "title": "Meet ouR Tech Stack",
    "section": "Quarto",
    "text": "Quarto\n\n\nQuarto is an open-source scientific publishing platform\nAllows you to integrate text with code\nKind of like a word processor for data science\nCan use it to create reports, books, websites, etc.\nCan make HTML, PDF, Word, and other formats\nCan use R, Python, Julia, and other languages"
  },
  {
    "objectID": "slides/week-1.2.html#project-oriented-workflow",
    "href": "slides/week-1.2.html#project-oriented-workflow",
    "title": "Meet ouR Tech Stack",
    "section": "Project Oriented Workflow",
    "text": "Project Oriented Workflow\n\n\nAlways start a document in a project folder\n\nThat way you don’t have to do setwd\nAlso can share easily with other people\n\nGo to File&gt;New Project\nCreate a Quarto project folder"
  },
  {
    "objectID": "slides/week-1.2.html#visual-editor",
    "href": "slides/week-1.2.html#visual-editor",
    "title": "Meet ouR Tech Stack",
    "section": "Visual Editor",
    "text": "Visual Editor\n\nThere are two ways to edit Quarto docs\n\nSource (markdown)\nVisual editor\n\nVisual editor\n\nWYSIWYM\nApproximates appearance\n\nTry both and see what you like"
  },
  {
    "objectID": "slides/week-1.2.html#rendering-documents",
    "href": "slides/week-1.2.html#rendering-documents",
    "title": "Meet ouR Tech Stack",
    "section": "Rendering Documents",
    "text": "Rendering Documents\n\nRendering = converting to another format\n\nDefault format is HTML\nCan also render to PDF, Word, etc.\n\nTo render a Quarto document\n\nClick on the Render button\n\nOr keyboard shortcut (Cmd/Ctrl + Shift + K)\n\n\nBy default, Quarto will preview the document in your browser\nBut you can also preview in Viewer pane\n\nClick on the gear icon next to the Render button\nSelect “Preview in Viewer Pane”"
  },
  {
    "objectID": "slides/week-1.2.html#illustration",
    "href": "slides/week-1.2.html#illustration",
    "title": "Meet ouR Tech Stack",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "slides/week-1.2.html#lets-try-quarto",
    "href": "slides/week-1.2.html#lets-try-quarto",
    "title": "Meet ouR Tech Stack",
    "section": "Let’s Try Quarto!",
    "text": "Let’s Try Quarto!\n\nCreate a new Quarto document\n\nFile&gt;New File&gt;Quarto Document\n\nSave the document in your project folder\nRender it\n\nClick on the Render button\nOr keyboard shortcut (Cmd/Ctr + Shift + K)\n\nTry out the visual editor\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/week-1.2.html#document-elements",
    "href": "slides/week-1.2.html#document-elements",
    "title": "Meet ouR Tech Stack",
    "section": "Document Elements",
    "text": "Document Elements\n\n\nYAML Header\nMarkdown content\nCode chunks"
  },
  {
    "objectID": "slides/week-1.2.html#yaml-header",
    "href": "slides/week-1.2.html#yaml-header",
    "title": "Meet ouR Tech Stack",
    "section": "YAML Header",
    "text": "YAML Header\n\n\nMetadata about the document\n\nTitle, author, date, etc.\n\nOutput format\nExecution options"
  },
  {
    "objectID": "slides/week-1.2.html#yaml-header-1",
    "href": "slides/week-1.2.html#yaml-header-1",
    "title": "Meet ouR Tech Stack",
    "section": "YAML Header",
    "text": "YAML Header\n---\ntitle: \"My Document\"\nauthor: \"Your Name\"\ndate: today\ndate-format: long\nformat: html\nexecute:\n  echo: false\n  message: false\n---\n\nTry adding some of these options in your document\nThen render it again\nLook in the Quarto guide for other options to try\nCan you figure out how to add a theme?\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-1.2.html#markdown",
    "href": "slides/week-1.2.html#markdown",
    "title": "Meet ouR Tech Stack",
    "section": "Markdown",
    "text": "Markdown\n\n\nMarkdown is a simple markup language\nYou can use it to format text\nYou can also use it to embed images, tables, etc.\nAnd to embed code chunks…"
  },
  {
    "objectID": "slides/week-1.2.html#markdown-syntax---basic-authoring",
    "href": "slides/week-1.2.html#markdown-syntax---basic-authoring",
    "title": "Meet ouR Tech Stack",
    "section": "Markdown Syntax - Basic Authoring",
    "text": "Markdown Syntax - Basic Authoring\n\nFor basic text you can just start typing\nFor line breaks use two spaces and return (enter)\nHeadings (use #, ##, ###, etc.)\n\n# is the largest heading (level 1)\n## is the next largest (level 2)\n### is the next largest (level 3)\nEtc."
  },
  {
    "objectID": "slides/week-1.2.html#markdown-syntax---styling",
    "href": "slides/week-1.2.html#markdown-syntax---styling",
    "title": "Meet ouR Tech Stack",
    "section": "Markdown Syntax - Styling",
    "text": "Markdown Syntax - Styling\n\nEmphasis = Italics (use *)\n\nBold (use **)\n\nLists\n\nBullet points (use -)\nNumbered lists (use 1.)"
  },
  {
    "objectID": "slides/week-1.2.html#markdown-syntax---content",
    "href": "slides/week-1.2.html#markdown-syntax---content",
    "title": "Meet ouR Tech Stack",
    "section": "Markdown Syntax - Content",
    "text": "Markdown Syntax - Content\n\nLinks (use [text](url))\nImages (use ![](file path or url))\nCode chunks\n\nR code chunks (```{r}…```)\nPython code chunks (```{python}…```)\nEtc."
  },
  {
    "objectID": "slides/week-1.2.html#try-some-markdown",
    "href": "slides/week-1.2.html#try-some-markdown",
    "title": "Meet ouR Tech Stack",
    "section": "Try Some Markdown",
    "text": "Try Some Markdown\n\n\nCheck out the Markdown Cheatsheet\nTry editing the markdown in your document\nTry some of the other things you find in the guide\nThen render it again\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-1.2.html#code-chunks",
    "href": "slides/week-1.2.html#code-chunks",
    "title": "Meet ouR Tech Stack",
    "section": "Code Chunks",
    "text": "Code Chunks\n\nIncorporate R code (could also be Python, Julia, etc.)\nAdd a code chunk with the ‘+’ button\nRun the code chunk by clicking the play button\n\nOr use keyboard shortcut (Cmd/Ctrl + Shift + Enter)\n\nRun all chunks up that point by clicking the down arrow\n\nOr use keyboard shortcut (Cmd/Ctrl + Shift + K)\n\nRun a single line with shortcut (Cmd/Ctrl + Enter)"
  },
  {
    "objectID": "slides/week-1.2.html#code-chunk-options",
    "href": "slides/week-1.2.html#code-chunk-options",
    "title": "Meet ouR Tech Stack",
    "section": "Code Chunk Options",
    "text": "Code Chunk Options\n\nUse #| (hash-pipe) to add options\nlabel is a unique identifier for the chunk\nOptions to control what happens when you render\n\necho controls whether the code is shown\neval controls whether the code is run\nmessage controls whether messages are shown\nwarning controls whether warnings are shown"
  },
  {
    "objectID": "slides/week-1.2.html#code-chunk-options-1",
    "href": "slides/week-1.2.html#code-chunk-options-1",
    "title": "Meet ouR Tech Stack",
    "section": "Code Chunk Options",
    "text": "Code Chunk Options\n\n\nCode-chunk options override global options set in YAML header\nSee documentation for more options\nYou can also use write chunk options inline with chunk name,\n\ne.g., {r, echo = FALSE} ..."
  },
  {
    "objectID": "slides/week-1.2.html#illustration-1",
    "href": "slides/week-1.2.html#illustration-1",
    "title": "Meet ouR Tech Stack",
    "section": "Illustration",
    "text": "Illustration"
  },
  {
    "objectID": "slides/week-1.2.html#try-it-yourself",
    "href": "slides/week-1.2.html#try-it-yourself",
    "title": "Meet ouR Tech Stack",
    "section": "Try it Yourself!",
    "text": "Try it Yourself!\n\nCreate a code chunk\nCopy this code chunk into your document\n\n\nlibrary(ggplot2)\n\nggplot(mpg, aes(displ, hwy, colour = class)) + \n  geom_point()\n\n\nTry adding some chunk options in your document\nThen render it again\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-5.2.html#how-do-we-get-tidyclean-data",
    "href": "slides/week-5.2.html#how-do-we-get-tidyclean-data",
    "title": "Working With Data",
    "section": "How Do We Get Tidy/Clean Data?",
    "text": "How Do We Get Tidy/Clean Data?\n\n\nGet lucky and find it (like on kaggle)\nWrangle it ourselves\nUse a package where it has been wrangled for us\nDownload via an API"
  },
  {
    "objectID": "slides/week-5.2.html#this-lesson",
    "href": "slides/week-5.2.html#this-lesson",
    "title": "Working With Data",
    "section": "This Lesson",
    "text": "This Lesson\n\nPractice with World Bank and V-Dem data\nWorld Bank data through wbstats\n\nThere is another package called WDI\nBoth packages for accessing data through WB API\n\nVarieties of Democracy (V-Dem) through vdemlite\n\nThere is also a package called vdemdata\nvdemlite offers more functionality, works better in the cloud"
  },
  {
    "objectID": "slides/week-5.2.html#filter-select-mutate",
    "href": "slides/week-5.2.html#filter-select-mutate",
    "title": "Working With Data",
    "section": "filter(), select(), mutate()",
    "text": "filter(), select(), mutate()\n\nAlong the way we will practice some important dplyr verbs:\n\n\nfilter() is used to select observations based on their values\nselect() is used to select variables\nmutate() is used to create new variables or modifying existing ones\n\n\nAs well as some helpful functions from the janitor package."
  },
  {
    "objectID": "slides/week-5.2.html#what-is-a-data-frame",
    "href": "slides/week-5.2.html#what-is-a-data-frame",
    "title": "Working With Data",
    "section": "What is a Data Frame?",
    "text": "What is a Data Frame?\n\n\nSpecial kind of tabular data used in data science\nEach column can be a different data type\nData frames are the most common data structure in R"
  },
  {
    "objectID": "slides/week-5.2.html#what-is-a-tibble",
    "href": "slides/week-5.2.html#what-is-a-tibble",
    "title": "Working With Data",
    "section": "What is a Tibble?",
    "text": "What is a Tibble?\n\n\nModern data frames in R\nOffers better printing and subsetting behaviors\nDoes not convert character vectors to factors by default\nDisplays only the first 10 rows and as many columns as fit on screen\nColumn names are preserved exactly, even if they contain spaces"
  },
  {
    "objectID": "slides/week-5.2.html#creating-a-tibble",
    "href": "slides/week-5.2.html#creating-a-tibble",
    "title": "Working With Data",
    "section": "Creating a Tibble",
    "text": "Creating a Tibble\n\n\nWhen you read data into R with readr you automatically get a tibble\nYou can create a tibble using tibble() from the tibble package:\n\n\n  library(tibble)\n  \n  # Create a tibble\n  my_tibble &lt;- tibble(\n    name = c(\"Alice\", \"Bob\", \"Charlie\"),\n    age = c(25, 30, 35),\n    height = c(160, 170, 180),\n    is_student = c(TRUE, FALSE, FALSE)\n  )\n  \nmy_tibble  \n\n# A tibble: 3 × 4\n  name      age height is_student\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt;     \n1 Alice      25    160 TRUE      \n2 Bob        30    170 FALSE     \n3 Charlie    35    180 FALSE"
  },
  {
    "objectID": "slides/week-5.2.html#common-data-types",
    "href": "slides/week-5.2.html#common-data-types",
    "title": "Working With Data",
    "section": "Common Data Types",
    "text": "Common Data Types\n\n&lt;chr&gt; (Character): Stores text strings\n\nExample: \"hello\", \"R programming\"\n\n&lt;dbl&gt; (Double): Stores decimal (floating-point) numbers\n\nExample: 3.14, -1.0\n\n&lt;int&gt; (Integer): Stores whole numbers (integers)\n\nExample: 1, -100, 42\n\n&lt;lgl&gt; (Logical): Stores boolean values (TRUE, FALSE, NA)\n\nExample: TRUE, FALSE, NA\n\n&lt;fct&gt; (Factor): Stores categorical variables with fixed levels\n\nExample: factor(c(\"low\", \"medium\", \"high\"))\n\n&lt;date&gt; (Date): Stores dates in the “YYYY-MM-DD” format\n\nExample: as.Date(\"2024-09-05\")"
  },
  {
    "objectID": "slides/week-5.2.html#other-data-types",
    "href": "slides/week-5.2.html#other-data-types",
    "title": "Working With Data",
    "section": "Other Data Types",
    "text": "Other Data Types\n\n\n&lt;dttm&gt; (Date-Time or POSIXct): Stores date-time objects (both date and time).\n\nExample: as.POSIXct(\"2024-09-05 14:30:00\")\n\n&lt;time&gt; (Time): Specifically stores time-of-day values (rarely seen without a date)\n\nExample: \"14:30:00\"\n\n&lt;list&gt; (List): Stores lists, where each entry can be a complex object.\n\nExample: list(c(1, 2, 3), c(\"a\", \"b\", \"c\"))"
  },
  {
    "objectID": "slides/week-5.2.html#dates-and-times-with-lubridate",
    "href": "slides/week-5.2.html#dates-and-times-with-lubridate",
    "title": "Working With Data",
    "section": "Dates and Times with lubridate",
    "text": "Dates and Times with lubridate\n\nlubridate is an R package that makes it easier to work with dates and times\nUse convenient functions to store dates in different formats\n\n\nlibrary(lubridate)\n  \n# Store a date\nmy_date &lt;- ymd(\"2024-09-05\")\nmy_date2 &lt;- mdy(\"09-05-2024\")\nmy_date3 &lt;- dmy(\"05-09-2024\")\n  \n# Print in long form\nformat(my_date, \"%B %d, %Y\")\n\n[1] \"September 05, 2024\""
  },
  {
    "objectID": "slides/week-5.2.html#your-turn",
    "href": "slides/week-5.2.html#your-turn",
    "title": "Working With Data",
    "section": "Your Turn",
    "text": "Your Turn\n\n\nCreate your own tibble\nMake it on a topic you find interesting\nTry to include at least three data types\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-5.2.html#apis-1",
    "href": "slides/week-5.2.html#apis-1",
    "title": "Working With Data",
    "section": "APIs",
    "text": "APIs\n\n\nAPI stands for “Application Programming Interface”\nWay for two computers to talk to each other\nIn our case, we will use APIs to download social science data"
  },
  {
    "objectID": "slides/week-5.2.html#apis-in-r",
    "href": "slides/week-5.2.html#apis-in-r",
    "title": "Working With Data",
    "section": "APIs in R",
    "text": "APIs in R\n\nAPIs are accessed through packages in R\nSometimes there can be more than one package for an API\nMuch easier than reading in data from messy flat file!\nWe will use a few API packages in this course\n\nWorld Bank data through wbstats (or WDI)\nfredr for Federal Reserve Economic Data\ntidycensus for US Census data\n\nBut there are many APIs out there (please explore!)"
  },
  {
    "objectID": "slides/week-5.2.html#searching-for-wb-indicators",
    "href": "slides/week-5.2.html#searching-for-wb-indicators",
    "title": "Working With Data",
    "section": "Searching for WB Indicators",
    "text": "Searching for WB Indicators\n\n\nflfp_indicators &lt;- wb_search(\"female labor force\") # store the list of indicators\n\nprint(flfp_indicators, n=26) # view the indicators"
  },
  {
    "objectID": "slides/week-5.2.html#wbstats-example",
    "href": "slides/week-5.2.html#wbstats-example",
    "title": "Working With Data",
    "section": "wbstats Example",
    "text": "wbstats Example\n\n\n# Load packages\nlibrary(wbstats) # for downloading WB data\nlibrary(dplyr) # for selecting, renaming and mutating\nlibrary(janitor) # for rounding\n\n# Store the list of indicators in an object\nindicators &lt;- c(\"flfp\" = \"SL.TLF.CACT.FE.ZS\", \"women_rep\" = \"SG.GEN.PARL.ZS\") \n\n# Download the data  \nwomen_emp &lt;- wb_data(indicators, mrv = 50) |&gt; # download data for last 50 yrs\n  select(!iso2c) |&gt; # drop the iso2c code which we won't be using\n  rename(year = date) |&gt; # rename date to year \n  mutate(\n    flfp = round_to_fraction(flfp, denominator = 100), # round to nearest 100th\n    women_rep = round_to_fraction(women_rep, denominator = 100) \n  )\n\n# View the data\nglimpse(women_emp)"
  },
  {
    "objectID": "slides/week-5.2.html#your-turn-1",
    "href": "slides/week-5.2.html#your-turn-1",
    "title": "Working With Data",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nSearch for a WB indicator\nDownload the data\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-5.2.html#the-v-dem-dataset",
    "href": "slides/week-5.2.html#the-v-dem-dataset",
    "title": "Working With Data",
    "section": "The V-Dem Dataset",
    "text": "The V-Dem Dataset\n\n\nV-Dem stands for Varieties of Democracy\nIt is a dataset that measures democracy around the world\nBased on expert assessments of the quality of democracy in each country\nTwo packages we will explore: vdemlite and vdemdata"
  },
  {
    "objectID": "slides/week-5.2.html#vdemlite",
    "href": "slides/week-5.2.html#vdemlite",
    "title": "Working With Data",
    "section": "vdemlite",
    "text": "vdemlite\n\n\nCovers a few hundred commonly used indicators and indices from 1970 onward\nCovers everything in this document\nAs opposed to 4000+ indicators from the 18th century onward\nAdds some functionality for working with the data\nEasier to work with in the cloud and apps"
  },
  {
    "objectID": "slides/week-5.2.html#vdemlite-fuctions",
    "href": "slides/week-5.2.html#vdemlite-fuctions",
    "title": "Working With Data",
    "section": "vdemlite fuctions",
    "text": "vdemlite fuctions\n\n\nfetchdem() to download the data\nsummarizedem() provides searchable table of indicators with summary stats\nsearchdem() to search for specific indicators or all indicators used to construct an index\nSee the vdemlite documentation for more details"
  },
  {
    "objectID": "slides/week-5.2.html#fetchdem",
    "href": "slides/week-5.2.html#fetchdem",
    "title": "Working With Data",
    "section": "fetchdem()",
    "text": "fetchdem()\n\n\n# Load packages\nlibrary(vdemlite) # to download V-Dem data\n\n# Polyarchy and clean elections index for USA and Sweden for 2000-2020\ndem_indicators &lt;- fetchdem(indicators = c(\"v2x_polyarchy\", \"v2xel_frefair\"),\n                           countries = c(\"USA\", \"SWE\"))\n\n# View the data\nglimpse(dem_indicators)"
  },
  {
    "objectID": "slides/week-5.2.html#summarizedem",
    "href": "slides/week-5.2.html#summarizedem",
    "title": "Working With Data",
    "section": "summarizedem()",
    "text": "summarizedem()\n\n\n# Summary statistics for the polyarchy index\nsummarizedem(indicator = \"v2x_polyarchy\")"
  },
  {
    "objectID": "slides/week-5.2.html#searchdem",
    "href": "slides/week-5.2.html#searchdem",
    "title": "Working With Data",
    "section": "searchdem()",
    "text": "searchdem()\n\n\nsearchdem()"
  },
  {
    "objectID": "slides/week-5.2.html#your-turn-2",
    "href": "slides/week-5.2.html#your-turn-2",
    "title": "Working With Data",
    "section": "Your Turn",
    "text": "Your Turn\n\n\nLook at the vdemlite documentation\nTry using searchdem() to find an indicator you are interested in using\nUse summarizedem() to get summary statistics for that variable\nUse fetchdem() to download the data for that variable for a country or countries of interest\nTry using mutate() to add region codes to the data\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-5.1.html#where-does-data-come-from",
    "href": "slides/week-5.1.html#where-does-data-come-from",
    "title": "Understanding Data",
    "section": "Where Does Data Come From?",
    "text": "Where Does Data Come From?\n\nThoughts? 😎 💭\n\n\nYour boss or a client sends you a file\nSurvey data collected by you or someone else\nYou can download it from a website\nYou can scrape it from a website\nA data package (e.g. unvotes)\nYou can access it through an API"
  },
  {
    "objectID": "slides/week-5.1.html#getting-started-with-data",
    "href": "slides/week-5.1.html#getting-started-with-data",
    "title": "Understanding Data",
    "section": "Getting Started with Data",
    "text": "Getting Started with Data\n\n\nTabular data is data that is organized into rows and columns\n\na.k.a. rectangular data\n\nA data frame is a special kind of tabular data used in data science\nA variable is something you can measure\nAn observation is a single unit or case in your data set\nThe unit of analysis is the level at which you are measuring\n\nIn a cross-section: country, state, county, city, individual, etc.\nIn a time-series: year, month, day, etc."
  },
  {
    "objectID": "slides/week-5.1.html#the-concept-of-tidy-data",
    "href": "slides/week-5.1.html#the-concept-of-tidy-data",
    "title": "Understanding Data",
    "section": "The Concept of “Tidy Data”",
    "text": "The Concept of “Tidy Data”\n\n\nEach column represents a single variable\nEach row represents a single observation\nEach cell represents a single value"
  },
  {
    "objectID": "slides/week-5.1.html#tidy-data-example",
    "href": "slides/week-5.1.html#tidy-data-example",
    "title": "Understanding Data",
    "section": "Tidy Data Example",
    "text": "Tidy Data Example"
  },
  {
    "objectID": "slides/week-5.1.html#what-are-clean-data",
    "href": "slides/week-5.1.html#what-are-clean-data",
    "title": "Understanding Data",
    "section": "What are Clean Data?",
    "text": "What are Clean Data?\n\nColumn names are easy to work with and are not duplicated\nMissing values have been dealt with\nThere are no repeated observations or columns\nThere are no blank observations or columns\nThe data are in the proper format, for example dates should be formatted as dates"
  },
  {
    "objectID": "slides/week-5.1.html#messy-data-example",
    "href": "slides/week-5.1.html#messy-data-example",
    "title": "Understanding Data",
    "section": "Messy Data Example",
    "text": "Messy Data Example"
  },
  {
    "objectID": "slides/week-5.1.html#which-of-these-is-likely-tidyclean",
    "href": "slides/week-5.1.html#which-of-these-is-likely-tidyclean",
    "title": "Understanding Data",
    "section": "Which of These is Likely Tidy/Clean?",
    "text": "Which of These is Likely Tidy/Clean?\n\n\nYour boss or a client sends you a file\nSurvey data collected by you or someone else\nYou can download it from a website\nYou can scrape it from a website\nA curated collection (e.g. unvotes)\nYou can access it through an API"
  },
  {
    "objectID": "slides/week-5.1.html#how-do-we-get-tidyclean-data",
    "href": "slides/week-5.1.html#how-do-we-get-tidyclean-data",
    "title": "Understanding Data",
    "section": "How Do We Get Tidy/Clean Data?",
    "text": "How Do We Get Tidy/Clean Data?\n\n\nGet lucky and find it\nWrangle it ourselves\nUse a package where it has been wrangled for us\nDownload via an API"
  },
  {
    "objectID": "slides/week-5.1.html#read-data-into-r",
    "href": "slides/week-5.1.html#read-data-into-r",
    "title": "Understanding Data",
    "section": "Read Data into R",
    "text": "Read Data into R\n\n\nUse read_csv() function from readr package\nreadr package is part of the tidyverse\nCan do more with it than base R functions"
  },
  {
    "objectID": "slides/week-5.1.html#r-code-review",
    "href": "slides/week-5.1.html#r-code-review",
    "title": "Understanding Data",
    "section": "R Code Review",
    "text": "R Code Review\n\n\n&lt;- is the assignment operator\n\nUse it to assign values to objects\n\n# is the comment operator\n\nUse it to comment out code or add comments\nDifferent function than in markdown text\n\nTo call a library, use library() and name of library\n\nname of library does not have to be in quotes, e.g. library(readr)\nonly when you install it, e.g. install.packages(\"readr\")"
  },
  {
    "objectID": "slides/week-5.1.html#read-data-into-r-1",
    "href": "slides/week-5.1.html#read-data-into-r-1",
    "title": "Understanding Data",
    "section": "Read Data into R",
    "text": "Read Data into R\n\n\n# load libraries\nlibrary(readr)\nlibrary(dplry)\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\") #notice file path\n\nglimpse(dem_summary)"
  },
  {
    "objectID": "slides/week-5.1.html#viewing-the-data-in-r",
    "href": "slides/week-5.1.html#viewing-the-data-in-r",
    "title": "Understanding Data",
    "section": "Viewing the Data in R",
    "text": "Viewing the Data in R\n\nUse glimpse() to see the columns and data types:\n\n# load libraries\nlibrary(readr)\nlibrary(dplyr)\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")\n\nglimpse(dem_summary)\n\nRows: 6\nColumns: 5\n$ region    &lt;chr&gt; \"The West\", \"Latin America\", \"Eastern Europe\", \"Asia\", \"Afri…\n$ polyarchy &lt;dbl&gt; 0.8709230, 0.6371358, 0.5387451, 0.4076602, 0.3934166, 0.245…\n$ gdp_pc    &lt;dbl&gt; 37.913054, 9.610284, 12.176554, 9.746391, 4.410484, 21.134319\n$ flfp      &lt;dbl&gt; 52.99082, 48.12645, 50.45894, 50.32171, 56.69530, 26.57872\n$ women_rep &lt;dbl&gt; 28.12921, 21.32548, 17.99728, 14.45225, 17.44296, 10.21568"
  },
  {
    "objectID": "slides/week-5.1.html#try-it-yourself",
    "href": "slides/week-5.1.html#try-it-yourself",
    "title": "Understanding Data",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\n\nOpen the CSV file to see what it looks like\nThen use this code to read it into R and view it\n\n\n# load libraries\nlibrary(readr)\nlibrary(dplyr)\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")\n\nglimpse(dem_summary)\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-5.1.html#write-a-new-csv-file",
    "href": "slides/week-5.1.html#write-a-new-csv-file",
    "title": "Understanding Data",
    "section": "Write a New CSV File",
    "text": "Write a New CSV File\n\nNow try writing the same data to a file with a different name\n\nwrite_csv(dem_summary, \"data/your_new_file_name.csv\") \n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/week-5.1.html#read-in-excel-file",
    "href": "slides/week-5.1.html#read-in-excel-file",
    "title": "Understanding Data",
    "section": "Read in Excel File",
    "text": "Read in Excel File\n\n\nlibrary(readxl)\n\ndem_summary &lt;- read_excel(\"data/dem_summary.xlsx\")\n\nglimpse(dem_summary)"
  },
  {
    "objectID": "slides/week-5.1.html#try-with-excel",
    "href": "slides/week-5.1.html#try-with-excel",
    "title": "Understanding Data",
    "section": "Try With Excel",
    "text": "Try With Excel\n\n\nRead in the Excel file\nFollow same steps as with CSV file\n\nuse read_excel() to read in the data\ninstall and experiment with writexl\n\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-5.1.html#import-data-from-google-sheets",
    "href": "slides/week-5.1.html#import-data-from-google-sheets",
    "title": "Understanding Data",
    "section": "Import Data from Google Sheets",
    "text": "Import Data from Google Sheets\n\n\nCan use googlesheets4\nHave a look at these Gapminder data\nUse gs4_deauth() to authenticate\nThen use read_sheet() to read in the data"
  },
  {
    "objectID": "slides/week-5.1.html#example-code",
    "href": "slides/week-5.1.html#example-code",
    "title": "Understanding Data",
    "section": "Example Code",
    "text": "Example Code\n\n\nlibrary(googlesheets4)\n\n# Deauthorize to access public sheets without credentials\ngs4_deauth()\n\n# Read in the gapminder Africa data\ngapminder_data &lt;- read_sheet(\"1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY\")"
  },
  {
    "objectID": "slides/week-5.1.html#or",
    "href": "slides/week-5.1.html#or",
    "title": "Understanding Data",
    "section": "Or…",
    "text": "Or…\n\n\nlibrary(googlesheets4)\n\n# Deauthorize to access public sheets without credentials\ngs4_deauth()\n\n# Read in the gapminder Africa data\ngapminder_data &lt;- read_sheet(\"1U6Cf_qEOhiR9AZqTqS3mbMF3zt2db48ZP5v3rkrAEJY\")"
  },
  {
    "objectID": "slides/week-5.1.html#or-1",
    "href": "slides/week-5.1.html#or-1",
    "title": "Understanding Data",
    "section": "Or…",
    "text": "Or…\n\n\nlibrary(googlesheets4)\n\n# Deauthorize to access public sheets without credentials\ngs4_deauth()\n\n# Read in the gapminder Africa data\ngapminder_data &lt;- googledrive::drive_get(\"gapminder\") |&gt;\n  read_sheet()"
  },
  {
    "objectID": "slides/week-5.1.html#try-it-yourself-1",
    "href": "slides/week-5.1.html#try-it-yourself-1",
    "title": "Understanding Data",
    "section": "Try It Yourself!",
    "text": "Try It Yourself!\n\n\nUse the code above to read in the data\nTry reading in Gapminder data for a different country\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-5.1.html#find-your-own-data",
    "href": "slides/week-5.1.html#find-your-own-data",
    "title": "Understanding Data",
    "section": "Find Your Own Data",
    "text": "Find Your Own Data\n\n\nVisit kaggle.com\nFind a dataset you like\nDownload it as a CSV\nUpload to your Posit Cloud project\nRead it into R\nExplore with glimpse() and View()\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/week-12.2.html#jobs-training-programs",
    "href": "slides/week-12.2.html#jobs-training-programs",
    "title": "Hypothesis Testing 1",
    "section": "Jobs Training Programs",
    "text": "Jobs Training Programs\n\nInternational development organizations are sometimes interested in providing training to people in order to help them find a job\nImagine the unemployment rate in a low-income country is 30%\nOne organization claims that its jobs training program is a success because only 15 of the 60 people that they trained did not have a job (25% unemployment rate)\nWhat should we think about this claim? Is this a successful program?"
  },
  {
    "objectID": "slides/week-12.2.html#section",
    "href": "slides/week-12.2.html#section",
    "title": "Hypothesis Testing 1",
    "section": "",
    "text": "Now let’s create some data to match our hypothetical example.\n\n\nlibrary(tidyverse)\n\njobs_program &lt;- tibble(\n  outcome = c(rep(\"unemployed\", 15), rep(\"employed\", 45))\n)"
  },
  {
    "objectID": "slides/week-12.2.html#section-1",
    "href": "slides/week-12.2.html#section-1",
    "title": "Hypothesis Testing 1",
    "section": "",
    "text": "Use the base R head() function to see the first five rows.\n\nhead(jobs_program)\n\n# A tibble: 6 × 1\n  outcome   \n  &lt;chr&gt;     \n1 unemployed\n2 unemployed\n3 unemployed\n4 unemployed\n5 unemployed\n6 unemployed"
  },
  {
    "objectID": "slides/week-12.2.html#section-2",
    "href": "slides/week-12.2.html#section-2",
    "title": "Hypothesis Testing 1",
    "section": "",
    "text": "Use the tail() function to see the last five rows.\n\ntail(jobs_program)\n\n# A tibble: 6 × 1\n  outcome \n  &lt;chr&gt;   \n1 employed\n2 employed\n3 employed\n4 employed\n5 employed\n6 employed"
  },
  {
    "objectID": "slides/week-12.2.html#section-3",
    "href": "slides/week-12.2.html#section-3",
    "title": "Hypothesis Testing 1",
    "section": "",
    "text": "Now let’s visualize it with a bar chart."
  },
  {
    "objectID": "slides/week-12.2.html#question",
    "href": "slides/week-12.2.html#question",
    "title": "Hypothesis Testing 1",
    "section": "Question",
    "text": "Question\n\nIs it possible to assess this hypothetical organization’s claim using the data and information presented thus far?\n\n“Our jobs program is a success because only 15 of the 60 people that we trained did not have a job. Thus our 25% unemployment rate beats the country’s unemployment rate of 30%.”"
  },
  {
    "objectID": "slides/week-12.2.html#correlation-vs.-causation",
    "href": "slides/week-12.2.html#correlation-vs.-causation",
    "title": "Hypothesis Testing 1",
    "section": "Correlation vs. causation",
    "text": "Correlation vs. causation\n\n\nNo.\nWe need to know more about how people were selected for the program in order to assess causality (e.g., were they randomly assigned?)\nBut, we can still ask whether the unemployment rate of \\(\\hat{p}\\) = 0.25 could be due to chance."
  },
  {
    "objectID": "slides/week-12.2.html#hypothesis-testing-intuition",
    "href": "slides/week-12.2.html#hypothesis-testing-intuition",
    "title": "Hypothesis Testing 1",
    "section": "Hypothesis Testing Intuition",
    "text": "Hypothesis Testing Intuition\n\n\nWe are going to assume “nothing is going on”\n\nIn this case, the jobs program had no impact\n\n\n\n\nWe are going to figure out what the distribution of outcomes we we might observe could be if nothing is going on\n\nIn this case: if we take a sample of 60 from a population where the parameter is 0.3\n\n\n\n\n\nWe will assess how likely we would be to observe our data if nothing is going on\n\nIf very unlikely, we conclude that something is probably going on"
  },
  {
    "objectID": "slides/week-12.2.html#stating-our-hypotheses",
    "href": "slides/week-12.2.html#stating-our-hypotheses",
    "title": "Hypothesis Testing 1",
    "section": "Stating our Hypotheses",
    "text": "Stating our Hypotheses\n\nNull hypothesis (\\(H_0\\)): “There is nothing going on.”\n\nUnemployment rate among those in the jobs program is no different than the country average of 30%.\n\n\nAlternative hypothesis (\\(H_A\\)): “There is something going on.”\n\nUnemployment rate is lower than the country average of 30%."
  },
  {
    "objectID": "slides/week-12.2.html#hypothesis-test",
    "href": "slides/week-12.2.html#hypothesis-test",
    "title": "Hypothesis Testing 1",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\n\n\nHypothesis test: If the null hypothesis were true, is the data we have in our sample likely to have been generated by chance (due to random variability)?\nIf yes, we do NOT reject the null hypothesis\nIf not very likely, we reject the null hypothesis"
  },
  {
    "objectID": "slides/week-12.2.html#hypothesis-testing-framework",
    "href": "slides/week-12.2.html#hypothesis-testing-framework",
    "title": "Hypothesis Testing 1",
    "section": "Hypothesis Testing Framework",
    "text": "Hypothesis Testing Framework\n\nStart with null hypothesis, \\(H_0\\), represents the status quo\nSet an alternative hypothesis, \\(H_A\\), that represents the research question, i.e. what we’re testing for\nConduct a hypothesis test under the assumption that the null hypothesis is true\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "slides/week-12.2.html#p-values-and-critical-values",
    "href": "slides/week-12.2.html#p-values-and-critical-values",
    "title": "Hypothesis Testing 1",
    "section": "p-values and Critical Values",
    "text": "p-values and Critical Values\n\n\nA p-value is the probability of observed or more extreme outcome given that the null hypothesis is true\nA critical value (\\(\\alpha\\)) is the threshold at which we will reject the null hypothesis\nIf the p-value is less than \\(\\alpha\\), we reject the null hypothesis\nA standard threshold for \\(\\alpha\\) is 0.05"
  },
  {
    "objectID": "slides/week-12.2.html#the-null-distribution",
    "href": "slides/week-12.2.html#the-null-distribution",
    "title": "Hypothesis Testing 1",
    "section": "The Null Distribution",
    "text": "The Null Distribution\n\n\nSince \\(H_0: p = 0.30\\), we need to simulate a null distribution where the probability of success (unemployment) for each trial (person in program) is 0.30\n\n\n\nWe want to know how likely we would be to get an unemployment rate of 0.25 in our sample of 60, if the true unemployment rate were 0.30"
  },
  {
    "objectID": "slides/week-12.2.html#what-do-we-expect",
    "href": "slides/week-12.2.html#what-do-we-expect",
    "title": "Hypothesis Testing 1",
    "section": "What do we expect?",
    "text": "What do we expect?\n\n\nSo the first step is to simulate our null distribution\nAnd the question is, when sampling from the null distribution, what is the expected proportion of unemployed?\nWe set up our simulator to select samples of 60 individuals with a 30% chance of being unemployed\nWe then calculate the proportion of unemployed in each sample"
  },
  {
    "objectID": "slides/week-12.2.html#simulation-1",
    "href": "slides/week-12.2.html#simulation-1",
    "title": "Hypothesis Testing 1",
    "section": "Simulation #1",
    "text": "Simulation #1\n\n\nsim1\n  employed unemployed \n        42         18 \n\n\n[1] 0.3"
  },
  {
    "objectID": "slides/week-12.2.html#simulation-2",
    "href": "slides/week-12.2.html#simulation-2",
    "title": "Hypothesis Testing 1",
    "section": "Simulation #2",
    "text": "Simulation #2\n\n\nsim2\n  employed unemployed \n        41         19 \n\n\n[1] 0.3166667"
  },
  {
    "objectID": "slides/week-12.2.html#simulation-3",
    "href": "slides/week-12.2.html#simulation-3",
    "title": "Hypothesis Testing 1",
    "section": "Simulation #3",
    "text": "Simulation #3\n\n\nsim3\n  employed unemployed \n        38         22 \n\n\n[1] 0.3666667"
  },
  {
    "objectID": "slides/week-12.2.html#we-need-to-do-this-many-times",
    "href": "slides/week-12.2.html#we-need-to-do-this-many-times",
    "title": "Hypothesis Testing 1",
    "section": "We need to do this many times…",
    "text": "We need to do this many times…"
  },
  {
    "objectID": "slides/week-12.2.html#tidymodels",
    "href": "slides/week-12.2.html#tidymodels",
    "title": "Hypothesis Testing 1",
    "section": "tidymodels",
    "text": "tidymodels\n\nWe can use the tidymodels package to help with this process…\n\n#load tidymodels\nlibrary(tidymodels)\n\n# simulate the distribution\nnull_dist &lt;- jobs_program |&gt;\n  specify(response = outcome, success = \"unemployed\") |&gt;\n  hypothesize(null = \"point\", p = c(\"unemployed\" = 0.30, \"employed\" = 0.70)) |&gt;\n  generate(reps = 2000, type = \"draw\") |&gt; \n  calculate(stat = \"prop\")"
  },
  {
    "objectID": "slides/week-12.2.html#what-is-being-stored-in-null_dist",
    "href": "slides/week-12.2.html#what-is-being-stored-in-null_dist",
    "title": "Hypothesis Testing 1",
    "section": "What is being stored in null_dist?",
    "text": "What is being stored in null_dist?\n\n\n\nResponse: outcome (factor)\nNull Hypothesis: point\n# A tibble: 2,000 × 2\n   replicate  stat\n       &lt;dbl&gt; &lt;dbl&gt;\n 1         1 0.367\n 2         2 0.2  \n 3         3 0.283\n 4         4 0.2  \n 5         5 0.4  \n 6         6 0.317\n 7         7 0.3  \n 8         8 0.333\n 9         9 0.283\n10        10 0.25 \n# ℹ 1,990 more rows"
  },
  {
    "objectID": "slides/week-12.2.html#the-null-distribution-1",
    "href": "slides/week-12.2.html#the-null-distribution-1",
    "title": "Hypothesis Testing 1",
    "section": "The null distribution",
    "text": "The null distribution\n\nWhere should this distribution be centered? Or, what should the mean be?\n\n\n\nnull_dist |&gt;\n  summarize(mean = mean(stat))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1 0.301"
  },
  {
    "objectID": "slides/week-12.2.html#visualizing-the-null-distribution",
    "href": "slides/week-12.2.html#visualizing-the-null-distribution",
    "title": "Hypothesis Testing 1",
    "section": "Visualizing the null distribution",
    "text": "Visualizing the null distribution"
  },
  {
    "objectID": "slides/week-12.2.html#calculate-the-p-value",
    "href": "slides/week-12.2.html#calculate-the-p-value",
    "title": "Hypothesis Testing 1",
    "section": "Calculate the p-value",
    "text": "Calculate the p-value\n p-value–in what % of the simulations was the simulated sample proportion at least as extreme as the observed sample proportion?\n\nnull_dist |&gt;\n  # select out the value in the null distribution that are less that 0.25\n  filter(stat &lt;= (15/60)) |&gt;\n  # calculate the proportion - (number less divided by all values in null_dist)\n  summarise(p_value = n()/nrow(null_dist))\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.238"
  },
  {
    "objectID": "slides/week-12.2.html#visualizing-the-p-value",
    "href": "slides/week-12.2.html#visualizing-the-p-value",
    "title": "Hypothesis Testing 1",
    "section": "Visualizing the p-value",
    "text": "Visualizing the p-value"
  },
  {
    "objectID": "slides/week-12.2.html#p-value-using-infer",
    "href": "slides/week-12.2.html#p-value-using-infer",
    "title": "Hypothesis Testing 1",
    "section": "p-value using infer",
    "text": "p-value using infer\n\n\nlibrary(infer)\n\n# calculate the p-value from null distribution\np_value &lt;- null_dist |&gt;\n  get_p_value(obs_stat = 15/60, direction = \"less\")\n\np_value\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.238"
  },
  {
    "objectID": "slides/week-12.2.html#visualing-the-p-value-using-infer",
    "href": "slides/week-12.2.html#visualing-the-p-value-using-infer",
    "title": "Hypothesis Testing 1",
    "section": "Visualing the p-value using infer",
    "text": "Visualing the p-value using infer\n\n\nvisualize(null_dist) +\n  shade_p_value(obs_stat = 15/60, direction = \"less\")"
  },
  {
    "objectID": "slides/week-12.2.html#significance-level",
    "href": "slides/week-12.2.html#significance-level",
    "title": "Hypothesis Testing 1",
    "section": "“Significance” level",
    "text": "“Significance” level\n\n\nConventionally, people use a p-value of 0.05 as a cutoff (“signifigance level”) for determining “statistical significance”\n\nThat is, whether the null hypothesis should be rejected\nThat is, whether the data we gathered is very unlikely to have been generated due to chance\n\nAlways remember that this is a convention\n\np=0.049 is under the cutoff, while p=0.051 is not: are these really different?\n\nWhen people report “statistically significant” results, they mean that the p-value from their analysis is less than 0.05"
  },
  {
    "objectID": "slides/week-12.2.html#our-hypothetical-study",
    "href": "slides/week-12.2.html#our-hypothetical-study",
    "title": "Hypothesis Testing 1",
    "section": "Our Hypothetical Study",
    "text": "Our Hypothetical Study\n\n\nOur finding: if the true unemployment rate were 30 percent and we draw samples of 60, about 23 percent of the time we will get an unemployment rate lower than the one among the participants in the program (simply due to random chance)\nWhat should we conclude?"
  },
  {
    "objectID": "slides/week-12.2.html#conclusion",
    "href": "slides/week-12.2.html#conclusion",
    "title": "Hypothesis Testing 1",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nWe do NOT reject the null hypothesis: the unemployment rate in the sample could likely have been due to chance"
  },
  {
    "objectID": "slides/week-12.2.html#your-turn",
    "href": "slides/week-12.2.html#your-turn",
    "title": "Hypothesis Testing 1",
    "section": "Your Turn!",
    "text": "Your Turn!\n\nWhat if the unemployment rate for the program was only 10%? - Would you reject the null hypothesis in this case?\n\nDemonstrate by calculating the p-value\n\nTry changing the true unemployment rate in the null distribution to 0.50 (50%)\n\nSimulate the null distribution\nWould you reject the null hypothesis if the observed unemployment rate was 23% in this case?"
  },
  {
    "objectID": "slides/week-12.1.html#sampling",
    "href": "slides/week-12.1.html#sampling",
    "title": "Sampling and Uncertainty",
    "section": "Sampling",
    "text": "Sampling\n\nSampling the act of selecting a subset of individuals, items, or data points from a larger population to estimate characteristics or metrics of the entire population\nVersus a census, which involves gathering information on every individual in the population\nWhy would you want to use a sample?"
  },
  {
    "objectID": "slides/week-12.1.html#what-proportion-of-all-milk-chocolate-mms-are-blue",
    "href": "slides/week-12.1.html#what-proportion-of-all-milk-chocolate-mms-are-blue",
    "title": "Sampling and Uncertainty",
    "section": "What proportion of all milk chocolate M&Ms are blue?",
    "text": "What proportion of all milk chocolate M&Ms are blue?\n\nM&Ms has a precise distribution of colors that it produces in its factories\nM&Ms are sorted into bags in factories in a fairly random process\nEach bag represents a sample from the full population of M&Ms"
  },
  {
    "objectID": "slides/week-12.1.html#activity",
    "href": "slides/week-12.1.html#activity",
    "title": "Sampling and Uncertainty",
    "section": "Activity",
    "text": "Activity\n\nGet in groups of 3. Each group will have 4-5 bags of M&Ms.\nKeep the contents of each bag separate, and do not eat (yet!)\nOpen up your first bag of M&Ms: calculate the proportion of the M&Ms that are blue. Write this down. What is your best guess (your estimate) for the proportion of all M&Ms that are blue?"
  },
  {
    "objectID": "slides/week-12.1.html#activity-1",
    "href": "slides/week-12.1.html#activity-1",
    "title": "Sampling and Uncertainty",
    "section": "Activity",
    "text": "Activity\n\nDo the same as above for the rest of your bags (you should have 4-5 estimates written down)\nDraw a histogram of your estimates (by hand)\nAdd your estimates to this Google Sheet\nAdd your estimates to the class histogram on the whiteboard"
  },
  {
    "objectID": "slides/week-12.1.html#lets-analyze-the-data",
    "href": "slides/week-12.1.html#lets-analyze-the-data",
    "title": "Sampling and Uncertainty",
    "section": "Let’s Analyze the Data",
    "text": "Let’s Analyze the Data\n\nWe will use the googlesheets4 package to pull the data into R so be sure to install it.\n\n#install.packages(\"googlesheets4\")\n\nlibrary(tidyverse)\nlibrary(googlesheets4)\n\ngs4_deauth() # to signify no authorization required\n\nmnm_data &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/136wGKZOnwOdo3O-4bUfF_TWKBCAWQUCyiVKSY4HyYAw/edit#gid=0\")\n\nglimpse(mnm_data)"
  },
  {
    "objectID": "slides/week-12.1.html#calculate-some-summary-stats",
    "href": "slides/week-12.1.html#calculate-some-summary-stats",
    "title": "Sampling and Uncertainty",
    "section": "Calculate Some Summary Stats",
    "text": "Calculate Some Summary Stats\n\n\nmnm_data |&gt;\n  summarize(\n    mean_blue = mean(proportion_blue),\n    median_blue = median(proportion_blue),\n    sd_blue = sd(proportion_blue)\n  )"
  },
  {
    "objectID": "slides/week-12.1.html#now-lets-make-a-histogram",
    "href": "slides/week-12.1.html#now-lets-make-a-histogram",
    "title": "Sampling and Uncertainty",
    "section": "Now Let’s Make a Histogram",
    "text": "Now Let’s Make a Histogram\n\n\nggplot(mnm_data, aes(x = proportion_blue)) +\n  geom_histogram(fill = \"steelblue\") +\n  labs(\n    title = \"Percentage of Blue M&Ms\",\n    x = \"Proportion Blue\",\n    y = \"Count\"\n  )"
  },
  {
    "objectID": "slides/week-12.1.html#discuss-with-neighbor",
    "href": "slides/week-12.1.html#discuss-with-neighbor",
    "title": "Sampling and Uncertainty",
    "section": "Discuss with Neighbor",
    "text": "Discuss with Neighbor\n\nWhat is the histogram/distribution showing?\nBased on the histogram on the board, what is your answer to the question of what proportion of all milk chocolate M&Ms are blue? Why do you give that answer?\nWhy do some bag of M&Ms have proportions of blues that are higher and lower than the number you gave above?\nHow do our estimates relate to the actual percentage of blue M&Ms manufactured (ask Google or ChatGPT)"
  },
  {
    "objectID": "slides/week-12.1.html#what-did-we-just-do",
    "href": "slides/week-12.1.html#what-did-we-just-do",
    "title": "Sampling and Uncertainty",
    "section": "What did we just do?",
    "text": "What did we just do?\n\nWe wanted to say something about the population of M&Ms\nThe parameter we care about is the proportion of M&Ms that are blue\nIt would be impossible to conduct a census and to calculate the parameter\nWe took a sample from the population and calculated a sample statistic\nstatistical inference: act of making a guess about a population using information from a sample"
  },
  {
    "objectID": "slides/week-12.1.html#what-did-we-just-do-1",
    "href": "slides/week-12.1.html#what-did-we-just-do-1",
    "title": "Sampling and Uncertainty",
    "section": "What did we just do?",
    "text": "What did we just do?\n\nWe completed this task many times\nThis produced a sampling distribution of our estimates\nThere is a distribution of estimates because of sampling variability\nDue to random chance, one estimate from one sample can differ from another\nThese are foundational ideas for statistical inference that we are going to keep building on"
  },
  {
    "objectID": "slides/week-12.1.html#target-population",
    "href": "slides/week-12.1.html#target-population",
    "title": "Sampling and Uncertainty",
    "section": "Target Population",
    "text": "Target Population\nIn data analysis, we are usually interested in saying something about a Target Population.\n\nWhat proportion of adult Russians support the war in Ukraine?\n\nTarget population: adult Russians (age 18+)\n\nHow many US college students check social media during their classes?\n\nTarget population: US college students\n\nWhat percentage of M&Ms are blue?\n\nTarget population: all of the M&Ms"
  },
  {
    "objectID": "slides/week-12.1.html#sample",
    "href": "slides/week-12.1.html#sample",
    "title": "Sampling and Uncertainty",
    "section": "Sample",
    "text": "Sample\n\nIn many instances, we have a Sample\n\nWe cannot talk to every Russian\nWe cannot talk to all college students\nWe cannot count all of the M&Ms"
  },
  {
    "objectID": "slides/week-12.1.html#parameters-vs-statistics",
    "href": "slides/week-12.1.html#parameters-vs-statistics",
    "title": "Sampling and Uncertainty",
    "section": "Parameters vs Statistics",
    "text": "Parameters vs Statistics\n\n\nThe parameter is the value of a calculation for the entire target population\nThe statistic is what we calculate on our sample\n\nWe calculate a statistic in order to say something about the parameter"
  },
  {
    "objectID": "slides/week-12.1.html#inference",
    "href": "slides/week-12.1.html#inference",
    "title": "Sampling and Uncertainty",
    "section": "Inference",
    "text": "Inference\n\n\nInference–The act of “making a guess” about some unknown\nStatistical inference–Making a good guess about a population from a sample\nCausal inference–Did X cause Y? [topic for later classes]"
  },
  {
    "objectID": "slides/week-12.1.html#section",
    "href": "slides/week-12.1.html#section",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "On December 19, 2014, the front page of Spanish national newspaper El País read “Catalan public opinion swings toward ‘no’ for independence, says survey”.1\n\n\n\n\n\n\n\n\n\nAlberto Cairo. The truthful art: Data, charts, and maps for communication. New Riders, 2016."
  },
  {
    "objectID": "slides/week-12.1.html#section-1",
    "href": "slides/week-12.1.html#section-1",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "The probability of the tiny difference between the ‘No’ and ‘Yes’ being just due to random chance is very high.1\n\n\n\n\n\n\n\n\n\nAlberto Cairo. “Uncertainty and Graphicacy”, 2017."
  },
  {
    "objectID": "slides/week-12.1.html#characterizing-uncertainty",
    "href": "slides/week-12.1.html#characterizing-uncertainty",
    "title": "Sampling and Uncertainty",
    "section": "Characterizing Uncertainty",
    "text": "Characterizing Uncertainty\n\n\nWe know from previous section that even unbiased procedures do not get the “right” answer every time\nWe also know that our estimates might vary from sample to sample due to random chance\nTherefore we want to report on our estimate and our level of uncertainty"
  },
  {
    "objectID": "slides/week-12.1.html#characterizing-uncertainty-1",
    "href": "slides/week-12.1.html#characterizing-uncertainty-1",
    "title": "Sampling and Uncertainty",
    "section": "Characterizing Uncertainty",
    "text": "Characterizing Uncertainty\n\n\nWith M&Ms, we knew the population parameter\nIn real life, we do not!\nWe want to generate an estimate and characterize our uncertainty with a range of possible estimates"
  },
  {
    "objectID": "slides/week-12.1.html#solution-create-a-confidence-interval",
    "href": "slides/week-12.1.html#solution-create-a-confidence-interval",
    "title": "Sampling and Uncertainty",
    "section": "Solution: Create a Confidence Interval",
    "text": "Solution: Create a Confidence Interval\n\n\nA plausible range of values for the population parameter is a confidence interval.\n\n\n\n95 percent confidence interval is standard\n\nWe are 95% confident that the parameter value falls within the range given by the confidence interval"
  },
  {
    "objectID": "slides/week-12.1.html#ways-to-estimate",
    "href": "slides/week-12.1.html#ways-to-estimate",
    "title": "Sampling and Uncertainty",
    "section": "Ways to Estimate",
    "text": "Ways to Estimate\n\n\nTake advantage of Central Limit Theorem to estimate using math\nUse simulation, bootstrapping"
  },
  {
    "objectID": "slides/week-12.1.html#with-math",
    "href": "slides/week-12.1.html#with-math",
    "title": "Sampling and Uncertainty",
    "section": "With Math…",
    "text": "With Math…\n\\[CI = \\bar{x} \\pm Z \\left( \\frac{\\sigma}{\\sqrt{n}} \\right)\\]\n\n\\(\\bar{x}\\) is the sample mean,\n\\(Z\\) is the Z-score corresponding to the desired level of confidence\n\\(\\sigma\\) is the population standard deviation, and\n\\(n\\) is the sample size"
  },
  {
    "objectID": "slides/week-12.1.html#section-2",
    "href": "slides/week-12.1.html#section-2",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "This part here represents the standard error:\n\\[\\left( \\frac{\\sigma}{\\sqrt{n}} \\right)\\]\n\nStandard deviation of the sampling distribution\nCharacterizes the spread of the sampling distribution\nThe bigger this is the bigger the CIs are going to be"
  },
  {
    "objectID": "slides/week-12.1.html#central-limit-theorem",
    "href": "slides/week-12.1.html#central-limit-theorem",
    "title": "Sampling and Uncertainty",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\\[CI = \\bar{x} \\pm Z \\left( \\frac{\\sigma}{\\sqrt{n}} \\right)\\]\n\nThis way of doing things depends on the Central Limit Theorem\nAs sample size gets bigger, the spread of the sampling distribution gets narrower\nThe shape of the sampling distributions becomes more normally distributed"
  },
  {
    "objectID": "slides/week-12.1.html#section-3",
    "href": "slides/week-12.1.html#section-3",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "\\[CI = \\bar{x} \\pm Z \\left( \\frac{\\sigma}{\\sqrt{n}} \\right)\\]\nThis is therefore a parametric method of calculating the CI. It depends on assumptions about the normality of the distribution."
  },
  {
    "objectID": "slides/week-12.1.html#bootstrapping",
    "href": "slides/week-12.1.html#bootstrapping",
    "title": "Sampling and Uncertainty",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\n\nPulling oneself up from their bootstraps …\nUse the data we have to estimate the sampling distribution\nWe call this the bootstrap distribution\nThis is a nonparametric method\nIt does not depend on assumptions about normality"
  },
  {
    "objectID": "slides/week-12.1.html#bootstrap-process",
    "href": "slides/week-12.1.html#bootstrap-process",
    "title": "Sampling and Uncertainty",
    "section": "Bootstrap Process",
    "text": "Bootstrap Process\n\n\nTake a bootstrap sample - a random sample taken with replacement from the original sample, of the same size as the original sample\n\n\n\nCalculate the bootstrap statistic - a statistic such as mean, median, proportion, slope, etc. computed on the bootstrap samples\n\n\n\n\nRepeat steps (1) and (2) many times to create a bootstrap distribution - a distribution of bootstrap statistics\n\n\n\n\nCalculate the bounds of the XX% confidence interval as the middle XX% of the bootstrap distribution (usually 95 percent confidence interval)"
  },
  {
    "objectID": "slides/week-12.1.html#russia",
    "href": "slides/week-12.1.html#russia",
    "title": "Sampling and Uncertainty",
    "section": "Russia",
    "text": "Russia\n\nWhat Proportion of Russians believe their country interfered in the 2016 presidential elections in the US?\n\nPew Research survey\n506 subjects\nData available in the openintro package"
  },
  {
    "objectID": "slides/week-12.1.html#section-4",
    "href": "slides/week-12.1.html#section-4",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "For this example, we will use data from the Open Intro package. Install that package before running this code chunk.\n\n\n#install.packages(\"openintro\")\nlibrary(openintro)\n\nglimpse(russian_influence_on_us_election_2016)\n\nRows: 506\nColumns: 1\n$ influence_2016 &lt;chr&gt; \"Did not try\", \"Did not try\", \"Did not try\", \"Don't kno…"
  },
  {
    "objectID": "slides/week-12.1.html#section-5",
    "href": "slides/week-12.1.html#section-5",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "Let’s use mutate() to recode the qualitative variable as a numeric one…\n\n\nrussiaData &lt;- russian_influence_on_us_election_2016 |&gt; \n  mutate(try_influence = ifelse(influence_2016 == \"Did try\", 1, 0))"
  },
  {
    "objectID": "slides/week-12.1.html#section-6",
    "href": "slides/week-12.1.html#section-6",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "Now let’s calculate the mean and standard deviation of the try_influence variable…\n\n\nrussiaData |&gt;\n  summarize( \n          mean = mean(try_influence),\n          sd = sd(try_influence)\n  )\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.150 0.358"
  },
  {
    "objectID": "slides/week-12.1.html#section-7",
    "href": "slides/week-12.1.html#section-7",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "And finally let’s draw a bar plot…\n\n\nggplot(russiaData, aes(x = try_influence)) +\n  geom_bar(fill = \"steelblue\", width = .75) +\n  labs(\n    title = \"Did Russia try to influence the U.S. election?\",\n    x = \"0 = 'No', 1 = 'Yes'\",\n    y = \"Frequncy\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-12.1.html#bootstrap-with-tidymodels",
    "href": "slides/week-12.1.html#bootstrap-with-tidymodels",
    "title": "Sampling and Uncertainty",
    "section": "Bootstrap with tidymodels",
    "text": "Bootstrap with tidymodels\nInstall tidymodels before running this code chunk…\n\n#install.packages(\"tidymodels\")\nlibrary(tidymodels)\n\nset.seed(66)\nboot_dist &lt;- russiaData |&gt;\n  # specify the variable of interest\n  specify(response = try_influence) |&gt;\n  # generate 10000 bootstrap samples\n  generate(reps = 10000, type = \"bootstrap\") |&gt;\n  # calculate the mean of each bootstrap sample\n  calculate(stat = \"mean\")\n\nglimpse(boot_dist)\n\nRows: 10,000\nColumns: 2\n$ replicate &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ stat      &lt;dbl&gt; 0.1146245, 0.1442688, 0.1343874, 0.1877470, 0.1521739, 0.138…"
  },
  {
    "objectID": "slides/week-12.1.html#section-9",
    "href": "slides/week-12.1.html#section-9",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "Calculate the mean of the bootstrap distribution (of the means of the individual draws)…\n\n\nboot_dist |&gt; summarize(mean = mean(stat))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1 0.150"
  },
  {
    "objectID": "slides/week-12.1.html#section-10",
    "href": "slides/week-12.1.html#section-10",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "Calculate the confidence interval. A 95% confidence interval is bounded by the middle 95% of the bootstrap distribution.\n\n\nboot_dist |&gt;\n  summarize(lower = quantile(stat, 0.025),\n            upper = quantile(stat, 0.975))\n\n# A tibble: 1 × 2\n  lower upper\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.119 0.182"
  },
  {
    "objectID": "slides/week-12.1.html#section-11",
    "href": "slides/week-12.1.html#section-11",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "Create upper and lower bounds for visualization.\n\n\n# for using these values later\nlower_bound &lt;- boot_dist |&gt; summarize(lower_bound = quantile(stat, 0.025)) |&gt; pull() \nupper_bound &lt;- boot_dist |&gt; summarize(upper_bound = quantile(stat, 0.975)) |&gt; pull()"
  },
  {
    "objectID": "slides/week-12.1.html#section-12",
    "href": "slides/week-12.1.html#section-12",
    "title": "Sampling and Uncertainty",
    "section": "",
    "text": "Visualize with a histogram\n\n\nggplot(data = boot_dist, mapping = aes(x = stat)) +\n  geom_histogram(binwidth =.01, fill = \"steelblue4\") +\n  geom_vline(xintercept = c(lower_bound, upper_bound), color = \"darkgrey\", size = 1, linetype = \"dashed\") +\n  labs(title = \"Bootstrap distribution of means\",\n       subtitle = \"and 95% confidence interval\",\n       x = \"Estimate\",\n       y = \"Frequency\") +\n  theme_bw()"
  },
  {
    "objectID": "slides/week-12.1.html#or-use-the-infer-package",
    "href": "slides/week-12.1.html#or-use-the-infer-package",
    "title": "Sampling and Uncertainty",
    "section": "Or use the infer package",
    "text": "Or use the infer package\n\nci &lt;- boot_dist |&gt; get_ci(level = 0.95) \n\nci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.119    0.182"
  },
  {
    "objectID": "slides/week-12.1.html#or-use-the-infer-package-1",
    "href": "slides/week-12.1.html#or-use-the-infer-package-1",
    "title": "Sampling and Uncertainty",
    "section": "Or use the infer package",
    "text": "Or use the infer package\n\nboot_dist |&gt;\n  visualize() +\n  shade_ci(ci, color = \"red\", fill = NULL) +\n  labs( \n    title = \"Distribution of the Means of the Bootstrap Samples\",\n    x = \"Mean\",\n    y = \"Count\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-12.1.html#or-use-the-infer-package-2",
    "href": "slides/week-12.1.html#or-use-the-infer-package-2",
    "title": "Sampling and Uncertainty",
    "section": "Or use the infer package",
    "text": "Or use the infer package\n\nboot_dist |&gt;\n  visualize() +\n  shade_ci(ci, color = \"red\", fill = NULL) +\n  labs( \n    title = \"Distribution of the Means of the Bootstrap Samples\",\n    x = \"Mean\",\n    y = \"Count\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "slides/week-12.1.html#interpret-the-confidence-interval",
    "href": "slides/week-12.1.html#interpret-the-confidence-interval",
    "title": "Sampling and Uncertainty",
    "section": "Interpret the confidence interval",
    "text": "Interpret the confidence interval\n\nThe 95% confidence interval was calculated as (lower_bound, upper_bound). Which of the following is the correct interpretation of this interval?\n\n(a) 95% of the time the percentage of Russian who believe that Russia interfered in the 2016 US elections is between lower_bound and upper_bound.\n(b) 95% of all Russians believe that the chance Russia interfered in the 2016 US elections is between lower_bound and upper_bound.\n(c) We are 95% confident that the proportion of Russians who believe that Russia interfered in the 2016 US election is between lower_bound and upper_bound.\n(d) We are 95% confident that the proportion of Russians who supported interfering in the 2016 US elections is between lower_bound and upper_bound."
  },
  {
    "objectID": "slides/week-12.1.html#your-turn",
    "href": "slides/week-12.1.html#your-turn",
    "title": "Sampling and Uncertainty",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nChange the reps argument in the generate() function to something way smaller, like 100. What happens to your estimates?\nTry progressively higher numbers. What happens to your estimates as the number of reps increases?"
  },
  {
    "objectID": "slides/week-12.1.html#why-did-we-do-these-simulations",
    "href": "slides/week-12.1.html#why-did-we-do-these-simulations",
    "title": "Sampling and Uncertainty",
    "section": "Why did we do these simulations?",
    "text": "Why did we do these simulations?\n\n\nThey provide a foundation for statistical inference and for characterizing uncertainty in our estimates\nThe best research designs will try to maximize or achieve good balance on bias vs precision"
  },
  {
    "objectID": "slides/week-6.2.html#horizontal-join-merge",
    "href": "slides/week-6.2.html#horizontal-join-merge",
    "title": "Merging Data Frames",
    "section": "Horizontal Join (Merge)",
    "text": "Horizontal Join (Merge)\n\n\nOften we have data from two different sources\nResults in two data frames\nHow to make them one so we can analyze?"
  },
  {
    "objectID": "slides/week-6.2.html#illustration",
    "href": "slides/week-6.2.html#illustration",
    "title": "Merging Data Frames",
    "section": "Illustration",
    "text": "Illustration\n\nSource: R for HR"
  },
  {
    "objectID": "slides/week-6.2.html#types-of-joins-in-dplyr",
    "href": "slides/week-6.2.html#types-of-joins-in-dplyr",
    "title": "Merging Data Frames",
    "section": "Types of Joins in dplyr",
    "text": "Types of Joins in dplyr\n\nMutating versus filtering joins\nFour types of mutating joins\n\ninner_join()\nfull_join()\nleft_join()\nright_join()\n\nFor the most part we will use left_join()"
  },
  {
    "objectID": "slides/week-6.2.html#inner_join",
    "href": "slides/week-6.2.html#inner_join",
    "title": "Merging Data Frames",
    "section": "inner_join()",
    "text": "inner_join()\n\n\nSource: R for HR"
  },
  {
    "objectID": "slides/week-6.2.html#full_join",
    "href": "slides/week-6.2.html#full_join",
    "title": "Merging Data Frames",
    "section": "full_join()",
    "text": "full_join()\n\n\nSource: R for HR"
  },
  {
    "objectID": "slides/week-6.2.html#left_join",
    "href": "slides/week-6.2.html#left_join",
    "title": "Merging Data Frames",
    "section": "left_join()",
    "text": "left_join()\n\n\nSource: R for HR"
  },
  {
    "objectID": "slides/week-6.2.html#right_join",
    "href": "slides/week-6.2.html#right_join",
    "title": "Merging Data Frames",
    "section": "right_join()",
    "text": "right_join()\n\n\nSource: R for HR"
  },
  {
    "objectID": "slides/week-6.2.html#scenario",
    "href": "slides/week-6.2.html#scenario",
    "title": "Merging Data Frames",
    "section": "Scenario",
    "text": "Scenario\n\n\nWe want to merge two data frames\nOne is from the World Bank\nThe other is from V-Dem\nHow do we do it?"
  },
  {
    "objectID": "slides/week-6.2.html#grab-some-wb-data",
    "href": "slides/week-6.2.html#grab-some-wb-data",
    "title": "Merging Data Frames",
    "section": "Grab Some WB Data",
    "text": "Grab Some WB Data\n\n\n# Load packages\nlibrary(wbstats) \nlibrary(dplyr) \nlibrary(janitor) \n\n# Store the list of indicators in an object\nindicators &lt;- c(\"flfp\" = \"SL.TLF.CACT.FE.ZS\", \"women_rep\" = \"SG.GEN.PARL.ZS\") \n\n# Download the data  \nwb_dta &lt;- wb_data(indicators, mrv = 25) |&gt; # most recent 25 years\n  select(!iso2c) |&gt; \n  rename(year = date) |&gt; \n  mutate(\n    flfp = round_to_fraction(flfp, denominator = 100), # round to nearest 100th\n    women_rep = round_to_fraction(women_rep, denominator = 100) \n  )\n\n# View the data\nglimpse(women_emp)"
  },
  {
    "objectID": "slides/week-6.2.html#grab-some-v-dem-data",
    "href": "slides/week-6.2.html#grab-some-v-dem-data",
    "title": "Merging Data Frames",
    "section": "Grab Some V-Dem Data",
    "text": "Grab Some V-Dem Data\n\n\nlibrary(vdemlite)\n\nvdem_dta &lt;- fetchdem(indicators = c(\"v2x_gender\", \"v2x_gencl\", \"e_regionpol_6C\") |&gt;\n                         start_year = 2000, end_year = 2020) |&gt; # 20 year span\n                 rename(\n                   women_polemp = v2x_gender,\n                   women_civlib = v2x_gencl,\n                   region = e_regionpol_6C\n                 ) |&gt;\n            mutate(\n              region = case_match(region, \n                                  1 ~ \"Eastern Europe\", \n                                  2 ~ \"Latin America\",  \n                                  3 ~ \"Middle East\",   \n                                  4 ~ \"Africa\", \n                                  5 ~ \"The West\", \n                                  6 ~ \"Asia\")\n              )\n\nglimpse(vdem_dta)"
  },
  {
    "objectID": "slides/week-6.2.html#key-questions",
    "href": "slides/week-6.2.html#key-questions",
    "title": "Merging Data Frames",
    "section": "Key Questions",
    "text": "Key Questions\n\n\nWhat is the unit of analysis?\nWhat is/are the corresponding identifier variables?\nAre the identifier variables in common?\nOr do they have to be added/transformed to match?"
  },
  {
    "objectID": "slides/week-6.2.html#merging-wb-and-v-dem-data",
    "href": "slides/week-6.2.html#merging-wb-and-v-dem-data",
    "title": "Merging Data Frames",
    "section": "Merging WB and V-Dem Data",
    "text": "Merging WB and V-Dem Data\n\n\nThese are both time-series, country-level data\nNeed to merge by country-year\nYear is easy\nBut there are many different country codes\nCan use countrycode package to assign country codes"
  },
  {
    "objectID": "slides/week-6.2.html#use-countrycode",
    "href": "slides/week-6.2.html#use-countrycode",
    "title": "Merging Data Frames",
    "section": "Use countrycode",
    "text": "Use countrycode\n\n\n# Load countrycode\nlibrary(countrycode)\n\n# Create new iso3c variable\nvdem_data &lt;- vdem_data |&gt;    \n  mutate(iso3c = countrycode(sourcevar = country_id, # what we are converting\n        origin = \"vdem\",         # we are converting from vdem\n        destination = \"wb\"))  |&gt; # and converting to the WB iso3c code \n  relocate(iso3c, .after = country_id) # move iso3c \n\n# View the data\nglimpse(dem_data)"
  },
  {
    "objectID": "slides/week-6.2.html#try-it-yourself",
    "href": "slides/week-6.2.html#try-it-yourself",
    "title": "Merging Data Frames",
    "section": "Try it Yourself",
    "text": "Try it Yourself\n\n\nUsing your democracy data frame from the last lesson\nUse mutate() and countrycode() to add iso3c country codes\nUse relocate to move your iso3c code to the “front” of your data frame (optional)\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-6.2.html#use-left_join-to-merge",
    "href": "slides/week-6.2.html#use-left_join-to-merge",
    "title": "Merging Data Frames",
    "section": "Use left_join() to Merge",
    "text": "Use left_join() to Merge\n\n\n# Perform left join using common iso3c variable and year\ndem_wb &lt;- left_join(vdem_dta, wb_dta, by = c(\"iso3c\", \"year\")) |&gt; #join\n  select(-c(country_text_id, country_id, country)) # drop extra country variables\n  \n# View the data\nglimpse(dem_wb)"
  },
  {
    "objectID": "slides/week-6.2.html#try-it-yourself-1",
    "href": "slides/week-6.2.html#try-it-yourself-1",
    "title": "Merging Data Frames",
    "section": "Try it Yourself",
    "text": "Try it Yourself\n\nTake your V-Dem data frame and your World Bank data frame\nUsing left_join() to merge on country code and year\nAlong the way, use rename() and select() to insure you have just one country name\nTry inner_join(), full_join(), and right_join() as time allows\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-6.2.html#summarize-the-data",
    "href": "slides/week-6.2.html#summarize-the-data",
    "title": "Merging Data Frames",
    "section": "Summarize the Data",
    "text": "Summarize the Data\n\n\nDo a group, summarize, arrange sequence on your merged data frame\nGroup and summarize by country (mean or median)\nTry using across() to summarize multiple columns at once\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-6.2.html#create-a-scatter-plot",
    "href": "slides/week-6.2.html#create-a-scatter-plot",
    "title": "Merging Data Frames",
    "section": "Create a Scatter Plot",
    "text": "Create a Scatter Plot\n\n\nNow you have one data point per country\nUse ggplot2 to create a scatter plot\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-8.2.html#your-turn",
    "href": "slides/week-8.2.html#your-turn",
    "title": "Regression Practice",
    "section": "Your Turn",
    "text": "Your Turn\n\nAre democracies less corrupt?\n\n\nV-Dem includes a Political Corruption Index, which aggregates corruption in a number of spheres (see codebook for details).\nThe variable name is: v2x_corr : lower values mean less corruption"
  },
  {
    "objectID": "slides/week-8.2.html#your-turn-1",
    "href": "slides/week-8.2.html#your-turn-1",
    "title": "Regression Practice",
    "section": "Your Turn",
    "text": "Your Turn\n\nAre democracies less corrupt?\n\n\n\nFilter the V-Dem data to only include the year 2019\nMake a scatterplot to visualize the relationship between democracy (X) and corruption (Y) (use the v2x_libdem variable for democracy)\nFit a linear model with lm()\nInterpret results for the slope and intercept\nFor a country with the average (mean) level of democracy, what is the predicted level of corruption?\n\n\n\n\n\n−+\n15:00"
  },
  {
    "objectID": "slides/week-8.2.html#create-your-own-model",
    "href": "slides/week-8.2.html#create-your-own-model",
    "title": "Regression Practice",
    "section": "Create Your Own Model",
    "text": "Create Your Own Model\n\n\nWhat is a theory of democracy you could test with V-Dem?\nWhat is the dependent variable?\nWhat is the independent variable?\nMap out steps to wrangle the data and fit a regression model\nWhat do you expect to find?\nNow go ahead and wrangle the data\nFit the model\nInterpret the coefficients and their significance\nDid the results match your expectations?\n\n\n\n\n−+\n15:00"
  },
  {
    "objectID": "slides/week-8.2.html#final-project",
    "href": "slides/week-8.2.html#final-project",
    "title": "Regression Practice",
    "section": "Final Project",
    "text": "Final Project\n\n\nProject assignment 1 due this Sunday\nCan access here\nLet’s get started!"
  },
  {
    "objectID": "slides/week-9.1.html#load-packages",
    "href": "slides/week-9.1.html#load-packages",
    "title": "Multiple Regression",
    "section": "Load packages",
    "text": "Load packages\n\n\nlibrary(tidyverse)\nlibrary(vdemdata)"
  },
  {
    "objectID": "slides/week-9.1.html#load-vdem-data",
    "href": "slides/week-9.1.html#load-vdem-data",
    "title": "Multiple Regression",
    "section": "Load VDEM Data",
    "text": "Load VDEM Data\n\n\nmodel_data &lt;- vdem |&gt;\n  filter(year == 2006) |&gt; \n  select(country_name, \n         libdem = v2x_libdem, \n         wealth = e_gdppc, \n         oil_rents = e_total_oil_income_pc,\n         polarization = v2cacamps, \n         corruption = v2x_corr, \n         judicial_review = v2jureview_ord, \n         region = e_regionpol_6C, \n         regime = v2x_regime) |&gt; \n  mutate(\n         region = factor(\n           region,\n           labels=c(\"Eastern Europe\", \n             \"Latin America\", \n             \"MENA\", \n             \"SSAfrica\", \n             \"Western Europe and North America\", \n             \"Asia and Pacific\"))\n          )\n\nglimpse(model_data)"
  },
  {
    "objectID": "slides/week-9.1.html#linear-model-with-multiple-predictors",
    "href": "slides/week-9.1.html#linear-model-with-multiple-predictors",
    "title": "Multiple Regression",
    "section": "Linear Model with Multiple Predictors",
    "text": "Linear Model with Multiple Predictors\n\n\nPreviously, we were interested in GDP per capita as a predictor of democracy\nNow, let’s consider another predictor: polarization (also measured by V-Dem)"
  },
  {
    "objectID": "slides/week-9.1.html#polarization-measure-in-the-usa",
    "href": "slides/week-9.1.html#polarization-measure-in-the-usa",
    "title": "Multiple Regression",
    "section": "Polarization Measure in the USA",
    "text": "Polarization Measure in the USA"
  },
  {
    "objectID": "slides/week-9.1.html#polarization-and-democracy-in-2006",
    "href": "slides/week-9.1.html#polarization-and-democracy-in-2006",
    "title": "Multiple Regression",
    "section": "Polarization and Democracy in 2006",
    "text": "Polarization and Democracy in 2006"
  },
  {
    "objectID": "slides/week-9.1.html#model-with-one-predictor",
    "href": "slides/week-9.1.html#model-with-one-predictor",
    "title": "Multiple Regression",
    "section": "Model with One Predictor",
    "text": "Model with One Predictor\n\n\nlibrary(broom) # for tidy() function\n\nlm(libdem ~ polarization, data = model_data) |&gt; \n  tidy() # for nicer regression output\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.376     0.0187     20.1  2.66e-47\n2 polarization  -0.0914    0.0139     -6.58 5.30e-10\n\n\n\nHow do we interpret the intercept and the slope estimate?\nSignificance?"
  },
  {
    "objectID": "slides/week-9.1.html#model-with-two-predictors",
    "href": "slides/week-9.1.html#model-with-two-predictors",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\n\nlm(libdem ~ polarization + log(wealth), data = model_data) |&gt; \n  tidy()\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic       p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)    0.196     0.0340      5.76 0.0000000390 \n2 polarization  -0.0584    0.0138     -4.24 0.0000363    \n3 log(wealth)    0.0944    0.0151      6.24 0.00000000334"
  },
  {
    "objectID": "slides/week-9.1.html#model-with-two-predictors-1",
    "href": "slides/week-9.1.html#model-with-two-predictors-1",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\n\\[ \\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc \\]\n\\[ \\hat{Y_i} = 0.18 + -0.05*Polarization + 0.10*GDPpc \\]"
  },
  {
    "objectID": "slides/week-9.1.html#model-with-two-predictors-2",
    "href": "slides/week-9.1.html#model-with-two-predictors-2",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\n\\[ \\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc \\]\n\\[ \\hat{Y_i} = 0.18 + -0.05*Polarization + 0.10*GDPpc \\]\n\\(a\\) is the predicted level of Y when BOTH GDP per capita and polarization are equal to 0"
  },
  {
    "objectID": "slides/week-9.1.html#model-with-two-predictors-3",
    "href": "slides/week-9.1.html#model-with-two-predictors-3",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\n\\[ \\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc \\]\n\\[ \\hat{Y_i} = 0.18 + -0.05*Polarization + 0.10*GDPpc \\]\n\n\\(b_1\\) is the impact of a 1-unit change in polarization on the predicted level of Y, holding GDP per capita fixed (all else equal)\nThe relationship between polarization and democracy, controlling for wealth"
  },
  {
    "objectID": "slides/week-9.1.html#model-with-two-predictors-4",
    "href": "slides/week-9.1.html#model-with-two-predictors-4",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\\[ \\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc \\]\n\\[ \\hat{Y_i} = 0.18 + -0.05*Polarization + 0.10*GDPpc \\]\n\n\\(b_2\\) is the impact of a 1-unit change in GDP per capita on the predicted level of Y, holding polarization fixed (all else equal)\nThe relationship between wealth and democracy, controlling for polarization"
  },
  {
    "objectID": "slides/week-9.1.html#model-with-two-predictors-5",
    "href": "slides/week-9.1.html#model-with-two-predictors-5",
    "title": "Multiple Regression",
    "section": "Model with Two Predictors",
    "text": "Model with Two Predictors\n\n\\[ \\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc \\]\n\nOLS is searching for combination of \\(a\\), \\(b_1\\), and \\(b_2\\) that minimize sum of squared residuals\nSame logic as model with one predictor, just more complicated"
  },
  {
    "objectID": "slides/week-9.1.html#model-with-three-predictors",
    "href": "slides/week-9.1.html#model-with-three-predictors",
    "title": "Multiple Regression",
    "section": "Model with Three Predictors",
    "text": "Model with Three Predictors\n\n\\[ \\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc + b_3*OilRents \\]"
  },
  {
    "objectID": "slides/week-9.1.html#model-with-three-predictors-1",
    "href": "slides/week-9.1.html#model-with-three-predictors-1",
    "title": "Multiple Regression",
    "section": "Model with Three Predictors",
    "text": "Model with Three Predictors\n\n\nlm(libdem ~ polarization + log(wealth) + oil_rents, data = model_data) |&gt; \n  tidy()\n\n# A tibble: 4 × 5\n  term           estimate  std.error statistic  p.value\n  &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   0.153     0.0317          4.82 3.30e- 6\n2 polarization -0.0577    0.0128         -4.50 1.32e- 5\n3 log(wealth)   0.131     0.0150          8.72 3.65e-15\n4 oil_rents    -0.0000413 0.00000607     -6.80 1.98e-10"
  },
  {
    "objectID": "slides/week-9.1.html#model-with-three-predictors-2",
    "href": "slides/week-9.1.html#model-with-three-predictors-2",
    "title": "Multiple Regression",
    "section": "Model with Three Predictors",
    "text": "Model with Three Predictors\n\n\\[ \\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc + b_3*OilRents \\]\n\\[ \\hat{Y_i} = a + -.05*Polarization + .13*GDPpc + .00004*OilRents \\] \n\\(b_3\\) is the impact of a 1-unit change in oil revenues per capita on the predicted level of Y, holding polarization and GDP fixed (all else equal)"
  },
  {
    "objectID": "slides/week-9.1.html#your-turn",
    "href": "slides/week-9.1.html#your-turn",
    "title": "Multiple Regression",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\nIn the last session, you examined levels of democracy and corruption\nNow, let’s fit a multiple regression model predicting corruption with two predictors: democracy (libdem) and polarization (polarization)\nThen, interpret the coefficients\nEstimate a second multiple regression model that adds in GDP per capita (lg_gdppc)\nInterpret the coefficients\nWhat happens to the coefficients of the other variables when we add GDP per capita to the model?\n\nWhy do you think this happens?\n\nTry adding in additional predictors if there is time\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-9.1.html#model-1",
    "href": "slides/week-9.1.html#model-1",
    "title": "Multiple Regression",
    "section": "Model 1",
    "text": "Model 1\n\n\nlm(corruption ~ libdem + polarization , data = model_data) |&gt;\n      tidy() \n\n# A tibble: 3 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.853     0.0267     32.0  9.12e-75\n2 libdem        -0.780     0.0592    -13.2  6.16e-28\n3 polarization   0.0459    0.0122      3.78 2.19e- 4"
  },
  {
    "objectID": "slides/week-9.1.html#model-2---adding-gdp-per-capita",
    "href": "slides/week-9.1.html#model-2---adding-gdp-per-capita",
    "title": "Multiple Regression",
    "section": "Model 2 - Adding GDP per capita",
    "text": "Model 2 - Adding GDP per capita\n\n\nlm(corruption ~ libdem + polarization + log(wealth), data = model_data) |&gt;\n      tidy() \n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    0.962     0.0278     34.6  1.25e-78\n2 libdem        -0.618     0.0574    -10.8  6.40e-21\n3 polarization   0.0298    0.0108      2.75 6.66e- 3\n4 log(wealth)   -0.0864    0.0125     -6.89 1.07e-10"
  },
  {
    "objectID": "slides/week-9.1.html#what-is-a-categorical-variable",
    "href": "slides/week-9.1.html#what-is-a-categorical-variable",
    "title": "Multiple Regression",
    "section": "What is a Categorical Variable?",
    "text": "What is a Categorical Variable?\n\n\nA variable that represents distinct groups or categories\nCategories can be named (e.g., “Male” or “Female”)\nOr numbered (e.g., 1, 2, 3)\nOr words (like different regions)\nCan be either nominal (unordered) or ordinal (ordered)\nAlso called “dummy variables” or “indicator variables”"
  },
  {
    "objectID": "slides/week-9.1.html#what-is-a-factor-in-r",
    "href": "slides/week-9.1.html#what-is-a-factor-in-r",
    "title": "Multiple Regression",
    "section": "What is a Factor in R?",
    "text": "What is a Factor in R?\n\n\nA data structure used to represent categorical variables\nCan be dichotomous (binary) or have multiple levels\nSupports both ordered and unordered categories\nCall factor() to convert a variable to a factor\nCall levels() to see the categories"
  },
  {
    "objectID": "slides/week-9.1.html#reference-categories",
    "href": "slides/week-9.1.html#reference-categories",
    "title": "Multiple Regression",
    "section": "Reference Categories",
    "text": "Reference Categories\n\n\nIn a regression model, every categorical variable needs a reference category\nThe reference category is the category that all other categories are compared to when interpreting the results\nCall relevel() to change the reference category"
  },
  {
    "objectID": "slides/week-9.1.html#judicial-review-and-democracy",
    "href": "slides/week-9.1.html#judicial-review-and-democracy",
    "title": "Multiple Regression",
    "section": "Judicial Review and Democracy",
    "text": "Judicial Review and Democracy\n\nJudicial Review:\n\nDo high courts (Supreme Court, Constitutional Court, etc) have the power to rule on whether laws or policies are constitutional/legal? (Yes or No)\nDimension of Judicial Independence"
  },
  {
    "objectID": "slides/week-9.1.html#judicial-review-and-democracy-1",
    "href": "slides/week-9.1.html#judicial-review-and-democracy-1",
    "title": "Multiple Regression",
    "section": "Judicial Review and Democracy",
    "text": "Judicial Review and Democracy"
  },
  {
    "objectID": "slides/week-9.1.html#judicial-review-and-democracy-2",
    "href": "slides/week-9.1.html#judicial-review-and-democracy-2",
    "title": "Multiple Regression",
    "section": "Judicial Review and Democracy",
    "text": "Judicial Review and Democracy\n\n\nlm(libdem ~ factor(judicial_review), data = model_data) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term                     estimate std.error statistic    p.value\n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)                 0.179    0.0493      3.64 0.000355  \n2 factor(judicial_review)1    0.267    0.0533      5.01 0.00000130"
  },
  {
    "objectID": "slides/week-9.1.html#judicial-review-and-democracy-3",
    "href": "slides/week-9.1.html#judicial-review-and-democracy-3",
    "title": "Multiple Regression",
    "section": "Judicial Review and Democracy",
    "text": "Judicial Review and Democracy\n\n\\[ \\widehat{Democracy_{i}} = 0.17 + 0.28*JudicialReview(yes) \\]\n\nSlope: countries with judicial review are expected, on average, to be 0.28 units more democratic on the liberal democracy index\n\nCompares baseline level (Judicial Review = 0) to the other level (Judicial Review = 1)\n\nIntercept: average democracy score of countries without judicial review\nAverage democracy score of countries with judicial review is 0.17 + 0.28 = 0.45"
  },
  {
    "objectID": "slides/week-9.1.html#dummy-variables",
    "href": "slides/week-9.1.html#dummy-variables",
    "title": "Multiple Regression",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\n\nWhen the categorical explanatory variable has many levels, they’re encoded to dummy variables\nWe always leave one category out of the model, as the omitted reference category\nEach coefficient describes is the expected difference between level of the factor and the baseline level\nEverything is relative to the omitted reference category"
  },
  {
    "objectID": "slides/week-9.1.html#democracy-and-world-region",
    "href": "slides/week-9.1.html#democracy-and-world-region",
    "title": "Multiple Regression",
    "section": "Democracy and World Region",
    "text": "Democracy and World Region\n\n\nDoes region predict levels of democracy?\nSince Eastern Europe is the first category, default in R is to use that as the omitted category in models.\n\n\nlevels(model_data$region)\n\n[1] \"Eastern Europe\"                   \"Latin America\"                   \n[3] \"MENA\"                             \"SSAfrica\"                        \n[5] \"Western Europe and North America\" \"Asia and Pacific\""
  },
  {
    "objectID": "slides/week-9.1.html#democracy-and-world-region-1",
    "href": "slides/week-9.1.html#democracy-and-world-region-1",
    "title": "Multiple Regression",
    "section": "Democracy and World Region",
    "text": "Democracy and World Region\n\nHow should we interpret intercept? How about the coefficient on Latin America?\n\n\nlm(libdem ~ region, data = model_data) |&gt;\n  tidy()\n\n# A tibble: 6 × 5\n  term                                   estimate std.error statistic  p.value\n  &lt;chr&gt;                                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                              0.434     0.0361     12.0  1.41e-24\n2 regionLatin America                      0.0664    0.0535      1.24 2.16e- 1\n3 regionMENA                              -0.236     0.0571     -4.13 5.63e- 5\n4 regionSSAfrica                          -0.139     0.0456     -3.06 2.61e- 3\n5 regionWestern Europe and North America   0.376     0.0541      6.94 7.84e-11\n6 regionAsia and Pacific                  -0.134     0.0519     -2.57 1.09e- 2"
  },
  {
    "objectID": "slides/week-9.1.html#democracy-and-world-region-2",
    "href": "slides/week-9.1.html#democracy-and-world-region-2",
    "title": "Multiple Regression",
    "section": "Democracy and World Region",
    "text": "Democracy and World Region\n\nWhat if you want a different baseline category? How do we interpret now?\n\n\n# make SS Africa the reference category\nmodel_data &lt;- model_data |&gt; \nmutate(newReg = relevel(region, ref=4)) \n\nlm(libdem ~ newReg, data = model_data) |&gt;\n      tidy()\n\n# A tibble: 6 × 5\n  term                                   estimate std.error statistic  p.value\n  &lt;chr&gt;                                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                             0.295      0.0279    10.6   2.24e-20\n2 newRegEastern Europe                    0.139      0.0456     3.06  2.61e- 3\n3 newRegLatin America                     0.206      0.0484     4.25  3.47e- 5\n4 newRegMENA                             -0.0962     0.0523    -1.84  6.74e- 2\n5 newRegWestern Europe and North America  0.515      0.0491    10.5   3.36e-20\n6 newRegAsia and Pacific                  0.00582    0.0466     0.125 9.01e- 1"
  },
  {
    "objectID": "slides/week-9.1.html#your-turn-1",
    "href": "slides/week-9.1.html#your-turn-1",
    "title": "Multiple Regression",
    "section": "Your Turn",
    "text": "Your Turn\n\nWhich types of regime have more corruption?\n\nV-Dem also includes a categorial regime variable: Closed autocracy (0), Electoral Autocracy (1), Electoral Democracy (2), Liberal Democracy (3)"
  },
  {
    "objectID": "slides/week-9.1.html#your-turn-2",
    "href": "slides/week-9.1.html#your-turn-2",
    "title": "Multiple Regression",
    "section": "Your Turn",
    "text": "Your Turn\n\nWhich types of regime have more corruption?\n\nFirst, let’s make this an easier factor variable to work with.\n\n# Make nicer regime factor variable\nmodel_data &lt;- model_data |&gt; \n  mutate(regime = factor(regime,\n                         labels = c(\"Closed Autocracy\",\n                                   \"Electoral Autocracy\",\n                                  \"Electoral Democracy\",\n                                  \"Liberal Democracy\")))\nlevels(model_data$regime)\n\n[1] \"Closed Autocracy\"    \"Electoral Autocracy\" \"Electoral Democracy\"\n[4] \"Liberal Democracy\""
  },
  {
    "objectID": "slides/week-9.1.html#your-turn-3",
    "href": "slides/week-9.1.html#your-turn-3",
    "title": "Multiple Regression",
    "section": "Your Turn",
    "text": "Your Turn\n\nWhich types of regime have more corruption?\n\n\nFilter data to include only the year 2019 (or run the code to use model_data)\nMake a plot to visualize the relationship between regime type and level of corruption. - Which kind of plot is best in this situation?\nFit a linear model\nInterpret the intercept and coefficients\n\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week-9.1.html#visualization",
    "href": "slides/week-9.1.html#visualization",
    "title": "Multiple Regression",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "slides/week-9.1.html#model",
    "href": "slides/week-9.1.html#model",
    "title": "Multiple Regression",
    "section": "Model",
    "text": "Model\n\n\nlm(corruption ~ regime, data = model_data) |&gt;\n      tidy()\n\n# A tibble: 4 × 5\n  term                      estimate std.error statistic  p.value\n  &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                 0.598     0.0397     15.1  2.81e-33\n2 regimeElectoral Autocracy   0.151     0.0475      3.18 1.74e- 3\n3 regimeElectoral Democracy  -0.0606    0.0484     -1.25 2.11e- 1\n4 regimeLiberal Democracy    -0.469     0.0502     -9.35 4.46e-17"
  },
  {
    "objectID": "slides/week-9.1.html#create-your-own-model",
    "href": "slides/week-9.1.html#create-your-own-model",
    "title": "Multiple Regression",
    "section": "Create Your Own Model",
    "text": "Create Your Own Model\n\n\nWhat is a theory that you would like to test with V-Dem data?\nWhat is the dependent variable?\nWhat are the independent variables?\nMap out steps to wrangle the data and fit a regression model\nWhat do you expect to find?\nNow go ahead and wrangle the data\nFit the model\nInterpret the coefficients and their significance\nDid the results match your expectations?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site was built with Quarto and is based on a template provided by Mine Çetinkaya-Rundel under a Creative Commons Attribution-NonCommercial 4.0 Iicense."
  },
  {
    "objectID": "project/project-assignment-3.html",
    "href": "project/project-assignment-3.html",
    "title": "Project Assignment 3",
    "section": "",
    "text": "Due dates\n\n\n\nNote that the submission instructions and due date for this assignment have changed.\nInstead of giving a lightning talk, you are going to submit recorded presentations of 5–7 minutes in length.\nThe due date for this is the same as the original due date for the assignment (May 2nd at 11:59 p.m.).\nThe due date for the report is now the day of the scheduled final exam for this course (May 9th at 11:59 p.m.).",
    "crumbs": [
      "Project",
      "Assignment 3"
    ]
  },
  {
    "objectID": "project/project-assignment-3.html#overview",
    "href": "project/project-assignment-3.html#overview",
    "title": "Project Assignment 3",
    "section": "Overview",
    "text": "Overview\nFor this final assignment, you will synthesize the work completed in Parts 1 and 2 into a polished deliverable that communicates your findings effectively. This assignment has two components:\n\nA Quarto RevealJS Presentation: A lightning talk summarizing your project, findings, and policy implications.\nA Final Written Report: A detailed document that provides a comprehensive overview of your analysis, results, and recommendations.\n\nTogether, these components should demonstrate your ability to apply data analysis skills to answer a question relevant to decision-making in a real-world context.",
    "crumbs": [
      "Project",
      "Assignment 3"
    ]
  },
  {
    "objectID": "project/project-assignment-3.html#component-1-revealjs-presentation-due-may-2nd",
    "href": "project/project-assignment-3.html#component-1-revealjs-presentation-due-may-2nd",
    "title": "Project Assignment 3",
    "section": "Component 1: RevealJS Presentation (due May 2nd)",
    "text": "Component 1: RevealJS Presentation (due May 2nd)\nYou will create a 5-minute presentation using Quarto’s RevealJS slide deck format. The goal is to provide a concise and engaging summary of your project.\nRough outline for your presentation:\n\nSlide 1: Title slide with project title, group member names, and date.\nSlide 2: Introduction of your research question and its importance.\nSlide 3: Overview of your data, including the source and key variables.\nSlides 4–5: Key results, supported by 1–2 visuals (e.g., plots or tables).\nSlide 6: Summary of findings and policy implications.\n\nTips:\n\nUse visuals to make your presentation engaging and accessible.\nKeep text minimal—focus on high-level points and key insights.\nPractice your timing to stay within the 5-minute limit.\nYou can add a few more slides if needed, but only if it still allows you to stay within the time limit.\n\nSubmission Instructions:\n\nOn May 2nd, upload your RevealJS presentation to Quarto Pub and share the link on Discord.\nAlso by May 2nd, record your presentation using a screen recording tool (e.g., Zoom, Vimeo) and post the link to the recording to Discord or, if you decided to do the writeup/reflection, post that to Discord instead.\nSubmit the QMD file for the presentation to Blackboard on May 9, along with your full project folder.",
    "crumbs": [
      "Project",
      "Assignment 3"
    ]
  },
  {
    "objectID": "project/project-assignment-3.html#component-2-final-written-report-due-may-9th",
    "href": "project/project-assignment-3.html#component-2-final-written-report-due-may-9th",
    "title": "Project Assignment 3",
    "section": "Component 2: Final Written Report (due May 9th)",
    "text": "Component 2: Final Written Report (due May 9th)\nThe final report should be a 3–5 page policy memo or technical report that provides a detailed narrative of your project. It should build upon Parts 1 and 2, integrating your research question, hypotheses, analysis, and conclusions.\nYour report should include:\n\nIntroduction\n\nClearly state your research question and why it is important.\nProvide background context, citing relevant literature or examples.\n\nData and Methodology\n\nDescribe your data sources and key variables (outcome and explanatory).\nJustify your analytical approach, including the methods used (e.g., regression, hypothesis testing).\n\nAnalysis and Results\n\nPresent your findings, supported by tables, plots, or statistical results.\nInclude at least one visualization (e.g., regression plot, histogram).\n\nDiscussion\n\nInterpret your results in context of your research question and hypotheses.\nDiscuss the statistical significance and implications of your findings.\n\nPolicy Implications\n\nSummarize how your findings inform policy or decision-making.\nPropose actionable recommendations based on your analysis.\n\nConclusion\n\nProvide a clear answer to your research question.\nReflect briefly on limitations and next steps.\n\n\nSubmission Instructions:\n\nRender your report in HTML format and upload it to Quarto Pub.\nSubmit the link on Discord.\nUpload your group’s zipped project folder (QMD file and data) to Blackboard.",
    "crumbs": [
      "Project",
      "Assignment 3"
    ]
  },
  {
    "objectID": "project/project-assignment-1.html",
    "href": "project/project-assignment-1.html",
    "title": "Project Assignment 1",
    "section": "",
    "text": "For this project you will work in groups to use data to answer a question. This is your chance to use the skills you are developing in this class to help guide decision-making in a real-world context.\nCrucially, your data analysis should be question-driven. Part of the challenge is to identify the most appropriate data and analysis to answer your question. You will need to think about how to do this and the answer will not always be obvious–a level of creativity is involved.\nFor this first project assignment, you are to form groups of 3-4 students. You will select a question to work on, do some preliminary research on the type of data that might help you to answer the question, and discuss how you will use the data to provide an answer.",
    "crumbs": [
      "Project",
      "Assignment 1"
    ]
  },
  {
    "objectID": "project/project-assignment-1.html#overview",
    "href": "project/project-assignment-1.html#overview",
    "title": "Project Assignment 1",
    "section": "",
    "text": "For this project you will work in groups to use data to answer a question. This is your chance to use the skills you are developing in this class to help guide decision-making in a real-world context.\nCrucially, your data analysis should be question-driven. Part of the challenge is to identify the most appropriate data and analysis to answer your question. You will need to think about how to do this and the answer will not always be obvious–a level of creativity is involved.\nFor this first project assignment, you are to form groups of 3-4 students. You will select a question to work on, do some preliminary research on the type of data that might help you to answer the question, and discuss how you will use the data to provide an answer.",
    "crumbs": [
      "Project",
      "Assignment 1"
    ]
  },
  {
    "objectID": "project/project-assignment-1.html#groups",
    "href": "project/project-assignment-1.html#groups",
    "title": "Project Assignment 1",
    "section": "Groups",
    "text": "Groups\nYou will form groups of 3-4 students. You will work with this group for the duration of the project. Please list the names of your group members here:",
    "crumbs": [
      "Project",
      "Assignment 1"
    ]
  },
  {
    "objectID": "project/project-assignment-1.html#topics",
    "href": "project/project-assignment-1.html#topics",
    "title": "Project Assignment 1",
    "section": "Topics",
    "text": "Topics\nYou should select from the following list of topics. If you have another topic in mind, please discuss with me before proceeding. After choosing your topic, delete the other non-chosen topics and discuss briefly why you chose it.\n\nYou are the CEO of a multinational corporation considering expansion into emerging markets. Based on available data, which countries offer the most stable and profitable environment for expansion over the next five years? What factors are most relevant for your decision?\nAs a cosultant for the World Bank, you are tasked with improving democratic governance in developing countries. Based on trends in political and economic data over the past decade, what key areas (e.g., electoral integrity, gender equity, economic policy) should you prioritize to enhance democracy?\nA city mayor is planning to allocate resources to improve public health outcomes in response to rising pollution levels. Based on historical data, which factors should the mayor focus on to achieve the best results? Options could include things like healthcare infrastructure, public awareness campaigns, or environmental regulations.\nAs the head of a national education department, you need to decide where to allocate a limited budget: digital infrastructure for remote learning, teacher training programs, or curriculum development for STEM subjects. Which option is likely to yield the most significant improvement in educational outcomes based on available data?\nA company is considering launching a CSR initiative focused on reducing carbon emissions. As the head of the sustainability team, you need to determine the most effective strategy: investing in renewable energy, improving supply chain efficiency, or partnering with NGOs for reforestation projects. Which approach offers the best return on investment based on environmental and economic data?",
    "crumbs": [
      "Project",
      "Assignment 1"
    ]
  },
  {
    "objectID": "project/project-assignment-1.html#hypotheses",
    "href": "project/project-assignment-1.html#hypotheses",
    "title": "Project Assignment 1",
    "section": "Hypotheses",
    "text": "Hypotheses\nBefore gathering any data, or even looking at any data, you should write down your hypotheses. These are the assumptions you are making about the world that you will test with your data. Or, another way to think about hypotheses is as the predicted relationships between variables that you will test with your data. What do you expect to find and how do you expect the variables to be related? Write these down in a list here.\n\nH1:\nH2:\nH3:\netc.",
    "crumbs": [
      "Project",
      "Assignment 1"
    ]
  },
  {
    "objectID": "project/project-assignment-1.html#data-sources",
    "href": "project/project-assignment-1.html#data-sources",
    "title": "Project Assignment 1",
    "section": "Data Sources",
    "text": "Data Sources\nNow it is time to identify your data sources. You can use any data source that will be relevant in answering your question. This could be data from a government agency, an international organization, a non-profit, a private company, or even data that you collect yourself. A partial list of data sources is available here.\nBriefly list and discuss the data sources you plan to use. What data do they contain? How will you use them to answer your question? I would limit the number of data sources to 2-3. But just one data source is also OK if you think it is sufficient to answer your question.\n\nData Source 1:\nData Source 2:\nData Source 3:",
    "crumbs": [
      "Project",
      "Assignment 1"
    ]
  },
  {
    "objectID": "project/project-assignment-1.html#submission",
    "href": "project/project-assignment-1.html#submission",
    "title": "Project Assignment 1",
    "section": "Submission",
    "text": "Submission\nThat is it for Project Assignment 1! Please export this document and submit it via Blackboard by the due date. Each student should submit a separate QMD file so that we have something in Blackboard to grade.",
    "crumbs": [
      "Project",
      "Assignment 1"
    ]
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Support",
    "section": "",
    "text": "Writing Center GW’s Writing Center cultivates confident writers in the University community by facilitating collaborative, critical, and inclusive conversations at all stages of the writing process. Working alongside peer mentors, writers develop strategies to write independently in academic and public settings. Appointments can be booked online here.\nAcademic Commons Academic Commons provides tutoring and other academic support resources to students in many courses. Students can schedule virtual one-on-one appointments or attend virtual drop-in sessions. Students may schedule an appointment, review the tutoring schedule, access other academic support resources, or obtain assistance here.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Support",
    "section": "",
    "text": "Writing Center GW’s Writing Center cultivates confident writers in the University community by facilitating collaborative, critical, and inclusive conversations at all stages of the writing process. Working alongside peer mentors, writers develop strategies to write independently in academic and public settings. Appointments can be booked online here.\nAcademic Commons Academic Commons provides tutoring and other academic support resources to students in many courses. Students can schedule virtual one-on-one appointments or attend virtual drop-in sessions. Students may schedule an appointment, review the tutoring schedule, access other academic support resources, or obtain assistance here.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#support-for-students-outside-the-classroom",
    "href": "course-support.html#support-for-students-outside-the-classroom",
    "title": "Support",
    "section": "Support for students outside the classroom",
    "text": "Support for students outside the classroom\nDisability Support Services (DSS) 202-994-8250 Any student who may need an accommodation based on the potential impact of a disability should contact Disability Support Services to establish eligibility and to coordinate reasonable accommodations.\nCounseling and Psychological Services 202-994-5300 GW’s Colonial Health Center offers counseling and psychological services, supporting mental health and personal development by collaborating directly with students to overcome challenges and difficulties that may interfere with academic, emotional, and personal success.\nGW aims to create a community that cares for each other.The CARE Team fosters this goal by creating a pathway through which students who may need additional support can be identified and referred to the most appropriate services. Through the CARE Team, students are given the support they need to persist and succeed at GW and beyond.\nSafety and Security:\n\nIn an emergency: call GWPD 202-994-6111 or 911.\nFor situation-specific actions: review the Emergency Response Handbook\nStay informed: safety.gwu.edu/stay-informed",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "modules/shinylive-cost-function-base-R.html",
    "href": "modules/shinylive-cost-function-base-R.html",
    "title": "Untitled",
    "section": "",
    "text": "#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\ndata_points &lt;- data.frame(x = c(1, 2, 3), y = c(1, 2, 3))\n\nui &lt;- fluidPage(\n  titlePanel(\"Interactive Cost Function Explorer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"slope\", \n                  \"Slope Parameter:\", \n                  min = -2, max = 4, value = 1, step = 0.1),\n      br(),\n      h4(\"Current Values:\"),\n      textOutput(\"current_slope\"),\n      textOutput(\"current_ssr\"),\n      br(),\n      p(\"Move the slider to see how the slope affects:\"),\n      tags$ul(\n        tags$li(\"The regression line (left plot)\"),\n        tags$li(\"Your position on the cost function (right plot)\"),\n        tags$li(\"The sum of squared residuals\")\n      )\n    ),\n    \n    mainPanel(\n      plotOutput(\"combined_plot\", height = \"400px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  current_ssr &lt;- reactive({\n    predictions &lt;- input$slope * data_points$x\n    residuals &lt;- data_points$y - predictions\n    sum(residuals^2)\n  })\n  \n  cost_data &lt;- reactive({\n    slopes &lt;- seq(-2, 4, by = 0.1)\n    ssr_values &lt;- sapply(slopes, function(b) {\n      preds &lt;- b * data_points$x\n      resids &lt;- data_points$y - preds\n      sum(resids^2)\n    })\n    list(slopes = slopes, ssr = ssr_values)\n  })\n  \n  output$current_slope &lt;- renderText({\n    paste(\"Slope:\", round(input$slope, 2))\n  })\n  \n  output$current_ssr &lt;- renderText({\n    paste(\"Sum of Squared Residuals:\", round(current_ssr(), 2))\n  })\n  \n  output$combined_plot &lt;- renderPlot({\n    par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))\n    \n    # Left: Regression plot\n    plot(data_points$x, data_points$y, pch = 19, col = \"blue\",\n         xlim = c(0, 4), ylim = c(-2, 6),\n         xlab = \"x\", ylab = \"y\", main = \"Regression Line\")\n    abline(0, input$slope, col = \"red\", lwd = 2)\n    \n    # Right: Cost function\n    cost &lt;- cost_data()\n    plot(cost$slopes, cost$ssr, type = \"l\", col = \"darkred\", lwd = 2,\n         xlab = \"Slope Parameter\", ylab = \"Sum of Squared Residuals\",\n         main = \"Cost Function\")\n    points(input$slope, current_ssr(), col = \"red\", pch = 19, cex = 1.5)\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "modules/module-2.2.html",
    "href": "modules/module-2.2.html",
    "title": "Module 2.2",
    "section": "",
    "text": "Prework\n\n\n\n\nStart a new QMD file for this module in your Module 2 project folder.\nInstall the nycflights13 package: install.packages(\"nycflights13\").\nFamiliarize yourself with the dplyr package.\nHave a look at the dplyr cheatsheet."
  },
  {
    "objectID": "modules/module-2.2.html#overview",
    "href": "modules/module-2.2.html#overview",
    "title": "Module 2.2",
    "section": "Overview",
    "text": "Overview\nSometimes we want to do more than just explore a dataset in its raw form—we want to tailor it to answer specific questions. Whether we’re preparing data for a report, cleaning it for visualization, or building a model, it’s often necessary to narrow down to the most relevant rows, select only the variables we care about, or generate new variables that express relationships or transformations more clearly. In this module, we’ll learn how to do just that using three foundational functions from the dplyr package: filter(), select(), and mutate()."
  },
  {
    "objectID": "modules/module-2.2.html#exploring-the-nycflights13-data",
    "href": "modules/module-2.2.html#exploring-the-nycflights13-data",
    "title": "Module 2.2",
    "section": "Exploring the nycflights13 Data",
    "text": "Exploring the nycflights13 Data\nLet’s use the flights dataset from the nycflights13 package to learn the grammar of data wrangling in R. This dataset contains detailed information on all flights departing from New York City airports in 2013. It includes a variety of variables, such as departure and arrival times, flight delays, air time, and more, making it an ideal dataset for practicing data wrangling techniques.\n\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nflights |&gt; glimpse()\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…"
  },
  {
    "objectID": "modules/module-2.2.html#filtering-rows-with-filter",
    "href": "modules/module-2.2.html#filtering-rows-with-filter",
    "title": "Module 2.2",
    "section": "Filtering Rows with filter()\n",
    "text": "Filtering Rows with filter()\n\nThe filter() function allows us to extract rows from a data frame that meet specific conditions. This is useful when we want to zoom in on a particular subset of the data. For example, we might be interested in analyzing only the flights that departed on a certain day, or perhaps only the flights from a specific carrier or airport.\nSuppose we want to examine only the flights that departed on January 1st. We can use:\n\nflights |&gt;\n  filter(month == 1, day == 1)\n\n# A tibble: 842 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 832 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nFiltering is essential when we are interested in targeted analyses, like studying seasonal trends, isolating outliers, or preparing training and test sets for modeling.\n\n\n\n\n\n\nCode Detail: == vs =\n\n\n\nIn the above code chunk, we used filter(month == 1, day == 1) to filter for flights departing on January 1. The == operator is used for comparison, meaning “equal to.” This is different from the = operator, which is used for assignment in R. When filtering data, we always use == to specify conditions we want to match (not =).\n\n\nThe filter() function can handle multiple conditions using logical operators. Here we used == for “equal to.” But we could also use != (not equal to), &lt; (less than), &gt; (greater than), & (and), and | (or). We can also combine operators to create more complex conditions. For example, if we wanted to filter for flights that occurred in the month of June or earlier, we could write filter(month &lt;= 6). Similarly, to find flights that were delayed by more than 30 minutes, we could use filter(dep_delay &gt; 30). Try experimenting with these operators in the filter() function in the code above to see how they affect the results.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nUse the filter() function to find all flights that departed from JFK airport.\nFilter the flights that arrived on or after June 1.\nFind flights that arrived before May 1 and were delayed by more than 60 minutes.\nFilter for flights that were operated by American Airlines (carrier code “AA”) or Delta Airlines (carrier code “DL”)."
  },
  {
    "objectID": "modules/module-2.2.html#selecting-columns-with-select",
    "href": "modules/module-2.2.html#selecting-columns-with-select",
    "title": "Module 2.2",
    "section": "Selecting Columns with select()\n",
    "text": "Selecting Columns with select()\n\nThe select() function is used to choose a subset of columns from a data frame. This is helpful when we are only interested in a few variables and want to avoid cluttering our analysis with unnecessary information.\nFor example, if we want to examine only the year, month, day, departure time, and flight number, we might write:\n\nflights |&gt;\n  select(year, month, day, dep_time, arr_time, carrier, flight)\n\n# A tibble: 336,776 × 7\n    year month   day dep_time arr_time carrier flight\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt;\n 1  2013     1     1      517      830 UA        1545\n 2  2013     1     1      533      850 UA        1714\n 3  2013     1     1      542      923 AA        1141\n 4  2013     1     1      544     1004 B6         725\n 5  2013     1     1      554      812 DL         461\n 6  2013     1     1      554      740 UA        1696\n 7  2013     1     1      555      913 B6         507\n 8  2013     1     1      557      709 EV        5708\n 9  2013     1     1      557      838 B6          79\n10  2013     1     1      558      753 AA         301\n# ℹ 336,766 more rows\n\n\nWe can also rename variables while selecting them, which can be useful for clarity or consistency with other datasets:\n\nflights |&gt;\n  select(date = time_hour, airline = carrier, flight)\n\n# A tibble: 336,776 × 3\n   date                airline flight\n   &lt;dttm&gt;              &lt;chr&gt;    &lt;int&gt;\n 1 2013-01-01 05:00:00 UA        1545\n 2 2013-01-01 05:00:00 UA        1714\n 3 2013-01-01 05:00:00 AA        1141\n 4 2013-01-01 05:00:00 B6         725\n 5 2013-01-01 06:00:00 DL         461\n 6 2013-01-01 05:00:00 UA        1696\n 7 2013-01-01 06:00:00 B6         507\n 8 2013-01-01 06:00:00 EV        5708\n 9 2013-01-01 06:00:00 B6          79\n10 2013-01-01 06:00:00 AA         301\n# ℹ 336,766 more rows\n\n\nSelecting variables makes our workflow cleaner, especially when preparing data for visualization or modeling.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nUse the select() function to keep only the year, month, day, dep_time, and arr_time columns.\nSelect the flight, origin, dest, and air_time columns and rename them to flight_number, departure_airport, arrival_airport, and flight_duration."
  },
  {
    "objectID": "modules/module-2.2.html#creating-new-variables-with-mutate",
    "href": "modules/module-2.2.html#creating-new-variables-with-mutate",
    "title": "Module 2.2",
    "section": "Creating New Variables with mutate()\n",
    "text": "Creating New Variables with mutate()\n\nThe mutate() function allows us to add new columns to a data frame or transform existing ones. This is useful for deriving new insights from raw data. For example, we might want to calculate how much time was gained or lost during a flight:\n\nflights |&gt;\n  mutate(time_gain = arr_delay - dep_delay) |&gt;\n  select(flight, dep_delay, arr_delay, time_gain)\n\n# A tibble: 336,776 × 4\n   flight dep_delay arr_delay time_gain\n    &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1   1545         2        11         9\n 2   1714         4        20        16\n 3   1141         2        33        31\n 4    725        -1       -18       -17\n 5    461        -6       -25       -19\n 6   1696        -4        12        16\n 7    507        -5        19        24\n 8   5708        -3       -14       -11\n 9     79        -3        -8        -5\n10    301        -2         8        10\n# ℹ 336,766 more rows\n\n\nThis calculation gives us a sense of whether flights tended to recover from delays during their journey. We can also use mutate() to convert units (e.g., minutes to hours), flag particular conditions, or apply mathematical transformations.\nWe can use many different arithmetic operations within mutate(), such as addition (+), subtraction (-), multiplication (*), and division (/) or exponentiation (^ or **). For example, if we wanted to convert air_time from minutes to hours, we could write mutate(air_time_hours = air_time / 60). Or, if we wanted to square the arr_delay variable, we could write mutate(arr_delay_squared = arr_delay^2) or mutate(arr_delay_squared = arr_delay ** 2). We will use arithmetic operators in the context of mutate a lot in the course, so it’s worth practicing with different operations to see how they work.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nUse the mutate() function to create a new variable called total_delay that sums dep_delay and arr_delay.\nCreate a new variable called air_time_hours that converts air_time from minutes to hours.\nAdd a new variable called flight_speed that calculates the average speed of the flight in miles per hour, assuming the distance is given in miles and after converting air time into hours."
  },
  {
    "objectID": "modules/module-2.2.html#combining-filter-select-and-mutate",
    "href": "modules/module-2.2.html#combining-filter-select-and-mutate",
    "title": "Module 2.2",
    "section": "Combining filter(), select(), and mutate()\n",
    "text": "Combining filter(), select(), and mutate()\n\nThese three functions are most powerful when used together. A typical workflow might involve narrowing down the dataset to a subset of interest, selecting relevant variables, and then creating new variables to aid analysis.\nHere’s an example:\n\nflights |&gt;\n  filter(month == 6, day == 15) |&gt;\n  select(flight, origin, dest, dep_delay, arr_delay) |&gt;\n  mutate(total_delay = dep_delay + arr_delay)\n\n# A tibble: 801 × 6\n   flight origin dest  dep_delay arr_delay total_delay\n    &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n 1   1431 EWR    CLT          -4       -22         -26\n 2    327 EWR    IAH           0        -3          -3\n 3    725 JFK    BQN          -4        34          30\n 4   1714 LGA    IAH          -3       -25         -28\n 5   1141 EWR    SFO          -2        -5          -7\n 6   5559 EWR    DTW          -9        -9         -18\n 7    635 LGA    ORD          -7       -23         -30\n 8    517 EWR    MCO          -6       -28         -34\n 9    731 LGA    DTW          -6       -16         -22\n10   1535 LGA    PHL          -6        -7         -13\n# ℹ 791 more rows\n\n\nIn this example, we look at flights on June 15, keep only columns that describe the flight and its delays, and compute the total delay time. This kind of pipeline helps us build a tidy and interpretable dataset suited for visualization or statistical analysis."
  },
  {
    "objectID": "modules/module-2.2.html#applying-the-concepts-to-the-v-dem-dataset",
    "href": "modules/module-2.2.html#applying-the-concepts-to-the-v-dem-dataset",
    "title": "Module 2.2",
    "section": "Applying the Concepts to the V-Dem Dataset",
    "text": "Applying the Concepts to the V-Dem Dataset\nNow that we have a solid understanding of how to filter, select, and mutate data using dplyr, let’s apply these concepts to a new dataset: the Varieties of Democracy (V-Dem) dataset. This dataset contains a wealth of information about democracy and governance across countries and over time.\nIn this video walkthrough, we’ll revisit the filter(), select(), and mutate() functions in the context of real-world political data. We’ll explore how to narrow our focus to particular years or countries, select the most relevant indicators of democracy, and construct new variables that help make sense of complex governance metrics.\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nUsing the nycflights dataset, filter the flights for February only.\nSelect carrier, origin, dest, and air_time.\nCreate a new variable that converts air_time from minutes to hours.\nExplore other interesting combinations using filter(), select(), and mutate().\nGo to kaggle.com and find a dataset that interests you. Use the dplyr functions to filter, select, and mutate the data. Optional: glimpse() your data and share a screenshot of your results in the discussion forum."
  },
  {
    "objectID": "modules/module-2.1.html",
    "href": "modules/module-2.1.html",
    "title": "Module 2.1",
    "section": "",
    "text": "Prework\n\n\n\n\nCreate a project in RStudio for Module 2.\nCreate a QMD file in your Module 2 project folder for this lesson.\nCreate a data subfolder in your Module 2 project folder to store any data files you download.\nLoad readr and dplyr packages for the exercises in this module."
  },
  {
    "objectID": "modules/module-2.1.html#overview",
    "href": "modules/module-2.1.html#overview",
    "title": "Module 2.1",
    "section": "Overview",
    "text": "Overview\nBefore we can analyze or visualize data, we need to understand its structure and quality. In this module, we explore where data comes from, how it is typically organized in R, and what it means for data to be tidy and clean. We introduce key concepts such as tabular data, variables, observations, and units of analysis, and we explain the principles of tidy data—a structure that makes data analysis more straightforward and consistent. We also discuss the importance of clean data, which ensures that values are well-formatted, column names are usable, and missing or duplicated entries are handled. By the end of the module, you will be able to assess the structure and quality of a dataset and begin transforming it into a form suitable for analysis."
  },
  {
    "objectID": "modules/module-2.1.html#where-does-data-come-from",
    "href": "modules/module-2.1.html#where-does-data-come-from",
    "title": "Module 2.1",
    "section": "Where Does Data Come From?",
    "text": "Where Does Data Come From?\nData enters our workflow in many ways. Often, it is sent to us directly by a boss or a client, usually in the form of a spreadsheet or CSV file. Sometimes we collect it ourselves through surveys, or we might use data collected by someone else. Increasingly, data is also available online, either for download or through scraping techniques. In R, we often work with data that comes packaged in libraries, such as unvotes or vdemlite. Many datasets can also be accessed programmatically through application programming interfaces, or APIs. Regardless of the source, the structure and quality of the data are not guaranteed, which brings us to the question of what makes data usable in practice.\n\n\n\n\n\n\nDid you know?\n\n\n\nMany packages ship with their own inbuilt datasets. For example, the ggplot2 package includes the mpg dataset, which contains information about fuel economy for various car models. Similarly dplyr includes the starwars dataset, which contains information about characters from the Star Wars universe.\nTo find out what datasets are included in a package, you can use the data(package = \"package_name\") function. For example, data(package = \"ggplot2\") will list all datasets available in the ggplot2 package. To use the datasets, you can load the package and simply call the dataset by its name, like mpg or starwars."
  },
  {
    "objectID": "modules/module-2.1.html#getting-started-with-data",
    "href": "modules/module-2.1.html#getting-started-with-data",
    "title": "Module 2.1",
    "section": "Getting Started with Data",
    "text": "Getting Started with Data\nMost of the data we encounter in applied work are tabular in nature. This means they are organized into rows and columns, and are sometimes referred to as rectangular data. In R, the standard way to represent this kind of data is with a data frame. Each column in the data frame corresponds to a variable or attribute, such as GDP per capita or life expectancy. Each row corresponds to a single unit of observation, such as a country, individual, or point in time.\nA data frame is built around a unit of analysis–the entity or level at which observations are recorded. In a cross-sectional dataset, the units might be countries, states, cities, or individuals at a specific moment. In a time-series dataset, the units are often repeated over time such as countries measured annually from 1990 to 2020. Understanding the unit of analysis is key to interpreting and analyzing data correctly."
  },
  {
    "objectID": "modules/module-2.1.html#the-concept-of-tidy-data",
    "href": "modules/module-2.1.html#the-concept-of-tidy-data",
    "title": "Module 2.1",
    "section": "The Concept of Tidy Data",
    "text": "The Concept of Tidy Data\nTo work effectively with data, especially in the R ecosystem, it is helpful to adhere to a structure known as tidy data. In tidy data, each column is reserved for a single variable, each row contains exactly one observation, and each cell holds a single value. This format aligns with the expectations of many functions in the tidyverse, making the data easier to filter, transform, and visualize. Tidy data is not merely a stylistic preference–it is a convention that supports a consistent and predictable approach to analysis.\n\n\nTidy Data Example\nThe structure of tidy data can be illustrated visually. The image below shows a dataset where each column is a variable, each row is a separate case, and each cell contains a single measurement:\n\n\n\nTidy Data\n\n\nThis clean organization contrasts sharply with the ad hoc formatting often encountered in real-world data files.\n\n\nWhat are Clean Data?\nWhere tidy data is about structure, clean data is about making sure that data are free of errors and inconsistencies that make analysis difficult or misleading. This includes ensuring that column names are not duplicated and are easy to reference in code. It also means that missing values have been addressed, either through imputation or removal. Clean data should not contain extra or blank rows or columns, and all values should be stored in the proper format—for example, dates should be stored as actual date objects rather than character strings. Clean data allow us to trust the integrity of our analysis and avoid downstream issues.\n\n\nMessy Data Example\nTo appreciate the importance of clean data, it helps to examine what happens when data are not clean or tidy. The image below shows an example of messy data—likely extracted from a real-world spreadsheet—where variable names may be repeated, values are spread across multiple columns inappropriately, and the structure does not conform to tidy principles:\n\n\n\nMessy Data\n\n\nThis kind of data requires significant wrangling before it can be used effectively in analysis."
  },
  {
    "objectID": "modules/module-2.1.html#how-do-we-get-tidy-and-clean-data",
    "href": "modules/module-2.1.html#how-do-we-get-tidy-and-clean-data",
    "title": "Module 2.1",
    "section": "How Do We Get Tidy and Clean Data?",
    "text": "How Do We Get Tidy and Clean Data?\nIn practice, the tidiness and cleanliness of data vary by source. Files sent from clients or supervisors often require substantial cleanup before they can be used. Survey data can range in quality depending on how it was collected and structured. Data downloaded from the web or scraped from a site often arrive in a messy format. On the other hand, data included in curated packages, or accessed through well-designed APIs, are more likely to be tidy and clean by default. We will be working with a variety of data sources throughout this course, and we will learn how to transform messy data into tidy and clean formats.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nGo to kaggle.com and search for a dataset that interests you. Download it and save it in the data subfolder that you created for this module.\nRead the data into R using the read_csv() function from the readr package.\nUse the glimpse() function to examine the structure of the data.\nIdentify the unit of analysis for the dataset.\nCheck if the data is tidy. If it is not, identify at least one way to make it tidy.\nCheck if the data is clean. If it is not, identify at least one way to make it clean.\nEvery dataset on Kaggle has a data usability score and a discussion section. Read through the discussion to see if others have identified issues with the data. If so, what are they? How would you address them? Does the usability score comport with what you are seeing?"
  },
  {
    "objectID": "modules/module-2.1.html#tibbles-a-modern-take-on-data-frames",
    "href": "modules/module-2.1.html#tibbles-a-modern-take-on-data-frames",
    "title": "Module 2.1",
    "section": "Tibbles: A Modern Take on Data Frames",
    "text": "Tibbles: A Modern Take on Data Frames\nWe have been talking a lot about data frames as a standard way of storing tabular data in R, where each column represents a variable and each row is an observation. In this section, we introduce tibbles, which are modern reimaginings of data frames designed for use within the tidyverse. While tibbles behave much like data frames, they offer a number of important improvements that make them more predictable and user-friendly.\nTibbles avoid some of the surprising behaviors of base R data frames. For example, they do not automatically convert character strings into factors. They also display data in a more readable way, showing only the first ten rows and as many columns as fit on screen. Column names are preserved exactly as written, even if they contain spaces or other special characters. In general, tibbles are slightly easier to work with than base R data frames when writing scripts or building reproducible workflows.\nWhenever you read data into R using a tidyverse package like readr, the result is automatically returned as a tibble. But you can also create your own tibble from scratch using the tibble() function from the tibble package. Here’s an example that constructs a small tibble containing names, ages, heights, and a logical variable indicating whether someone is a student:\nlibrary(tibble)\n\nmy_tibble &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  height = c(160, 170, 180),\n  is_student = c(TRUE, FALSE, FALSE)\n)\n\nmy_tibble\nThis tibble contains several different types of data. Each column is stored using an appropriate data type, which R identifies using short labels that appear when printing the tibble. Understanding these types is important because they affect how R interprets and manipulates your data."
  },
  {
    "objectID": "modules/module-2.1.html#common-data-types-in-tibbles",
    "href": "modules/module-2.1.html#common-data-types-in-tibbles",
    "title": "Module 2.1",
    "section": "Common Data Types in Tibbles",
    "text": "Common Data Types in Tibbles\nTibbles can store a variety of column types, each suited to a particular kind of data. For example, text is stored as character strings, numbers can be stored as integers or doubles, and categorical values are stored as factors. Tibbles also support more specialized types, including dates, date-times, and even lists. The table below summarizes the most common data types you are likely to encounter when working with tibbles in R:\n\n\n\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\n&lt;chr&gt;\nStores text strings\n\"hello\", \"R programming\"\n\n\n&lt;dbl&gt;\nStores decimal (floating-point) numbers\n3.14, -1.0\n\n\n&lt;int&gt;\nStores whole numbers (integers)\n1, -100, 42\n\n\n&lt;lgl&gt;\nStores boolean values\nTRUE, FALSE, NA\n\n\n&lt;fct&gt;\nStores categorical variables with fixed levels\nfactor(c(\"low\", \"medium\", \"high\"))\n\n\n&lt;date&gt;\nStores calendar dates (class Date) in YYYY-MM-DD format\nas.Date(\"2024-09-05\")\n\n\n&lt;dttm&gt;\nStores date-time values (class POSIXct)\nas.POSIXct(\"2024-09-05 14:30:00\")\n\n\n&lt;time&gt;\nStores time-of-day values (rarely used without a date)\n\"14:30:00\"\n\n\n&lt;list&gt;\nStores lists, where each entry can be a complex object\nlist(c(1, 2, 3), c(\"a\", \"b\", \"c\"))"
  },
  {
    "objectID": "modules/module-2.1.html#working-with-dates-and-times-using-lubridate",
    "href": "modules/module-2.1.html#working-with-dates-and-times-using-lubridate",
    "title": "Module 2.1",
    "section": "Working with Dates and Times Using lubridate",
    "text": "Working with Dates and Times Using lubridate\nHandling dates and times in R can be tricky, especially when working with data from different sources that use inconsistent formats. The lubridate package simplifies this process by providing a consistent and readable set of functions for parsing, manipulating, and formatting dates and times. Unlike base R, which requires more manual effort to convert character strings into date objects and extract components like year or month, lubridate is designed to make these tasks intuitive.\nOne of the key advantages of lubridate is that its functions are named according to the structure of the date format itself. For example, ymd() expects input in a year-month-day format, while mdy() expects month-day-year. This eliminates the need to specify format strings manually, reducing the chance of error and improving code readability.\nHere’s a quick example showing how to use lubridate to parse dates in multiple formats:\nlibrary(lubridate)\n\n# Parse dates in different formats\nmy_date &lt;- ymd(\"2024-09-05\")\nmy_date2 &lt;- mdy(\"09-05-2024\")\nmy_date3 &lt;- dmy(\"05-09-2024\")\n\n# Print in long form\nformat(my_date, \"%B %d, %Y\")  # \"September 05, 2024\"\nOnce stored correctly as date objects, these values can be used in filtering, plotting, or date-based calculations without additional conversion steps.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nCreate your own tibble on a topic that interests you—this could be anything from meals you’ve cooked to books you’ve read or places you’ve visited.\nTry to include at least three different data types, such as character, numeric, and logical.\nInclude a date or date-time column using lubridate to parse your date values.\nUse tibble() to create your tibble and print it to inspect the structure and data types."
  },
  {
    "objectID": "modules/module-2.4.html",
    "href": "modules/module-2.4.html",
    "title": "Module 2.4",
    "section": "",
    "text": "Prework\n\n\n\n\nStart a QMD file for this module.\nInstall the WDI package and study the documentation\n\nVisit the World Development Indicators database and explore the available indicators.\nInstall the countrycode package and read about it.\nRead about mutating joins in dplyr."
  },
  {
    "objectID": "modules/module-2.4.html#overview",
    "href": "modules/module-2.4.html#overview",
    "title": "Module 2.4",
    "section": "Overview",
    "text": "Overview\nIn this module, we will learn how to join data frames in R using the dplyr package. Joining data frames is a fundamental operation in data analysis, allowing you to combine information from different sources based on common keys. We will explore various types of joins, including inner joins, left joins, right joins, and full joins, and apply a left join to merge two real-world datasets."
  },
  {
    "objectID": "modules/module-2.4.html#packages-for-api-data",
    "href": "modules/module-2.4.html#packages-for-api-data",
    "title": "Module 2.4",
    "section": "Packages for API Data",
    "text": "Packages for API Data\nAs more organizations publish their data online, APIs—Application Programming Interfaces—have become a standard way to provide structured access to that information. Rather than downloading spreadsheets or scraping websites, we can use R code to request data directly from a server and receive it in a tidy format.\nIn R, a growing number of packages are designed specifically to help you access data from these APIs. These tools handle the technical details of connecting to the API and parsing the response, so you can focus on analysis. Packages like WDI and wbstats (for World Bank data), fredr (for U.S. Federal Reserve data), and tidycensus (for U.S. Census data) make it much easier to pull data into your workflow with just a few lines of code.\nIn this module, we will use the WDI package by Vincent Arel-Bundock to access data from the World Bank’s open data API. The package provides a streamlined interface to over 40 datasets published by the World Bank, including the World Development Indicators, Gender Statistics, Education Statistics, and more. These databases cover a wide range of topics such as health, labor, infrastructure, environment, and governance, with data available for nearly every country in the world. The package returns the data in a tidy format, making it easy to integrate into your analysis workflow.\nThe basic syntax for using the WDI package is as follows:\n\nlibrary(WDI)\nlibrary(dplyr)\n\nwbdata &lt;- WDI(\n  indicator = \"NY.GDP.PCAP.KD\", # per capita GDP in constant US dollars\n  country = c(\"USA\", \"CAN\", \"MEX\"), # Countries to include\n  start = 2000, # Start year\n  end = 2020, # End year\n  extra = TRUE # Include extra metadata (default is FALSE)\n)\n\nglimpse(wbdata)\n\nRows: 63\nColumns: 13\n$ country        &lt;chr&gt; \"Canada\", \"Canada\", \"Canada\", \"Canada\", \"Canada\", \"Cana…\n$ iso2c          &lt;chr&gt; \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"…\n$ iso3c          &lt;chr&gt; \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\",…\n$ year           &lt;int&gt; 2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2…\n$ NY.GDP.PCAP.KD &lt;dbl&gt; 42366.129, 45100.291, 44907.344, 44339.389, 43551.343, …\n$ status         &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ lastupdated    &lt;chr&gt; \"2025-04-15\", \"2025-04-15\", \"2025-04-15\", \"2025-04-15\",…\n$ region         &lt;chr&gt; \"North America\", \"North America\", \"North America\", \"Nor…\n$ capital        &lt;chr&gt; \"Ottawa\", \"Ottawa\", \"Ottawa\", \"Ottawa\", \"Ottawa\", \"Otta…\n$ longitude      &lt;chr&gt; \"-75.6919\", \"-75.6919\", \"-75.6919\", \"-75.6919\", \"-75.69…\n$ latitude       &lt;chr&gt; \"45.4215\", \"45.4215\", \"45.4215\", \"45.4215\", \"45.4215\", …\n$ income         &lt;chr&gt; \"High income\", \"High income\", \"High income\", \"High inco…\n$ lending        &lt;chr&gt; \"Not classified\", \"Not classified\", \"Not classified\", \"…\n\n\n\n\n\n\n\n\nISO Country Codes\n\n\n\nThe country parameter takes either ISO aplha 2 or 3 codes which you can find here. Familiarize yourself with these codes a bit as they will become important when we want merge datasets based on them.\n\n\nIf you want to download multiple indicators at once, you can pass a vector of indicator codes to the indicator parameter. You can also rename variables as you select them. For example:\n\nindicators &lt;- c(gdp_pc = \"NY.GDP.PCAP.KD\", population = \"SP.POP.TOTL\") \n\nwbdata2 &lt;- WDI(\n  indicator = indicators, # GDP and total pop\n  country = c(\"USA\", \"CAN\", \"MEX\"),\n  start = 2000,\n  end = 2020\n)\n\nglimpse(wbdata2)\n\nRows: 63\nColumns: 6\n$ country    &lt;chr&gt; \"Canada\", \"Canada\", \"Canada\", \"Canada\", \"Canada\", \"Canada\",…\n$ iso2c      &lt;chr&gt; \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\",…\n$ iso3c      &lt;chr&gt; \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CA…\n$ year       &lt;int&gt; 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,…\n$ gdp_pc     &lt;dbl&gt; 37906.860, 38200.456, 38921.667, 39270.023, 40108.759, 4100…\n$ population &lt;dbl&gt; 30685730, 31020855, 31359199, 31642461, 31938807, 32242732,…\n\n\nThere are two ways to find indicators to download with WDI. One is to use the built-in WDIsearch() function to search for indicators by keyword, like this:\n\nWDIsearch('labor force participation') |&gt; \n  as_tibble() |&gt; # Convert to tibble for easier viewing\n  slice(1:10) # Show first 10 results\n\n# A tibble: 10 × 2\n   indicator         name                                                       \n   &lt;chr&gt;             &lt;chr&gt;                                                      \n 1 9.0.Labor.All     Labor Force Participation Rate (%)                         \n 2 9.0.Labor.B40     Labor Force Participation Rate (%)-Bottom 40 Percent       \n 3 9.0.Labor.T60     Labor Force Participation Rate (%)-Top 60 Percent          \n 4 9.1.Labor.All     Labor Force Participation Rate (%), Male                   \n 5 9.1.Labor.B40     Labor Force Participation Rate (%)-Bottom 40 Percent, Male \n 6 9.1.Labor.T60     Labor Force Participation Rate (%)-Top 60 Percent, Male    \n 7 9.2.Labor.All     Labor Force Participation Rate (%), Female                 \n 8 9.2.Labor.B40     Labor Force Participation Rate (%)-Bottom 40 Percent, Fema…\n 9 9.2.Labor.T60     Labor Force Participation Rate (%)-Top 60 Percent, Female  \n10 JI.TLF.ACTI.FE.ZS Labor force participation rate, aged 15-64, female (% of f…\n\n\nOr with regular expressions like this:\n\nWDIsearch('labor.*participation.*female') |&gt;\n    as_tibble() |&gt; \n    slice(1:10) \n\n# A tibble: 10 × 2\n   indicator                 name                                               \n   &lt;chr&gt;                     &lt;chr&gt;                                              \n 1 9.2.Labor.All             Labor Force Participation Rate (%), Female         \n 2 9.2.Labor.B40             Labor Force Participation Rate (%)-Bottom 40 Perce…\n 3 9.2.Labor.T60             Labor Force Participation Rate (%)-Top 60 Percent,…\n 4 JI.TLF.ACTI.FE.ZS         Labor force participation rate, aged 15-64, female…\n 5 SL.TLF.ACTI.1524.FE.NE.ZS Labor force participation rate for ages 15-24, fem…\n 6 SL.TLF.ACTI.1524.FE.ZS    Labor force participation rate for ages 15-24, fem…\n 7 SL.TLF.ACTI.FE.ZS         Labor force participation rate, female (% of femal…\n 8 SL.TLF.CACT.2534.FE.ZS    Labor participation rate, female (% of female popu…\n 9 SL.TLF.CACT.2554.FE.ZS    Labor participation rate, female (% of female popu…\n10 SL.TLF.CACT.3554.FE.ZS    Labor participation rate, female (% of female popu…\n\n\n\n\n\n\n\n\nNote\n\n\n\nA regular expression (or regex) is a special pattern used to match text. In WDIsearch(), regular expressions let you search for indicators based on the presence and order of words. For example, the pattern “labor.*participation.*female” matches any indicator name that contains the word “labor,” followed by “participation,” and then “female,” with any characters in between. See this chapter of R for Data Science for more on regular expressions.\n\n\nNote that here we are using the dplyr slice() function to limit the results to the first 10 rows. You can adjust this number as needed. The output will show you the indicator name, code, and description, which you can use in the WDI() function. We also convert the output to a tibble for easier viewing in a rendered Quarto document (but this is not necessary in a notebook or in the console).\nThe other way you can hunt for indicators is to browse the World Bank’s World Development Indicators database directly. Here you can go to the Series tab and simply search for the indicator you are interested in. Then click on the information icon to view the indicator’s code, which you can use in the WDI() function.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nUse WDIsearch() to find an indicator of interest.\nUse WDI() to download that variable for a country or set of countries.\nUse WDI() to download multiple indicators at once."
  },
  {
    "objectID": "modules/module-2.4.html#joining-datasets",
    "href": "modules/module-2.4.html#joining-datasets",
    "title": "Module 2.4",
    "section": "Joining Datasets",
    "text": "Joining Datasets\nNow let’s say that we want to download some data from the World Bank with WDI and now we want to merge it with another dataset. For example, we might want to analyze the relationship between some economic indicators like GDP and population, and some political indicators like democracy scores from the V-Dem dataset. To do this, we need to join the two datasets together based on a common key, which in this case is the country code.\nWhen we talk about a join in data wrangling, we are usually referring to a horizontal merge—that is, combining two data frames side by side by matching rows based on shared values in one or more columns (like country and year). This is common when we have data from two different sources, such as World Bank and V-Dem, and we want to analyze them together.\nIn dplyr, the most common joins fall into two categories: mutating joins and filtering joins. Mutating joins are the ones you’ll use most often when combining datasets. There are four main types.\nAn inner join keeps only the rows that match in both datasets. If a country-year pair is missing in either dataset, it will be dropped from the result.\n\n\nSource: R for HR\n\nA full join keeps everything from both datasets. If a country-year pair exists in only one of them, you’ll still see it in the final data, with missing values (NA) filled in where needed.\n\n\nSource: R for HR\n\nA left join keeps all rows from the left dataset (typically the one you’re focusing your analysis on) and adds columns from the right dataset wherever there’s a match.\n\n\nSource: R for HR\n\nA right join is like a left join, but it keeps all rows from the right dataset and adds data from the left wherever possible.\n\n\nSource: R for HR\n\nMost of the time, we will use a left join because we want to keep the structure of one main dataset and supplement it with additional information. The syntax for a left join in dplyr is straightforward:\n\nleft_join(data1, data2, by = \"common_column\")"
  },
  {
    "objectID": "modules/module-2.4.html#common-keys-and-the-countrycode-package",
    "href": "modules/module-2.4.html#common-keys-and-the-countrycode-package",
    "title": "Module 2.4",
    "section": "Common Keys (and the countrycode Package)",
    "text": "Common Keys (and the countrycode Package)\nThat brings us to our next point, which is that when we are joining datasets, it’s crucial to ensure that the columns you’re joining on have the same data type and format. For example, if one dataset uses ISO alpha-3 country codes (like “USA”) and another uses alpha-2 codes (like “US”), you’ll need to standardize them before joining.\nIn the context of cross-country analysis, the countrycode package (also authored by Vincent Arel-Bundock) is immensely helpful. It allows you to convert between different country code formats, such as ISO alpha-2, alpha-3, numeric codes, and even country names.\nTo convert country codes using the countrycode package, you can use mutate() along with the countrycode() function. Let’s try it with the wbdata dataset we created earlier:\n\nlibrary(countrycode)\n\n# Convert World Bank ISO alpha-2 codes to V-Dem alpha-3 codes\nwbdata_with_vdem_codes &lt;- wbdata2 |&gt; \n  mutate(vdem_country_code = countrycode(\n    sourcevar = iso3c, \n    origin = \"wb\", \n    destination = \"vdem\")\n    ) |&gt;\n  relocate(vdem_country_code, .after = iso3c)\n\nglimpse(wbdata_with_vdem_codes)\n\nRows: 63\nColumns: 7\n$ country           &lt;chr&gt; \"Canada\", \"Canada\", \"Canada\", \"Canada\", \"Canada\", \"C…\n$ iso2c             &lt;chr&gt; \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\"…\n$ iso3c             &lt;chr&gt; \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CAN\", \"CA…\n$ vdem_country_code &lt;dbl&gt; 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, …\n$ year              &lt;int&gt; 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008…\n$ gdp_pc            &lt;dbl&gt; 37906.860, 38200.456, 38921.667, 39270.023, 40108.75…\n$ population        &lt;dbl&gt; 30685730, 31020855, 31359199, 31642461, 31938807, 32…\n\n\nHere we are adding a new column to our wbdata dataset called vdem_country_code, which contains the V-Dem alpha-3 codes corresponding to the ISO alpha-2 codes in the iso2c column. The first argument in the countrycode() function is the vector of country codes that we want to convert, the second is the source code type (in this case, World Bank style iso3c codes or “wb”), and the third is the target code type (in this case, “vdem”). We also use the dplyr relocate() verb to move the new vdem_country_code column right after the original iso3c column for better organization."
  },
  {
    "objectID": "modules/module-2.4.html#performing-a-join",
    "href": "modules/module-2.4.html#performing-a-join",
    "title": "Module 2.4",
    "section": "Performing a Join",
    "text": "Performing a Join\n\nNow let’s put it all together and use our join skills to explore the relationship between democracy and development. First we will grab the GDP per capita and population for all of the countries for the years 2000 to 2020 from the World Bank using the WDI() function. Then we will summarize the data across this period by taking the mean GDP per capita and population for each country. And finally we will use the countrycode package to convert the World Bank country codes to V-Dem country codes so we can join it with the V-Dem democracy data (which we will download next).\n\nindicators &lt;- c(\n  gdp_pc = \"NY.GDP.PCAP.KD\", \n  population = \"SP.POP.TOTL\" \n)\n\nwbdata_all_countries &lt;- WDI(\n  indicator = indicators, \n  country = \"all\", # get data for all countries\n  start = 2000,\n  end = 2020,\n  extra = TRUE # Include extra metadata\n) |&gt;\n  filter(region != \"Aggregates\") |&gt; # Exclude aggregate regions\n  group_by(iso3c, region) |&gt; # Group by country\n  summarize(\n    gdp_pc = mean(gdp_pc, na.rm = TRUE), \n    population = mean(population, na.rm = TRUE)\n  ) |&gt; \n  ungroup() |&gt; # Ungroup to remove grouping structure\n  mutate(vdem_country_code = countrycode(\n    sourcevar = iso3c, \n    origin = \"wb\", \n    destination = \"vdem\")\n  ) |&gt;\n  relocate(vdem_country_code, .after = iso3c)\n\nglimpse(wbdata_all_countries)\n\nRows: 215\nColumns: 5\n$ iso3c             &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"ALB\", \"AND\", \"ARE\", \"ARG\", \"AR…\n$ vdem_country_code &lt;dbl&gt; NA, 36, 104, 12, NA, 207, 37, 105, NA, NA, 67, 144, …\n$ region            &lt;chr&gt; \"Latin America & Caribbean\", \"South Asia\", \"Sub-Saha…\n$ gdp_pc            &lt;dbl&gt; 29293.9856, 465.7585, 2761.2866, 3374.7261, 38048.31…\n$ population        &lt;dbl&gt; 101674.52, 28965247.52, 23886139.33, 2943197.10, 752…\n\n\n\n\n\n\n\n\nWhy ungroup()?\n\n\n\nAfter summarizing by country and year, we use ungroup() to remove the grouping structure. This ensures that any subsequent operations—like mutate() or joins—aren’t accidentally performed within each group. It helps avoid unexpected behavior and keeps the data frame’s state clean for downstream analysis.\n\n\nNote that when performing the merge we used a filter() to exclude aggregate regions like “World” or regional aggregates from the World Bank data. But even after doing this we will see that there are still some countries that do not have V-Dem codes. This is fine. It just meansthat V-Dem does not cover all countries, especially smaller ones or those with limited data availability. When we perform our left join, these will drop out. We could also load tidyr and use the drop_na() function to remove these rows, but we will leave them in for now.\nNow let’s go ahead and fetch the V-Dem democracy scores for the same countries and years. We will use the fetchdem() function from the vdemlite package to get the V-Dem democracy scores, specifically the polyarchy score (v2x_polyarchy) and the fair elections score (v2xel_frefair). We will then rename these columns to make them more intuitive and then group and summarize the data so that we have one set of democracy scores per country.\n\nlibrary(vdemlite)\n\n# Fetch democracy scores for the same countries and years as wbdata\ndemocracy_scores &lt;- fetchdem(\n  indicators = c(\"v2x_polyarchy\", \"v2xel_frefair\"),\n  start_year = 2000, end_year = 2020\n) |&gt;\n  rename(\n    polyarchy = v2x_polyarchy,\n    fair_elections = v2xel_frefair\n  ) |&gt;\n  group_by(country_id) |&gt; \n  summarize(\n    polyarchy = mean(polyarchy, na.rm = TRUE),\n    fair_elections = mean(fair_elections, na.rm = TRUE)\n  )\n\nglimpse(democracy_scores)\n\nRows: 179\nColumns: 3\n$ country_id     &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 19, 20…\n$ polyarchy      &lt;dbl&gt; 0.6599524, 0.7645238, 0.9137143, 0.8975238, 0.7366667, …\n$ fair_elections &lt;dbl&gt; 0.7781429, 0.8423810, 0.9628571, 0.9379524, 0.7590000, …\n\n\nNow we can perform a left join to combine the two datasets based on the V-Dem country codes and the year. We will use the left_join() function from dplyr to do this. Let’s be sure to put the V-Dem data set as our left dataset, since we want to keep all of the V-Dem data and only add the World Bank data where it matches.\n\n# Perform a left join to combine the datasets\nwb_democracy_data &lt;- left_join( \n  democracy_scores, \n  wbdata_all_countries,\n  by = c(\"country_id\" = \"vdem_country_code\")\n)\n\nglimpse(wb_democracy_data)\n\nRows: 179\nColumns: 7\n$ country_id     &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 19, 20…\n$ polyarchy      &lt;dbl&gt; 0.6599524, 0.7645238, 0.9137143, 0.8975238, 0.7366667, …\n$ fair_elections &lt;dbl&gt; 0.7781429, 0.8423810, 0.9628571, 0.9379524, 0.7590000, …\n$ iso3c          &lt;chr&gt; \"MEX\", \"SUR\", \"SWE\", \"CHE\", \"GHA\", \"ZAF\", \"JPN\", \"MMR\",…\n$ region         &lt;chr&gt; \"Latin America & Caribbean\", \"Latin America & Caribbean…\n$ gdp_pc         &lt;dbl&gt; 9681.1700, 7938.9413, 48091.5799, 80543.6345, 1457.1354…\n$ population     &lt;dbl&gt; 113311064.9, 549054.4, 9459610.6, 7863746.1, 25596248.3…\n\n\nNotice here that while we had a common key for the country codes, the columns had different names. Therefore, we specified the by argument as a named vector, where we matched the vdem_country_code from the World Bank data to the country_code in the V-Dem data.\nNow let’s have a little bit of fun and try a visualizing the relationship between GDP per capita and the V-Dem polyarchy score using ggplot2:\n\nlibrary(ggplot2)\n\nggplot(wb_democracy_data, aes(x = gdp_pc, y = polyarchy, size = population, color = region)) +\n  geom_point(alpha = 0.5) +\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  scale_size_continuous( # options for customizing population legend\n    name = \"Population\",\n    breaks = c(1e6, 1e7, 5e7, 1e8, 5e8, 1e9),  \n    labels = c(\"1M\", \"10M\", \"50M\", \"100M\", \"500M\", \"1B\")\n  ) +\n  labs(\n    title = \"Wealth and Democracy, 2000 - 2020\",\n    x = \"GDP per Capita\",\n    y = \"V-Dem Polyarchy Score\",\n    color = \"Region\"\n  ) +\n  theme_minimal() +\n  scale_colour_viridis_d(option = \"plasma\", end = 0.9) \n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nUse the WDI() function to download a set of economic indicators for a country or set of countries.\nUse the countrycode package to convert the country codes in your World Bank data to V-Dem codes.\nUse the fetchdem() function to download V-Dem democracy scores for the same countries and years as your World Bank data.\nUse left_join() to merge the two datasets based on the V-Dem country codes and year.\nVisualize the relationship between one of your economic indicators and a V-Dem democracy score using ggplot2."
  },
  {
    "objectID": "modules/module-4.1.html",
    "href": "modules/module-4.1.html",
    "title": "Module 4.1",
    "section": "",
    "text": "Statistical modeling is one of the most powerful tools in data science. We use models for two primary purposes: to explore relationships between variables and to make predictions. Sometimes we are interested in understanding causal relationships (Does oil wealth impact regime type?), while other times we focus on predictive accuracy (Where is violence most likely to happen during an election? Is this email spam?).\nIn this module, we will explore linear regression, one of the foundational techniques in statistical modeling. We will learn how to quantify relationships between variables using correlation coefficients, fit linear models to data, and interpret the results. Throughout, we will work with real data examining the relationship between a country’s wealth and its level of democracy.\nBy the end of this module, you will be able to calculate correlations, fit simple linear regression models, and interpret the coefficients in meaningful ways. You will also understand the crucial distinction between correlation and causation."
  },
  {
    "objectID": "modules/module-4.1.html#overview",
    "href": "modules/module-4.1.html#overview",
    "title": "Module 4.1",
    "section": "",
    "text": "Statistical modeling is one of the most powerful tools in data science. We use models for two primary purposes: to explore relationships between variables and to make predictions. Sometimes we are interested in understanding causal relationships (Does oil wealth impact regime type?), while other times we focus on predictive accuracy (Where is violence most likely to happen during an election? Is this email spam?).\nIn this module, we will explore linear regression, one of the foundational techniques in statistical modeling. We will learn how to quantify relationships between variables using correlation coefficients, fit linear models to data, and interpret the results. Throughout, we will work with real data examining the relationship between a country’s wealth and its level of democracy.\nBy the end of this module, you will be able to calculate correlations, fit simple linear regression models, and interpret the coefficients in meaningful ways. You will also understand the crucial distinction between correlation and causation."
  },
  {
    "objectID": "modules/module-4.1.html#understanding-relationships-between-variables",
    "href": "modules/module-4.1.html#understanding-relationships-between-variables",
    "title": "Module 4.1",
    "section": "Understanding Relationships Between Variables",
    "text": "Understanding Relationships Between Variables\nWhen we build statistical models, we need to distinguish between different types of variables. The response variable (also called the dependent variable, outcome variable, target, or Y variable) is what we are trying to understand or predict. The explanatory variables (also called independent variables, predictors, features, or X variables) are what we use to explain variation in the response.\n\n’&gt;}}\nFor example, if we want to understand what factors influence a country’s level of democracy, democracy would be our response variable. Potential explanatory variables might include GDP per capita, education levels, natural resource wealth, or historical factors.\nLet’s examine a real-world example using data on countries’ wealth and democratic institutions.\nExample: GDP per Capita and Democracy\nWe’ll use data from the Varieties of Democracy (V-Dem) project, which provides comprehensive measures of democratic institutions around the world.\n\nlibrary(tidyverse)\nlibrary(vdemlite)\n\n# Load V-Dem data for 2019\nmodel_data &lt;- fetchdem(indicators = c(\"v2x_libdem\", \"e_gdppc\", \"v2cacamps\"),\n                       start_year = 2019, end_year = 2019) |&gt;\n  rename(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc,\n    polarization = v2cacamps\n    ) |&gt;\n  filter(!is.na(lib_dem), !is.na(wealth))\n\nglimpse(model_data)\n\nRows: 174\nColumns: 7\n$ country         &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\"…\n$ country_text_id &lt;chr&gt; \"MEX\", \"SUR\", \"SWE\", \"CHE\", \"GHA\", \"ZAF\", \"JPN\", \"MMR\"…\n$ country_id      &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 19, 2…\n$ year            &lt;dbl&gt; 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, 2019, …\n$ lib_dem         &lt;dbl&gt; 0.434, 0.580, 0.871, 0.866, 0.615, 0.607, 0.755, 0.260…\n$ wealth          &lt;dbl&gt; 16.814, 11.752, 48.804, 56.110, 5.608, 11.345, 39.061,…\n$ polarization    &lt;dbl&gt; 1.643, -0.631, -1.784, -1.543, -0.615, 0.204, -1.340, …\n\n\nLet’s visualize the relationship between these two variables:\n\nggplot(model_data, aes(x = wealth, y = lib_dem)) +\n  geom_point(color = \"steelblue\") +\n  geom_smooth(method = \"lm\", color = \"#E48957\", se = FALSE) +\n  scale_x_log10(labels = scales::label_dollar(suffix = \"k\")) +\n  labs(\n    title = \"Wealth and Democracy, 2019\",\n    x = \"GDP per capita (log scale)\", \n    y = \"Liberal Democracy Index\",\n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHere we use scale_x_log10() to transform the x-axis to a logarithmic scale. This is often useful when dealing with variables that are not normally distributed or that span several orders of magnitude. Later, when we analyze the relationship between GDP and other variables, we will take a log transformation of it (for the same reason). To do this we will call the log() function on the wealth variable in our model data, e.g. log(model_data$wealth).\n\n\nWhat do you notice about this relationship? There appears to be a positive association between wealth and democracy; countries with higher GDP per capita tend to have higher democracy scores. At the same time there are a number of outliers in the southwest corner of the plot. These represent wealthy countries that are not very democratic. A lot of these tend to be oil rich states like Saudi Arabia and the United Arab Emirates."
  },
  {
    "objectID": "modules/module-4.1.html#correlation-measuring-linear-relationships",
    "href": "modules/module-4.1.html#correlation-measuring-linear-relationships",
    "title": "Module 4.1",
    "section": "Correlation: Measuring Linear Relationships",
    "text": "Correlation: Measuring Linear Relationships\nBefore we fit a formal model, let’s quantify the strength of the linear relationship using a correlation coefficient. The correlation coefficient (often denoted as r) measures the strength and direction of a linear relationship between two quantitative variables.\nCorrelation coefficients range from -1 to +1:\n\n\nr = +1: Perfect positive linear relationship\n\nr = 0: No linear relationship\n\n\nr = -1: Perfect negative linear relationship\n\n|r| &gt; 0.7: Strong linear relationship\n\n0.3 &lt; |r| &lt; 0.7: Moderate linear relationship\n\n\n|r| &lt; 0.3: Weak linear relationship\n\nThere are several ways to calculate correlations in R. The base R function cor() provides a quick way:\n\n# Using base R - note we need to use log(wealth) to match our plot\ncor(log(model_data$wealth), model_data$lib_dem)\n\n[1] 0.5295884\n\n\nWe can also use tidyverse approaches for more complex analyses:\n\n# Using tidyverse approach with summarize\nmodel_data |&gt;\n  summarize(\n    correlation = cor(polarization, lib_dem),\n    correlation_rounded = round(correlation, 3)\n  )\n\n  correlation correlation_rounded\n1  -0.4267386              -0.427\n\n\nThe correlation of approximately 0.53 indicates a moderate positive linear relationship between log GDP per capita and democracy levels. The relationship would likely be stronger if it were not for the outliers we saw in the plot.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nCalculate the correlation between democracy and the polarization variable. How does it compare to the correlation using logged wealth?"
  },
  {
    "objectID": "modules/module-4.1.html#linear-models-as-functions",
    "href": "modules/module-4.1.html#linear-models-as-functions",
    "title": "Module 4.1",
    "section": "Linear Models as Functions",
    "text": "Linear Models as Functions\n\n’&gt;}}\nWe can represent the relationship between variables using mathematical functions. A function describes the relationship between an output and one or more inputs - plug in the inputs and receive back the output.\nFor a simple linear model with one explanatory variable, we write:\n\\[Y = a + bX\\]\nWhere:\n\n\n\\(Y\\) is the response variable (outcome, target)\n\n\\(X\\) is the explanatory variable (predictor)\n\n\n\\(a\\) is the intercept (predicted value of Y when X = 0)\n\n\\(b\\) is the slope (change in Y for a one-unit change in X)\n\nRunning a Linear Model in R\nLet’s fit a linear model to our democracy and wealth data using the base R lm() function. Since we saw that the relationship looks more linear when we log-transform wealth, we’ll use that approach. We will then use the summary() function to display the model results.\n\n# Run the model\ndemocracy_model &lt;- lm(lib_dem ~ log(wealth), data = model_data)\n\n# Display model results\nsummary(democracy_model)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,    Adjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n\n\nOur fitted model is:\n\\[Democracy = 0.13 + 0.12 × log(wealth)\\]\nModel Interpretation\nLet’s break down what these coefficients mean:\nIntercept (a = 0.13): This is the predicted democracy level when log(wealth) = 0. Since log(wealth) = 0 when wealth = $1, this represents the predicted democracy score for a country with $1 GDP per capita - essentially a theoretical baseline.\nSlope (b = 0.12): This is the key coefficient for interpretation. For every one-unit increase in log(wealth), we predict democracy to increase by 0.12 points.\nWhen we use logarithmic transformations, interpretation requires special care. When we change from one wealth level to another, we need to calculate the difference in their logarithms and multiply by our slope coefficient.\nFor any percentage increase in GDP per capita, we multiply our slope (0.12) by the natural log of the multiplier. For example, a 10% increase means multiplying by 1.1, doubling means multiplying by 2, tripling means multiplying by 3, and so on:\n\nA 10% increase (e.g., from $10,000 to $11,000) increases the democracy score by 0.0114 points since \\(0.12 \\times \\ln(1.1) \\approx 0.12 \\times 0.0953 = 0.0114\\)\nDoubling GDP per capita (e.g., $10,000 → $20,000) increases the democracy score by \\(0.12 \\times \\ln(2) \\approx 0.12 \\times 0.693 = 0.083\\)\nTripling GDP per capita (e.g., $10,000 → $30,000) increases the democracy score by: \\(0.12 \\times \\ln(3) \\approx 0.12 \\times 1.099 = 0.132\\)\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nUse the model we just ran to make predictions for specific countries or wealth levels, e.g. $50,000 per capita, $100,000 per capita, etc.\nTry regressing democracy on polarization. Interpret the model coefficients for different levels of polarization. (Note that this variable is not logged.)"
  },
  {
    "objectID": "modules/module-4.1.html#understanding-predicted-values-and-residuals",
    "href": "modules/module-4.1.html#understanding-predicted-values-and-residuals",
    "title": "Module 4.1",
    "section": "Understanding Predicted Values and Residuals",
    "text": "Understanding Predicted Values and Residuals\nEvery model produces predicted values - these are the outputs from our model function given specific input values (like the ones we calculate earlier for our wealth and democracy model). We often write these as Ŷ (Y-hat) to distinguish them from observed values.\nResiduals measure how far each observation is from its predicted value:\n\\[Residual = Observed Value (Y) - Predicted Value (\\hat{Y})\\]\nLet’s visualize this:\n\n\n\n\n\n\n\n\nCountries above the line have positive residuals (higher democracy than predicted by wealth alone), while countries below the line have negative residuals (lower democracy than predicted)."
  },
  {
    "objectID": "modules/module-4.1.html#model-performance-and-limitations",
    "href": "modules/module-4.1.html#model-performance-and-limitations",
    "title": "Module 4.1",
    "section": "Model Performance and Limitations",
    "text": "Model Performance and Limitations\nHow well does our model perform? We can assess this using R-squared, which tells us what proportion of variation in democracy is explained by wealth. If we call summary again on our model, we can see this value:\n\nsummary(democracy_model)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,    Adjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n\n\nThe R-squared value is approximately 0.28, meaning that about 28% of the variation in democracy scores can be explained by log GDP per capita. This is a reasonable result for a social science model, but it also indicates that there are many other factors influencing democracy that we have not accounted for. Approximately 72% of the variation remains unexplained by our model!\n\n\n\n\n\n\nCorrelation vs. Causation\n\n\n\nOur model shows a strong association between wealth and democracy, but this does not prove that wealth causes democracy. There are several possible explanations for this relationship:\n\n\nWealth → Democracy: Perhaps economic development creates conditions that support democratic institutions\n\nDemocracy → Wealth: Perhaps democratic institutions promote economic growth\n\n\nThird Variable: Perhaps other factors (education, culture, geography) influence both wealth and democracy\n\nReverse Causation: The relationship might work in both directions simultaneously\n\nEstablishing causation requires more sophisticated methods beyond simple correlation and regression, such as natural experiments, instrumental variables, or randomized controlled trials. In the context of linear regression, causal identification is beyond the scope of this course."
  },
  {
    "objectID": "modules/module-4.1.html#conclusion",
    "href": "modules/module-4.1.html#conclusion",
    "title": "Module 4.1",
    "section": "Conclusion",
    "text": "Conclusion\nLinear regression is a foundational tool in data science, but it’s just the beginning. In future modules, we’ll explore multiple regression (with several explanatory variables), non-linear relationships, and more sophisticated modeling techniques."
  },
  {
    "objectID": "modules/module-3.1.html",
    "href": "modules/module-3.1.html",
    "title": "Module 3.1",
    "section": "",
    "text": "Prework\n\n\n\nClick on Code toggle below to unfold the setup code chunk. Then, copy and run the code in your Quarto notebook to load the necessary packages and create the data frame for this lesson.\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\nvdem2022 &lt;- fetchdem(indicators = c(\n    \"v2x_polyarchy\",\n    \"v2x_gender\",\n    \"v2cacamps\",\n    \"v2x_regime\",\n    \"e_regionpol_6C\"\n    ),\n    start_year = 2022, \n    end_year = 2022) |&gt;\n  rename(\n    country = country_name, \n    polyarchy = v2x_polyarchy, \n    women_empowerment = v2x_gender,\n    polarization = v2cacamps,\n    regime = v2x_regime, \n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = case_match(region, \n                     1 ~ \"Eastern Europe\", \n                     2 ~ \"Latin America\",  \n                     3 ~ \"Middle East\",   \n                     4 ~ \"Africa\", \n                     5 ~ \"The West\", \n                     6 ~ \"Asia\"),\n    regime = case_match(regime,\n                    0 ~ \"Closed Autocracy\",\n                    1 ~ \"Electoral Autocracy\",\n                    2 ~ \"Electoral Democracy\",\n                    3 ~ \"Liberal Democracy\")\n  )\n\n#glimpse(vdem2022)"
  },
  {
    "objectID": "modules/module-3.1.html#overview",
    "href": "modules/module-3.1.html#overview",
    "title": "Module 3.1",
    "section": "Overview",
    "text": "Overview\nIn this module, we explore how to work with categorical data, focusing on how to classify, summarize, and visualize it effectively. We begin by discussing the different types of data—categorical vs. numerical, discrete vs. continuous, and so on—and why it matters to distinguish among them. Then, using democracy indicators from the V-Dem dataset, we learn how to visualize distributions of categorical variables with geom_bar(). Finally, we take our analysis further by examining how regime types vary by world region, introducing the distinction between nominal and ordinal categorical variables and learning how to create comparative bar plots using proportions. Along the way, you’ll have opportunities to apply what you’ve learned through hands-on coding and interpretation."
  },
  {
    "objectID": "modules/module-3.1.html#what-kind-of-data-do-we-have",
    "href": "modules/module-3.1.html#what-kind-of-data-do-we-have",
    "title": "Module 3.1",
    "section": "What Kind of Data Do We Have?",
    "text": "What Kind of Data Do We Have?\nBefore we can summarize or model any dataset, we need to pause and reflect on what kind of data we’re actually dealing with. This is an important consideration because how we classify our data shapes the types of questions we can ask, the summaries we can produce, and the visualizations we can make.\nData can differ in many important ways. One useful distinction is between anecdotal and representative data. Anecdotal data might come from a single experience or a small number of observations, like a friend’s story about a trip or a journalist’s report on a protest. These can be powerful or evocative, but they don’t necessarily generalize. In contrast, representative data are collected systematically with an eye toward capturing a larger population or phenomenon. For example, a random sample of households in a country tells us much more about living conditions overall than a few interviews with individual families.\nAnother key distinction is between census and sample data. A census tries to gather data on the entire population—think of the U.S. Census or a complete list of all registered voters. Most often, though, we work with samples: smaller, more manageable subsets of data drawn from a larger group. Whether we’re looking at a dozen countries or a thousand people, it’s important to understand how that sample was drawn and what population it represents.\nWe also need to be clear on whether our data come from an observational or an experimental study. Observational data are collected without interfering with the system we’re studying—just watching and recording. Experimental data, on the other hand, involve interventions or treatments, like randomly assigning people to different programs and comparing outcomes.\nOne of the most fundamental distinctions—and the focus of this module—is between categorical and numerical variables. Categorical variables place observations into groups, like types of political regimes or preferred news sources. Numerical variables are measured on a scale, such as GDP per capita or number of protests.\nAmong numerical variables, we often distinguish between discrete variables, which take whole number values (like counts), and continuous variables, which can take on any value in a range (like income or temperature).\nThe way we collect data also matters. Are we working with a cross-sectional snapshot, showing one moment in time? Or is it a time series, tracking changes over time? In this class, we will be focused primarily on cross-sectional data, but it is important to realize when you are looking at a dataset that incorporates a time-series dimension.\nFinally, not all data come in neat rows and columns. In this class we work with structured data: spreadsheets, tables, and rectangular data frames. But data can also be unstructured, like text, images, or videos, which require special tools to analyze.\nUnderstanding these distinctions gives us a foundation for exploring the types of variables we encounter in political, social, and economic datasets.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nClassify the following variables based on the distinctions we’ve just covered:\n\nIs a country a democracy? (yes/no)\nPolity score (ranges from -10 to 10)\nV-Dem Polyarchy index (0 to 1)\nV-Dem Regimes of the World classification (closed autocracy, electoral autocracy, etc.)\nNumber of protest events\nProtest types (sit-in, march, strike, etc.)\n\nThink about whether each variable is categorical or numerical, and if so, what kind. Can you spot any that might be tricky to classify?"
  },
  {
    "objectID": "modules/module-3.1.html#exploring-categorical-data",
    "href": "modules/module-3.1.html#exploring-categorical-data",
    "title": "Module 3.1",
    "section": "Exploring Categorical Data",
    "text": "Exploring Categorical Data\nLet’s now take a closer look at categorical variables by examining a real-world dataset. One commonly used measure of democracy is V-Dem’s Regimes of the World classification. This variable categorizes countries into four types:\n\nClosed Autocracy\nElectoral Autocracy\nElectoral Democracy\nLiberal Democracy\n\nThese categories are mutually exclusive and ordered from least to most democratic, making this an ordinal categorical variable.\nLet’s use the data from the prework section of this module to explore the distribution of these regime types across the world in 2022. We can start by just glimpsing the data to see what we have:\n\nglimpse(vdem2022)\n\nRows: 179\nColumns: 9\n$ country           &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghan…\n$ country_text_id   &lt;chr&gt; \"MEX\", \"SUR\", \"SWE\", \"CHE\", \"GHA\", \"ZAF\", \"JPN\", \"MM…\n$ country_id        &lt;dbl&gt; 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 19,…\n$ year              &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022…\n$ polyarchy         &lt;dbl&gt; 0.571, 0.775, 0.896, 0.897, 0.664, 0.711, 0.817, 0.0…\n$ women_empowerment &lt;dbl&gt; 0.773, 0.834, 0.932, 0.942, 0.823, 0.839, 0.794, 0.3…\n$ polarization      &lt;dbl&gt; 1.586, -0.625, -1.486, -1.268, -0.615, 0.411, -1.715…\n$ regime            &lt;chr&gt; \"Electoral Democracy\", \"Electoral Democracy\", \"Liber…\n$ region            &lt;chr&gt; \"Latin America\", \"Latin America\", \"The West\", \"The W…\n\n\nTo begin summarizing the distribution of regimes, we can use the count() function:\n\nvdem2022 |&gt;\n  count(regime)\n\n               regime  n\n1    Closed Autocracy 33\n2 Electoral Autocracy 54\n3 Electoral Democracy 58\n4   Liberal Democracy 34\n\n\nThis gives us a frequency table showing how many countries fall into each regime category. But sometimes, a table doesn’t give us the full picture. Visualizations can help us better see patterns and communicate them clearly. Let’s create a bar plot using the geom_bar() geom:\n\nvdem2022 |&gt;\n  ggplot(aes(x = regime)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    x = \"Regime\",\n    y = \"Frequency\",\n    title = \"Regimes of the World in 2022\",\n    caption = \"Source: V-Dem Institute\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nThis simple bar plot provides an immediate visual summary of the global distribution of regime types in 2022. We see that electoral democracies are the most common regime type, followed by electoral autocracies, with closed autocracies being the least common.\nThe plot highlights the usefulness of geom_bar() as a straightforward way to create bar plots in ggplot2 when working with categorical variables. Unlike geom_col(), which requires both x and y aesthetics (typically used when you already have counts or proportions computed), geom_bar() automatically calculates the heights of the bars for you. It counts the number of occurrences of each category in the variable you supply to the x aesthetic.\nThis makes geom_bar() particularly convenient for quick summaries. It’s similar in spirit to geom_histogram(), which bins and counts continuous data, except that here, the binning is categorical. You only need to specify the variable for the x-axis—ggplot2 handles the y-axis internally based on the counts.\nFrom here, we can begin asking deeper questions such as whether certain regime types are more common in some regions than others, or how these distributions change over time. We’ll explore these questions as we move forward.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nExpore the distribution of regimes for a different year\nPreprocess your data to include only the year you are interested in\nVisualize the distribution of regimes using geom_bar()\n\nUse the labs() function to change title\nWhat is different about the year that you chose relative to 2022?"
  },
  {
    "objectID": "modules/module-3.1.html#regimes-by-region",
    "href": "modules/module-3.1.html#regimes-by-region",
    "title": "Module 3.1",
    "section": "Regimes by Region",
    "text": "Regimes by Region\nNow that we’ve examined the overall distribution of regime types, let’s take things a step further and explore how these regimes vary across different parts of the world.\nTo do this, we can use a grouped bar plot by using geom_bar() with two categorical variables: region and regime type. Region will go on the x-axis, and regime type will be mapped to the fill aesthetic. This allows us to visualize how the distribution of political regimes differs from one world region to another.\n\nvdem2022 |&gt;\n  ggplot(aes(x = region, fill = regime)) +\n      geom_bar() +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Frequency\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)\n\n\n\n\n\n\n\nIt’s worth noting the types of categorical variables we’re using here. Regime type is an ordinal categorical variable—its categories follow a meaningful order, from closed autocracy to liberal democracy. Region, on the other hand, is a nominal categorical variable—its categories (like Asia, Latin America, or Africa) don’t have an inherent order.\nBy combining a nominal categorical variable with an ordinal one in a single plot, we can explore important patterns in the data, such as where the most liberal democracies are located or where autocratic regimes are most common.\nThis chart gives us a useful breakdown of regime types by region. However, one challenge immediately becomes apparent: the number of countries varies across regions. Africa, for example, has far more countries than Eastern Europe or the Middle East. So, even if every region had the same proportions of regime types, the bars would be taller in regions with more countries.\nTo address this, we can switch from raw counts to proportions using position = \"fill\" inside geom_bar(). This stacks the bars so that they all have the same height, and each segment reflects the proportion of countries in each regime category:\n\nvdem2022 %&gt;%\n  ggplot(., aes(x = region, fill = regime)) +\n      geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(\n    x = \"Region\",\n    y = \"Proportion\",\n    title = \"Regimes of the World by World Region in 2022\",\n    caption = \"Source: V-Dem Institute\",\n    fill = \"Regime\"\n  ) +\n  scale_fill_viridis_d(option = \"inferno\", end = .8)\n\n\n\n\n\n\n\nNow we can better compare regime distributions across regions, without having to worry about differences in the number of countries. For example, we might notice that liberal democracies are more prevalent in “The West,” while electoral democracies concentrate in Eastern Europe and electoral autocracies dominate in Africa.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nExplore the distribution of regimes by region for a different year:\n\nUse fetchdem to load data on regimes for a year that you are interested\nVisualize the distribution of regimes using geom_bar() and position = \"fill\"\n\nUse the labs() function to change the plot title\nWhat’s different about the year you chose compared to 2022?"
  },
  {
    "objectID": "modules/module-1.2.html",
    "href": "modules/module-1.2.html",
    "title": "Module 1.2",
    "section": "",
    "text": "Prework\n\n\n\n\nInstall R, R Studio and the Tidyverse collection of packages if you have not done so already (see getting started).\nRead this markdown guide.\nStart a project folder for module 1 in the way that we learned in the getting started module. You can call it module-1 or whatever you like, but try to use a consistent naming convention like kebab-case, snake_case or camelCase.\nCreate a new Quarto document in your project folder and name it module-1.2.qmd. Use that to take notes and do the exercise at the end of the module."
  },
  {
    "objectID": "modules/module-1.2.html#overview",
    "href": "modules/module-1.2.html#overview",
    "title": "Module 1.2",
    "section": "Overview",
    "text": "Overview\nIn this module, we’ll build on your first experience with Quarto documents and go deeper into effective authoring and project organization. By the end, you should feel comfortable creating reproducible reports that include well-organized text, code, and outputs."
  },
  {
    "objectID": "modules/module-1.2.html#why-project-oriented-workflows-matter",
    "href": "modules/module-1.2.html#why-project-oriented-workflows-matter",
    "title": "Module 1.2",
    "section": "Why Project-Oriented Workflows Matter",
    "text": "Why Project-Oriented Workflows Matter\nWhen you’re just starting out, it’s tempting to save your files wherever is convenient—maybe the Desktop, maybe the Downloads folder. But as your analysis grows more complex or involves collaborators, maintaining an organized file structure becomes essential. A good Quarto project keeps all your materials—data, scripts, and reports—in a consistent, portable format. This not only helps avoid broken file paths, but also supports reproducibility, sharing, and long-term clarity.\nA typical layout might include your main .qmd document, a data/ folder for input files, a separate folder for images or plots, and a designated location for rendered outputs. Keeping everything bundled in a single project folder also makes it easier to track changes and collaborate via Git or GitHub."
  },
  {
    "objectID": "modules/module-1.2.html#yaml-front-matter-revisited",
    "href": "modules/module-1.2.html#yaml-front-matter-revisited",
    "title": "Module 1.2",
    "section": "YAML Front Matter Revisited",
    "text": "YAML Front Matter Revisited\nYou’ve already encountered the YAML block at the top of a Quarto document. Now let’s take a closer look at what it does.\nThe YAML front matter sets key metadata and document options. For example:\ntitle: \"My Report\"\nauthor: \"Your Name\"\ndate: today\nformat: html\nexecute:\n  echo: false\n  message: false\nHere, the title, author, and date fields define the document header. The format field tells Quarto what type of file to create—HTML in this case. The execute section controls global options for code chunks. In this example, code won’t be shown (echo: false) and messages will be suppressed (message: false). These defaults apply to all chunks in the document, though individual chunks can override them."
  },
  {
    "objectID": "modules/module-1.2.html#authoring-with-markdown",
    "href": "modules/module-1.2.html#authoring-with-markdown",
    "title": "Module 1.2",
    "section": "Authoring with Markdown",
    "text": "Authoring with Markdown\nQuarto documents are written in Markdown, a lightweight markup language designed for readability. You don’t need to remember complex formatting commands. To create section headings, you simply use hash signs—# for the main title, ## for sections, and so on. Italics and bold are created with asterisks, like *italic* and **bold**.\n\nYou can also create numbered or unnumbered lists by just typing them directly, and you can include links with [text](https://quarto.org) syntax. Images work the same way—just add an exclamation mark in front, like ![](images/myplot.png). The goal is to keep your writing clear and uncluttered while supporting rich formatting features."
  },
  {
    "objectID": "modules/module-1.2.html#code-chunks-in-depth",
    "href": "modules/module-1.2.html#code-chunks-in-depth",
    "title": "Module 1.2",
    "section": "Code Chunks in Depth",
    "text": "Code Chunks in Depth\nOne of the key advantages of Quarto is the ability to weave together code and prose. You can insert a code chunk by typing three backticks and specifying the language, like this:\n\n```{r}\nsummary(cars)\n```\n\nThere are also a couple of shortcuts you can use to create code chunks in RStudio. You can use the keyboard shortcut Ctrl + Alt + I (or Cmd + Option + I on Mac) to insert a code chunk, or you can go to the menu bar and select on this icon:\n\n\n\n\nYou can also control how chunks behave using chunk options. Quarto supports a special #| syntax that lets you configure each chunk individually. For instance, you can use #| echo: to control whether the code is displayed along with the output, or #| label: to add a label to a code chunk (labels can be useful if you want to identify where an error is coming from or to reuse code from a chunk).\n\n\n\n\nIn this example, we’ve given the chunk a label (mtcars_plot) and hidden the code from the final output using echo: false. Notice how in the rendered version the first chunk shows the code but the second and third do not. This is useful when you want to show the results—like a plot—without showing the code that generated it. You can also hide messages, warnings, and even outputs in a similar way.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nLet’s put everything together in a small project. Start by creating a new folder and placing a new Quarto document inside it. Here is an illustration to remind you how to do that (for a fuller explanation, review the Getting Started module).\n\n\n\n\nWrite a few short paragraphs of narrative text and include a code chunk that generates a plot with ggplot2. If you have images or data files, place them in subfolders and link to them using relative paths.\nOnce you’ve written your report, render the document to HTML. Open the result in your browser and make sure everything displays correctly. Along the way, try adjusting chunk options to control what appears in the final output. If you run into rendering errors, use the error messages to debug step-by-step—this is a valuable part of learning."
  },
  {
    "objectID": "modules/module-1.2.html#final-tips",
    "href": "modules/module-1.2.html#final-tips",
    "title": "Module 1.2",
    "section": "Final Tips",
    "text": "Final Tips\nAs a rule of thumb, always use relative file paths inside your Quarto projects. This means referring to files based on their position in the folder structure, not their full computer location. It helps keep your project portable so that it runs smoothly on any machine, whether yours or a collaborator’s.\nWhen in doubt, label your chunks, write clearly, and remember that Quarto documents are meant to evolve. The more you write and render, the more confident you’ll become."
  },
  {
    "objectID": "modules/module-1.1.html",
    "href": "modules/module-1.1.html",
    "title": "Module 1.1",
    "section": "",
    "text": "All of our work for this course will be done in the R language and we will be working with R in RStudio. RStudio is an integrated development environment (IDE) develop by a company named Posit. Please be sure to download and install the most recent versions of R and R Studio from Posit’s website.\nIt is a good idea to periodically update R and RStudio. RStudio will prompt you when it is time to update and you can follow the same process of downloading and installing from the Posit website that we just did here. For R, there are a number of ways to do it, but the easiest is to use packages like installr for Windows and updateR for Mac. Here is a good blog post that walks you through the steps of how to update R using these packages. I usually update R once a semester.\nWe are going to be using a number of R packages throughout the course. One essential set of packages are those that comprise the Tidyverse, but especially readr, dplyr, ggplot2 and tidyr. You can install the entire Tidyverse collection of packages by typing install.packages(\"tidyverse\") in your console. We will talk about these packages in detail as we go through the course, but have a look at this basic description now to gain some basic familiarity.\nAnother thing that you really want to do is to make sure that you have the native pipe operator (|&gt;) enabled. In RStudio, go to Tools&gt;Global Options, then go to Code and select “Use native pipe operator.”\nWhile you are here, you can also go to Appearance to select a different editor theme or to Pane Layout to change how the four panes in R Studio are organized. Next, familiarize yourself with how to expand and minimize the four windows. The most important window that I want to highlight here is the source window. This is where we are going to be working most of the time in this course. And if I tell you to send your source code, I mean to send the file that you are working on in this window. This could be a Quarto document, an R script or an app.R file for a Shiny app.\nThe next window is the Console and there we also see tabs for Terminal and Background Jobs. The console is where you can run R code one line at a time. The terminal is relevant for more advanced users and we will make some use of it when we talk about publishing Quarto documents. Background Jobs is going to be helpful when we want troubleshoot a Quarto document that is not rendering properly.\nFrom there, the next pane we want to explore is Environment, History, etc. Environment tells us what files are currently available to us. The other important tab here is Git which will be where we push things to GitHub. You will be using this a lot in the course.\nFinally, we see a pane with Files, Plots, Packages etc. Files tells us what files are in our project folder and enables us to copy, and delete files associated with our project. Plots is a window for viewing our visualizations. And Packages shows us what packages are available and loaded into our environment.\nBefore you move on to the next section, take some time to familiarize yourself with the various user-friendly buttons and shortcuts available to you like the drop down menu for the pane layout, a spell checker, a button for inserting a code chunk and other features that you can play around with."
  },
  {
    "objectID": "modules/module-1.1.html#sec-rstudio-setup",
    "href": "modules/module-1.1.html#sec-rstudio-setup",
    "title": "Module 1.1",
    "section": "",
    "text": "All of our work for this course will be done in the R language and we will be working with R in RStudio. RStudio is an integrated development environment (IDE) develop by a company named Posit. Please be sure to download and install the most recent versions of R and R Studio from Posit’s website.\nIt is a good idea to periodically update R and RStudio. RStudio will prompt you when it is time to update and you can follow the same process of downloading and installing from the Posit website that we just did here. For R, there are a number of ways to do it, but the easiest is to use packages like installr for Windows and updateR for Mac. Here is a good blog post that walks you through the steps of how to update R using these packages. I usually update R once a semester.\nWe are going to be using a number of R packages throughout the course. One essential set of packages are those that comprise the Tidyverse, but especially readr, dplyr, ggplot2 and tidyr. You can install the entire Tidyverse collection of packages by typing install.packages(\"tidyverse\") in your console. We will talk about these packages in detail as we go through the course, but have a look at this basic description now to gain some basic familiarity.\nAnother thing that you really want to do is to make sure that you have the native pipe operator (|&gt;) enabled. In RStudio, go to Tools&gt;Global Options, then go to Code and select “Use native pipe operator.”\nWhile you are here, you can also go to Appearance to select a different editor theme or to Pane Layout to change how the four panes in R Studio are organized. Next, familiarize yourself with how to expand and minimize the four windows. The most important window that I want to highlight here is the source window. This is where we are going to be working most of the time in this course. And if I tell you to send your source code, I mean to send the file that you are working on in this window. This could be a Quarto document, an R script or an app.R file for a Shiny app.\nThe next window is the Console and there we also see tabs for Terminal and Background Jobs. The console is where you can run R code one line at a time. The terminal is relevant for more advanced users and we will make some use of it when we talk about publishing Quarto documents. Background Jobs is going to be helpful when we want troubleshoot a Quarto document that is not rendering properly.\nFrom there, the next pane we want to explore is Environment, History, etc. Environment tells us what files are currently available to us. The other important tab here is Git which will be where we push things to GitHub. You will be using this a lot in the course.\nFinally, we see a pane with Files, Plots, Packages etc. Files tells us what files are in our project folder and enables us to copy, and delete files associated with our project. Plots is a window for viewing our visualizations. And Packages shows us what packages are available and loaded into our environment.\nBefore you move on to the next section, take some time to familiarize yourself with the various user-friendly buttons and shortcuts available to you like the drop down menu for the pane layout, a spell checker, a button for inserting a code chunk and other features that you can play around with."
  },
  {
    "objectID": "modules/module-1.1.html#quarto",
    "href": "modules/module-1.1.html#quarto",
    "title": "Module 1.1",
    "section": "Quarto",
    "text": "Quarto\n\nOnce you have R, R Studio and Quarto installed, you are ready to start integrating text and code with Quarto. Quarto is an open source publishing platform that enables you to integrate text with code. If you have used R Markdown before then Quarto will look familiar to you because Quarto is the next generation of R Markdown.\nRStudio comes with a version of Quarto already installed, but it can be useful to install the most recent version separately and because doing so will allow you to use Quarto with another IDE like VS Code. You can install the most recent version of Quarto by visiting this page and selecting the version for your operating system.\nNow take a little time to create a Quarto project in R Studio and make sure everything is working properly. But before you get started, create a new folder(directory) for this course on your computer somewhere. Once that is done, go to File &gt; New Project. Then select Quarto Project and name the project something like “test-project” in the Directory name field. Next, select Browse and navigate to the folder that you created for this course. Select Create Project.\nYou will notice that in your new project folder there is a file with an .Rproj extension. The .Rproj file is what tells RStudio which files are associated with the project and it obviates the need to set the working directory. It also makes it possible to share the folder with anyone who is running R and RStudio and have them run your code without having to set a working directory. This is what we refer to as a project-based workflow and we will use it for every assignment in this class.\nNow try rendering the document with the Render toggle button. By default, Quarto renders an .html file that it will open in a browser and save to your project folder. Next we want to try rendering a .pdf document. To do this, we have to install tinytex, a lightweight version of LaTeX. To do this, go to the Terminal and type quarto install tinytex. Now, change the format from html to pdf by inserting format: pdf in the YAML header. Then render the document again. A .pdf document should open up.\nNow take a few minutes and try changing more of the code in the YAML header. You can try changing the title, adding a subtitle (subtitle:) or changing the execution options. By default, Quarto uses the visual editor but behind the scenes it is using Markdown. Try and edit some text using the toggle buttons available in the visual editor and then switch to Source to view the underlying Markdown code. Play with the R code chunks embedded in the document or try adding new code chunks.\nYou may already have some experience writing in Markdown, which is a lightweight markup language that enables you to format plaintext files. If you have not used Markdown, or if your memory is hazy, don’t worry: it is really easy to learn. Have a look at this Markdown cheat sheet and try to familiarize yourself with its basic syntax. Finally, take some time to get familiar with the Guide and Reference sections of the Quarto website. Then take a look at the gallery so that you can get an idea of the kinds of things you can produce with Quarto."
  },
  {
    "objectID": "modules/module-1.5.html",
    "href": "modules/module-1.5.html",
    "title": "Module 1.5",
    "section": "",
    "text": "Prework\n\n\n\n\nHave a look at the documentation for ggplot2\n\nFamiliarize yourself with the ggplot2 cheatseet\n\nGenerate a QMD file in your modules project folder so that you can code along with me and do the exercises.\n\nIf you have installed the Tidyverse, then you should already have the packages for this model, including ggplot2. You can go ahead and load ggplot2 along with readr and dplyr.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "modules/module-1.5.html#overview",
    "href": "modules/module-1.5.html#overview",
    "title": "Module 1.5",
    "section": "Overview",
    "text": "Overview\nIn the last module we learned about the Tidyverse. This week we are going to delve into the key package used to visualize data in the context of the Tidyverse–ggplot2. We will learn how to make bar charts (also known as column charts), histograms, line charts and scatter plots. Along the way we are going to be talking about the “grammar of graphics” that ggplot2 is based on."
  },
  {
    "objectID": "modules/module-1.5.html#the-grammar-of-graphics",
    "href": "modules/module-1.5.html#the-grammar-of-graphics",
    "title": "Module 1.5",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe “gg” in ggplot stands for “grammar of graphics”, which is a layered approach to constructing graphs based on a book by Leland Wilkinson.\nThe grammar of graphics is a framework for thinking about how to construct visualizations in a consistent and coherent way. The basic idea behind it is that data visualization has a language with its own grammar, and that the elements of this grammar can be combined in a systematic way to create a wide variety of visualizations.\nThe basic components of a ggplot2 visualization are the data that you want to visualize, the aesthetics (or dimensions) of the visualization, the geometric objects (or geoms) that you want to use to represent the data, and various other elements like color scales, themes, and annotations. As you will see, each of these elments is “added” onto the plot in a systematic way quite literally using the + sign."
  },
  {
    "objectID": "modules/module-1.5.html#which-visualization-do-i-use",
    "href": "modules/module-1.5.html#which-visualization-do-i-use",
    "title": "Module 1.5",
    "section": "Which Visualization Do I Use?",
    "text": "Which Visualization Do I Use?\nChoosing the right visualization depends on the type of question you’re asking and the structure of your data. If you’re looking at trends over time, such as stock prices, a line chart is typically most effective. To show the distribution of a variable, like income within a country, use a histogram, density plot, or boxplot. When comparing values across categories—such as female labor force participation across MENA countries—a bar chart works well. Finally, to explore relationships between two variables, like poverty and inequality across countries, a scatterplot is the appropriate choice."
  },
  {
    "objectID": "modules/module-1.5.html#bar-charts",
    "href": "modules/module-1.5.html#bar-charts",
    "title": "Module 1.5",
    "section": "Bar charts",
    "text": "Bar charts\n\nLet’s get started with our first visualization–a basic bar chart. Our aim here is going to be to summarize levels of democracy across different regions like we did in the last lesson, but this time we will illustrate the differences with a chart.\nWe will start by loading in the dem_summary.csv file that we made in the last lesson. You should already have it stored in your data subfolder for this module. But just in case here it is again:\n📥 Download dem_summary.csv\nNext we will use these data for our first ggplot() call. The ggplot() function takes two arguments: data and mapping. data refers to the data frame that includes the variables we want to visualize and mapping refers to the aesthetics mappings for the visualization. The aesthetics mappings are themselves presented in a quoting function aes() that defines the x and y values of the plot along with other aesthetic values like fill, color and linetype. We will focus on x and y values here and return to these additional aesthetic values later.\nAfter our ggplot() call, we can add a series of additional functions to define our visualization following a + sign. The most important group are the geoms which will define the basic type of plot we want to make. In this case, we are calling geom_col() for our histogram and specifying that the fill color should be “steelblue.”\nFrom there we will further customize our visualization with the labs() function to provide a title, axis labels and a caption.\n\ndem_summary &lt;- read_csv(\"data/dem_summary.csv\")\n\nggplot(dem_summary, aes(x = region, y = polyarchy)) + # ggplot call\n  geom_col(fill = \"steelblue\") + # we use geom_col() for a a bar chart\n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )\n\n\n\n\n\n\n\nThis looks like a pretty good columnn chart, but frequently we would want the bars of our bar chart to be sorted in order of the values being displayed. Let’s go ahead and add the reorder() function to our aes() call so that we are reordering the bars based on descending values of the average polyarchy score.\n\nggplot(dem_summary, aes(x = reorder(region, -polyarchy), y = polyarchy)) +\n  geom_col(fill = \"steelblue\") + \n  labs(\n    x = \"Region\", \n    y = \"Avg. Polyarchy Score\", \n    title = \"Democracy by region, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\n\nglimpse() the data\nFind a new variable and a new to visualize\nMake a bar chart with it\nChange the color of the bars\nOrder the bars on the x-axis according to the value of the y-axis variable\nAdd labels\nTry adding a theme\n\nTry saving your plot as an object\nThen change the labels and/or theme"
  },
  {
    "objectID": "modules/module-1.5.html#histograms",
    "href": "modules/module-1.5.html#histograms",
    "title": "Module 1.5",
    "section": "Histograms",
    "text": "Histograms\n\nNow let’s do another ggplot() call to make a histogram. We’ll start by reading in the dem_women.csv file from our previous lesson. Again, these should already be stored in the data subfolder for this module, but just in case:\n📥 Download dem_women.csv\nFor these data, we are going to have select one year to visualize because they feature a time series. We will use the filter() function from dplyr to select the year 2015. We use the pipe operator (|&gt;) to pass the data frame to the filter() function. We are going to cover the filter() verb and the pipe operator in more detail in the next module, but for now just know that filter() selects a column and the pipe operator allows us to pass the data frame to the next function without having to re-specify it.\nFrom there, we call ggplot(), specifying the polyarchy score on x-axis. But this time we change the geom to geom_histogram(). We also change the title and axis labels to reflect the fact that we are plotting the number of cases falling in each bin.\n\n\n\n\n\n\nCode Detail: Blank y-axis\n\n\n\nNote that we leave the y-axis blank for the histogram because ggplot will automatically know to plot the number of units in each bin on the y-axis.\n\n\n\ndem_women_2015 &lt;- read_csv(\"data/dem_women.csv\") |&gt; \n  filter(year == 2015) \n\nggplot(dem_women_2015, aes(x = polyarchy)) + # only specify x for histogram\n  geom_histogram(fill = \"steelblue\") + # geom is a histogram\n  labs(\n    x = \"Polyarchy Score, 2015\", \n    y = \"Count\",\n    title = \"Distribution of democracy, 2015\", \n    caption = \"Source: V-Dem Institute\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nPick a variable that you want to explore the distribution of1\n\nMake a histogram\n\nOnly specify x = in aes()\n\nSpecify geom as geom_histogram\n\n\n\nChoose color for bars\nChoose appropriate labels\nChange number of bins\nChange from count to density"
  },
  {
    "objectID": "modules/module-1.5.html#footnotes",
    "href": "modules/module-1.5.html#footnotes",
    "title": "Module 1.5",
    "section": "Footnotes",
    "text": "Footnotes\n\nNote that the data for GDP per capita spotty after 2015↩︎"
  },
  {
    "objectID": "modules/module-3.6.html",
    "href": "modules/module-3.6.html",
    "title": "Module 3.6",
    "section": "",
    "text": "In this module, you’ll learn how to test whether relationships exist between two variables using permutation tests. We’ll explore this through a case study examining potential racial discrimination in hiring practices, using data from résumés sent to employers with randomly assigned names. Here is a video that introduces the concept of using bootstrapping methods to evaluate discrimination but through the lense of gender:"
  },
  {
    "objectID": "modules/module-3.6.html#overview",
    "href": "modules/module-3.6.html#overview",
    "title": "Module 3.6",
    "section": "",
    "text": "In this module, you’ll learn how to test whether relationships exist between two variables using permutation tests. We’ll explore this through a case study examining potential racial discrimination in hiring practices, using data from résumés sent to employers with randomly assigned names. Here is a video that introduces the concept of using bootstrapping methods to evaluate discrimination but through the lense of gender:"
  },
  {
    "objectID": "modules/module-3.6.html#understanding-hypotheses-for-group-comparisons",
    "href": "modules/module-3.6.html#understanding-hypotheses-for-group-comparisons",
    "title": "Module 3.6",
    "section": "Understanding Hypotheses for Group Comparisons",
    "text": "Understanding Hypotheses for Group Comparisons\nWhen we want to determine whether a treatment or grouping variable has a real effect on an outcome, we need to set up two competing hypotheses. The null hypothesis states that there is no relationship between treatment and outcome, meaning any difference we observe is due to chance. The alternative hypothesis proposes that there is a genuine relationship, and the difference is not due to chance alone.\nThe key insight behind our approach is that under the null hypothesis, treatment has no impact on the outcome variable. This means that if we were to change the values of the treatment variable, the values on the outcome would stay the same. We can use this logic to simulate what we would expect to see if there truly was no effect.\nOur strategy involves reshuffling the treatment variable, calculating the treatment effect, and repeating this process many times. This allows us to ask a fundamental question: how likely would we be to observe the treatment effect in our data if there really is no effect of the treatment?"
  },
  {
    "objectID": "modules/module-3.6.html#the-résumé-experiment",
    "href": "modules/module-3.6.html#the-résumé-experiment",
    "title": "Module 3.6",
    "section": "The Résumé Experiment",
    "text": "The Résumé Experiment\nTo illustrate these concepts, we’ll examine a study by Bertrand and Mullainathan that investigated racial discrimination in responses to job applications in Chicago and Boston. The researchers sent 4,870 résumés to potential employers, randomly assigning names associated with different racial groups to otherwise identical résumés.\n\nlibrary(openintro)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nresume_dta &lt;- resume"
  },
  {
    "objectID": "modules/module-3.6.html#analyzing-callback-rates-by-race",
    "href": "modules/module-3.6.html#analyzing-callback-rates-by-race",
    "title": "Module 3.6",
    "section": "Analyzing Callback Rates by Race",
    "text": "Analyzing Callback Rates by Race\nSince race of applicant was randomly assigned in this experiment, any systematic differences in callback rates can be attributed to the racial associations of the names. Let’s examine the callback rates for each group:\n\nmeans &lt;- resume_dta |&gt;\n  group_by(race) |&gt; \n  summarize(calls = mean(received_callback))\n\nmeans\n\n# A tibble: 2 × 2\n  race   calls\n  &lt;chr&gt;  &lt;dbl&gt;\n1 black 0.0645\n2 white 0.0965\n\n\nWe can save these means for easier access and then calculate the treatment effect, which is simply the difference in means between the two groups:\n\nmean_white = means$calls[2]\nmean_black = means$calls[1]\n\nteffect &lt;- mean_white - mean_black\nteffect\n\n[1] 0.03203285"
  },
  {
    "objectID": "modules/module-3.6.html#examining-the-data-with-confidence-intervals",
    "href": "modules/module-3.6.html#examining-the-data-with-confidence-intervals",
    "title": "Module 3.6",
    "section": "Examining the Data with Confidence Intervals",
    "text": "Examining the Data with Confidence Intervals\nBefore conducting formal hypothesis tests, it’s valuable to examine both the point estimates and their confidence intervals. This gives us a sense of the precision of our estimates and whether the observed differences might be meaningful.\nLet’s start by calculating the mean callback rates for each racial group:\n\n# Bootstrap CIs for black applicants\nci_black &lt;- resume_dta |&gt;\n  filter(race == \"black\") |&gt;\n  specify(response = received_callback) |&gt;\n  generate(reps = 10000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\") |&gt;\n  get_ci(level = 0.95)\n\n# Bootstrap CIs for white applicants  \nci_white &lt;- resume_dta |&gt;\n  filter(race == \"white\") |&gt;\n  specify(response = received_callback) |&gt;\n  generate(reps = 10000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\") |&gt;\n  get_ci(level = 0.95)\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nThese two code chunks create bootstrap confidence intervals for the callback rates of Black and White applicants in a resume audit study. specify() identifies the response variable of interest (received_callback), generate() creates 10,000 bootstrap resamples for each group, calculate(stat = \"mean\") computes the callback rate in each resample and get_ci() calculates a 95% confidence interval from these bootstrap statistics.\n\n\nNext, let’s put together the means and confidence intervals for both groups into a tibble so we can visualize them together:\n\n# Combine for plotting\nplot_dta &lt;- tibble(\n  race = c(\"black\", \"white\"),\n  mean_calls = c(mean_black, mean_white),\n  lower_95 = c(ci_black$lower_ci, ci_white$lower_ci),\n  upper_95 = c(ci_black$upper_ci, ci_white$upper_ci)\n)\n\nplot_dta\n\n# A tibble: 2 × 4\n  race  mean_calls lower_95 upper_95\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 black     0.0645   0.0546   0.0747\n2 white     0.0965   0.0850   0.108 \n\n\nNow we can use ggplot2 to create a visualization that shows both the callback rates and their uncertainty:\n\nggplot(plot_dta, aes(\n  y = mean_calls, \n  x = race, \n  ymin = lower_95, \n  ymax = upper_95\n  )) +\n  geom_col(fill = \"steelblue4\") +\n  geom_errorbar(width = .05) +\n  theme_bw()  +\n ylim(0, .15) +\n  labs(x = \"Race of Applicant\",\n       y = \"Call Back Rate\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nThis ggplot creates a bar chart with error bars to visualize callback rates by race. The geom_col() function creates bars showing the mean callback rates, while geom_errorbar() adds the 95% confidence intervals. The ymin and ymax aesthetics define the error bar endpoints. We use theme_bw() for a clean appearance and ylim() to set consistent y-axis limits for better comparison between groups.\n\n\nLooking at this plot, we can see clear differences between the groups and non-overlapping confidence intervals, which suggests there may be evidence of racial discrimination. But how can we formally test the null hypothesis to decide whether to reject it?"
  },
  {
    "objectID": "modules/module-3.6.html#the-logic-of-permutation-testing",
    "href": "modules/module-3.6.html#the-logic-of-permutation-testing",
    "title": "Module 3.6",
    "section": "The Logic of Permutation Testing",
    "text": "The Logic of Permutation Testing\nTo conduct a formal hypothesis test, we need to simulate what would happen under the null hypothesis. Our approach involves calculating the difference in means between white and black applicants, then shuffling the race variable and calculating the difference in means for the shuffled data. By repeating this process many times, we can simulate the null distribution of differences in callbacks. This is a called a permutation test—a method where we simulate the null distribution by shuffling group labels (e.g., race) to see what differences in means we would expect if the treatment had no effect."
  },
  {
    "objectID": "modules/module-3.6.html#understanding-the-permutation-process",
    "href": "modules/module-3.6.html#understanding-the-permutation-process",
    "title": "Module 3.6",
    "section": "Understanding the Permutation Process",
    "text": "Understanding the Permutation Process\nLet’s walk through this process using a simplified hypothetical example with just six applicants:\nHypothetical Original Data\n\n\nApplicant\nRace\nCallback\n\n\n\nA\nBlack\nYes\n\n\nB\nBlack\nNo\n\n\nC\nBlack\nNo\n\n\nD\nWhite\nYes\n\n\nE\nWhite\nNo\n\n\nF\nWhite\nNo\n\n\n\nThe first step is to calculate the original difference in callback rates. This establishes our baseline understanding of the initial association between race and callback rates.\nThe second step involves shuffling or permuting the race variable. We randomly reassign race labels while keeping the callback outcomes exactly the same. This simulation reflects what we would expect to see if race truly had no effect on callbacks.\nHypothetical Shuffled Data\n\n\nApplicant\nRace (Shuffled)\nCallback\n\n\n\nA\nWhite\nYes\n\n\nB\nBlack\nNo\n\n\nC\nWhite\nNo\n\n\nD\nWhite\nYes\n\n\nE\nBlack\nNo\n\n\nF\nBlack\nNo\n\n\n\nAfter shuffling, we calculate the difference in callback rates again between the Black and White groups. This tells us what kind of difference we might observe purely due to chance.\nWe repeat this shuffling process thousands of times to generate a distribution of differences that could occur by chance alone. If our observed difference is extreme compared to this null distribution (meaning the p-value is low), we have strong evidence to reject the null hypothesis."
  },
  {
    "objectID": "modules/module-3.6.html#implementing-the-permutation-test",
    "href": "modules/module-3.6.html#implementing-the-permutation-test",
    "title": "Module 3.6",
    "section": "Implementing the Permutation Test",
    "text": "Implementing the Permutation Test\nIn practice, we use the tidymodels package to handle the computational details of this simulation. The infer package provides a clean workflow for permutation testing:\n\nnull_dist &lt;- resume_dta |&gt;\n  specify(response = received_callback, explanatory = race) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(10000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in means\", \n            order = c(\"white\", \"black\"))\n\nOnce we have our null distribution, we can calculate the p-value using the get_pvalue function from the infer package:\n\nget_p_value(null_dist, obs_stat = teffect, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nHere we get a p-value of exactly zero, suggesting that none of the simulated differences in means was as extreme as the observed difference. This indicates that the observed racial gap in callbacks is highly unlikely to have occurred by chance alone.\n\n\n\n\n\n\nNote\n\n\n\nYou may notice a message saying to “be careful of reporting a p-value of zero” and that this is an artifact of the number of reps chosen in the generate() step. If you increase the number of reps, you may get a p-value that is very close to zero but not exactly zero because the p-value is calculated as the proportion of simulated differences that are greater than or equal to the observed difference. But if you have the patience (or a fast computer), you can increase the number of reps to 100,000 or more and see what happens!"
  },
  {
    "objectID": "modules/module-3.6.html#visualizing-the-results",
    "href": "modules/module-3.6.html#visualizing-the-results",
    "title": "Module 3.6",
    "section": "Visualizing the Results",
    "text": "Visualizing the Results\nWe can visualize our null distribution along with our observed statistic to better understand our results:\n\nvisualize(null_dist) +\n  shade_p_value(obs_stat = teffect, direction = \"greater\") +\n  labs(\n    x = \"Estimated Difference under the Null\",\n    y = \"Count\"\n  ) + \n  theme_bw()"
  },
  {
    "objectID": "modules/module-3.6.html#drawing-conclusions",
    "href": "modules/module-3.6.html#drawing-conclusions",
    "title": "Module 3.6",
    "section": "Drawing Conclusions",
    "text": "Drawing Conclusions\nThe p-value we obtained is very small, well below the conventional 0.05 threshold. This means that if there were truly no racial discrimination, we would almost never observe a difference as large as what we found in the data. Therefore, we reject the null hypothesis and conclude that the racial gap is extremely unlikely to have occurred due to chance alone. This provides statistical evidence of racial discrimination in hiring.\n\n\n\n\n\n\nYour Turn!\n\n\n\nNow apply these same methods to investigate a different question using the same dataset:\n\nUse the gender variable in the resume data to assess whether there is gender discrimination in call backs\nPlot means and 95% confidence intervals for the call back rate for men and women\nWrite the null and alternative hypotheses\nSimulate the null distribution\nVisualize the null distribution and the gender gap\nCalculate the p-value\nWhat do you conclude from your test?"
  },
  {
    "objectID": "modules/module-1.6.html",
    "href": "modules/module-1.6.html",
    "title": "Module 1.6",
    "section": "",
    "text": "Tip\n\n\n\n\nCreate a QMD file in your Module 1 project folder for this lesson.\nLoad the necessary packages in your setup code chunk, including readr, dplyr and ggplot2.\nYou are also free to try loading the tidyverse this time instead of loading the individual packages, but try to remember which packages we are actually using.\n\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "modules/module-1.6.html#overview",
    "href": "modules/module-1.6.html#overview",
    "title": "Module 1.6",
    "section": "Overview",
    "text": "Overview\nIn the last lesson, we learned how to make bar (column) charts and historgrams. In this lesson we are going to be learning how to make two more types of visualizations that we can create with ggplot2–line charts and scatter plots. We will also delve into some more advanced topics with respect to scatter plots including adding trend lines, facet wrapping and labeling points."
  },
  {
    "objectID": "modules/module-1.6.html#line-charts",
    "href": "modules/module-1.6.html#line-charts",
    "title": "Module 1.6",
    "section": "Line charts",
    "text": "Line charts\n\nLet’s create a line chart. For this visualization, we will try to illustrate Samuel Huntington’s waves of democracy by showing how countries representing each of the three waves. The U.S. represents the first wave, Japan the second wave starting with the allied victory in WWII, and Portugal represents the first country to transition in the third wave.\nFirst, let’s grab the relevant data. We are going to be downloading the polyarchy measure for the U.S., Japan and Portugal as far back as the data are available. These are already wrangled and saved for you in an object called dem_waves_ctrs.\n📥 Download dem_summary.csv\nLet’s go ahead and download those, save them in the data subfolder for this module and read them into R.\n\ndem_waves_ctrs &lt;- read_csv(\"data/dem_waves_ctrs.csv\")\n\nNext, we are going to do our ggplot() call. The data will be the dem_waves_ctrs object that we just created. For the aesthetics mapping, we will put the year on the x-axis and the polyarchy score on the y-axis. We will also specify color in the aes() call so that we can color the lines by region.\nTo get a line chart, we have to specify geom_line(). Then within the geom_line() function we will set the linewidth equal to `1’ so that the lines are a bit more visible.\nFinally, we will add a labs() call as with the previous visualizations. But in addition to title, axis labels and a caption, we will also add color = \"Country\" to change the label of the legend to “Country” with a capital “C.”\n\n# in this ggplot() call, we add a third dimension for line color\nggplot(dem_waves_ctrs, aes(x = year, y = polyarchy, color = country)) +\n  geom_line(linewidth = 1) + # our geom is a line with a width of 1\n  labs(\n    x = \"Year\", \n    y = \"Polyarchy Score\", \n    title = 'Democracy in countries representing three different \"waves\"', \n    caption = \"Source: V-Dem Institute\", \n    color = \"Country\" # make title of legend to upper case\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nSee table three of this article\n\nSelect three countries to visualize based on which wave of democratization they belong to\nAdjust setup code to filter data on those countries\nVisualize with geom_line()"
  },
  {
    "objectID": "modules/module-1.6.html#scatter-plots",
    "href": "modules/module-1.6.html#scatter-plots",
    "title": "Module 1.6",
    "section": "Scatter plots",
    "text": "Scatter plots\n\nThe last visualization we are going to cover is a scatter plot. In this example, we are going to illustrating modernization theory, which predicts a positive relationship between wealth and democracy, while also incorporating levels of women’s representation into our analysis.\nWe are going to start with the dem_women.csv file we used in the last module. We will then group the data by country and calculate the mean for each variable. In the group_by() call we also include region because we will want to keep it so that we can color our points by region.\n\n\n\n\n\n\nNote\n\n\n\nGrouping and summarizing variables will be covered more in a future module, but here just know that we are doing this to get the average of each variable for each country. Another option could be to filter the data for one year, as we did for the histogram in the last module.\n\n\n\ndem_summary_ctry &lt;- read_csv(\"data/dem_women.csv\") |&gt;\n  group_by(country, region) |&gt; # group by country, keep region\n  summarize(\n    polyarchy = mean(polyarchy, na.rm = TRUE),\n    gdp_pc = mean(gdp_pc, na.rm = TRUE), \n    flfp = mean(flfp, na.rm = TRUE), \n    women_rep = mean(women_rep, na.rm = TRUE)\n  )\n\nNow let’s create our first scatter plot. Our ggplot() call looks similar to previous ones except for a few things. First we are calling geom_point() for our geom. But also notice that our aesthetics mapping includes four dimenstions: x, y, color and size. So here we are telling ggplot2 that we want wealth on the x-axis, the polyarchy score on the y-axis, to color the points based on region, and to vary the size of the points in relation to the level of women’s representation.\nOne last thing we want to do is to put our x-axis on a log scale and change the labels to reflect their dollar values. For the log scale, we can use the scale_x_log10() function and for the labels we can use the label_number() function from the scales package. We set the prefix to “$” and the suffix to “k” so that each number on the x-axis starts with a dollar sign and ends with “k” denoting “thousands.”\n\n\n\n\n\n\nNote\n\n\n\nWe will encounter other useful scales functions including label_dollar() and label_percent() in future lessons.\nNotice that in this example we introduce the scales package by including it as a prefix to the label_number() function, e.g. scales::label_number(prefix = \"$\", suffix = \"k\"). This allows us to use the package without having to load it, e.g. library(scales). It also has the benefit of generating a list of auto-complete suggestions for the many available functions in the scales package.\n\n\n\n# in this ggplot() call we have four dimensions\n# x, y, color, and size\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy, color = region, size = women_rep)) + \n  geom_point() + # use geom_point() for scatter plots\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\",\n    size = \"Women Reps\"\n    )\n\n\n\n\n\n\n\nThe plot does a good job of illustrating the basic point of modernization theory in that we do see the positive correlation between wealth and democracy. But we also see that there are some outliers and that a lot of the outlier countries are concentrated in the Middle East.\nWe also see that the distribution of women’s representation is somewhat orthogonal to wealth and democracy. Most wealthy western countries have high levels of women’s representation, but so do a lot of low- and middle-income countries in Africa, Asia and Latin America.\nAdding a trend line\nWe can definitely see a relationship between wealth and democracy in the scatter plot, but how strong is it? One way to find out is to add a trend line. Let’s do this by adding another geom, geom_smooth(), and specifying a linear model with the argument method = \"lm\" We acn also set the linewidth of the trend line to 1 so that the line is more visible.\nIf we want to add a single trend while also maintaining the coloring by region, then we have to reconfigure the ggplot() call a bit. Specifically, we will want to move color = region to a separate aes() call in the geom_point() function, e.g. geom_point(aes(color = region)). If we don’t do this we will get separate trend lines for each region (try it and see!).\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point(aes(color = region)) + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\", \n    color = \"Region\"\n    )\n\n\n\n\n\n\n\nFacet wrapping\nNow let’s imagine that we really interested in drilling down into the “heterogeneous effects” of wealth on democracy by region. In other words, we want to see more clearly how wealth is related to democracy in some regions but not others. For this, we can use facet_wrap() to get a separate chart for each region rather than just shading the points by region. Inside of facet_wrap() we identify region as the variable that we want to use to separate the plots, e.g. facet_wrap(~region). Notice how we have to include a tilde (~) here.\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  facet_wrap(~ region) +\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )\n\n\n\n\n\n\n\nHere we can clearly see a relationship between wealth and democracy in all of the countries except for the Middle East and Africa. We could speculate that the lack of a relationship in the Middle East could be evidence of an oil curse dynamic whereas perhaps the lack of a relationship in Africa is due to weak institutions.\nThe relationship between wealth and democracy in the West would be apparent, but it is obscured by the fact that western countries because the high wealth and polyarchy values result in extreme bunching in the northwest quadrant of the graph. To deal with this issue, we could add the scales = \"free\" argument to our plot.\n\nggplot(dem_summary_ctry, aes(x = gdp_pc, y = polyarchy)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", linewidth = 1) + \n  facet_wrap(~ region, scales = \"free\") +\n  scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n  labs(\n    x= \"GDP per Capita\", \n    y = \"Polyarchy Score\",\n    title = \"Wealth and democracy, 1990 - present\", \n    caption = \"Source: V-Dem Institute\"\n    )\n\n\n\n\n\n\n\nBut notice there is a bit of a tradeoff here. With the scales = free option set, we now have separate axes for each of the plots. This is less of a clean look than having common x and y axes.\n### Labeling points\nNow let’s try drilling down into one of the regions to get a better sense of what countries are driving the relationship. To do this, we can filter our data set for a region that we are interested in and then add country labels to the points in the scatter plot. Here we are going to filter for “Asia” and we will ad a geom_text() call to add country labels. In the geom_text() call we include arguments for size and vjust to adjust the size and vertical location of the labels relative to the points.\n\ndem_summary_ctry |&gt; \n  filter(region == \"Asia\") |&gt;\n  ggplot(aes(x = gdp_pc, y = polyarchy)) + \n    geom_point() + \n    geom_text(aes(label = country), size = 2, vjust = 2) +\n    geom_smooth(method = \"lm\", linewidth = 1) +\n    scale_x_log10(labels = scales::label_number(prefix = \"$\", suffix = \"k\")) +\n      labs(\n        x= \"GDP Per Capita\", \n        y = \"Polyarchy Score\",\n        title = \"Wealth and democracy in Asia, 1990 - present\", \n        caption = \"Source: V-Dem Institute\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nThere are four variables in dem_summary_ctry\n\nPick one related to women’s empowerment\nVisualize it on the y-axis with gdp_pc or polyarchy on the x-axis\nChange labels and legend titles to match your visualization\nInterpret your plot\nAdd a trend line\nAdd country labels\nTry filtering for a region of your choice\nTry facet wrapping by region"
  },
  {
    "objectID": "modules/module-4.3.html",
    "href": "modules/module-4.3.html",
    "title": "Module 4.3",
    "section": "",
    "text": "Prework\n\n\n\n\nLook over the documentation for the broom package, which we will use to compute regression diagnostics.\nLook over the documentation for the datawizard package, which we will use to winsorize our data).\nRun this code chunk to load the necessary packages and data for this module:\n\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\n# Load V-Dem data for 2019\nmodel_data &lt;- fetchdem(\n  indicators = c(\n  \"v2x_libdem\", \n  \"e_gdppc\", \n  \"v2cacamps\"),\n  start_year = 2019, \n  end_year = 2019\n  ) |&gt;\n  rename(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc,\n    polarization = v2cacamps\n    ) |&gt;\n  filter(!is.na(lib_dem), !is.na(wealth))"
  },
  {
    "objectID": "modules/module-4.3.html#overview",
    "href": "modules/module-4.3.html#overview",
    "title": "Module 4.3",
    "section": "Overview",
    "text": "Overview\nRemember those wealthy but undemocratic countries (like Saudi Arabia and UAE) that appeared as outliers in our GDP-democracy analysis? In this module, we’ll learn systematic approaches for identifying and handling such outliers. You’ll discover that outliers aren’t always “bad” data points to remove—sometimes they represent the most interesting cases in your analysis!\nWe will explore a number of key strategies for dealing with outliers once identified, helping you make informed decisions for your own projects. By the end of this module, you’ll have a toolkit for handling outliers in your regression analyses and understand when each approach is most appropriate."
  },
  {
    "objectID": "modules/module-4.3.html#what-are-outliers-and-why-do-they-matter",
    "href": "modules/module-4.3.html#what-are-outliers-and-why-do-they-matter",
    "title": "Module 4.3",
    "section": "What Are Outliers and Why Do They Matter?",
    "text": "What Are Outliers and Why Do They Matter?\nLet’s start by revisiting our democracy and GDP analysis from Module 4.1. Remember this plot?\n\n\n\n\n\n\nNotice those points in the southwest corner—wealthy countries with low democracy scores. These are our outliers: observations that don’t fit the general pattern of the data. Try hovering over the points to see which countries they include.\nBut it is important to note that not all outliers are created equal. In our case, these outliers are not measurement errorr but instead substantively interesting cases! They largely represent oil-rich authoritarian states, which tells us something important about the relationship between wealth and democracy.\nTypes of Outliers in Regression\nIn regression analysis, we distinguish between different types of unusual observations:\n\n\nLeverage points: Extreme values on the x-axis (very high or low GDP)\n\nInfluential points: Points that significantly change the regression line when removed\n\nResidual outliers: Points far from the regression line (high residuals)\n\nA point can be one, two, or all three of these simultaneously."
  },
  {
    "objectID": "modules/module-4.3.html#identifying-outliers-visual-and-statistical-methods",
    "href": "modules/module-4.3.html#identifying-outliers-visual-and-statistical-methods",
    "title": "Module 4.3",
    "section": "Identifying Outliers: Visual and Statistical Methods",
    "text": "Identifying Outliers: Visual and Statistical Methods\nBoxplots for Visual Identification\nThe simplest way to spot outliers is with boxplots. Let’s examine both our variables with boxplots. Here we will create boxplots for both the democracy scores and the raw GDP per capita (before any transformations) using geom_boxplot(), which we learned about in an earlier lesson. We will store each object as a separate object and then use patchwork to combine them into a single figure.\n\n# Create boxplots for both variables\nlibrary(patchwork)  # for combining plots\n\n# Boxplot for democracy scores\np1 &lt;- ggplot(model_data, aes(y = lib_dem)) +\n  geom_boxplot() +\n  labs(y = \"Liberal Democracy Score\", \n       title = \"Democracy Outliers\") +\n  theme_minimal()\n\n# Boxplot for raw GDP (before transformation)\np2 &lt;- ggplot(model_data, aes(y = wealth)) +\n  geom_boxplot() +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(y = \"GDP per Capita\", \n       title = \"GDP Outliers (Raw Data)\") +\n  theme_minimal()\n\n# Combine the plots\np1 | p2\n\n\n\n\n\n\n\nThe IQR Method\nThe Interquartile Range (IQR) method is a common statistical approach for identifying outliers and forms the basis of the boxplot. It classifies a data point as an outlier if it falls more than 1.5 times the IQR below the first quartile (Q1) or above the third quartile (Q3). This helps flag unusually high or low values that may deserve further scrutiny.\nIn the code below, we identify outliers in GDP per capita using the base R function boxplot.stats(), which applies the IQR method internally. We use the %in% operator to check which GDP values are identified as outliers and store this as a new logical column, gdp_outlier. Since our earlier boxplot analysis showed no democracy outliers, we simplify the analysis here by focusing only on GDP. We then summarize how many outliers there are and list the countries identified as GDP outliers.\n\n# Identify GDP outliers \nmodel_data &lt;- model_data |&gt;\n  mutate(gdp_outlier = wealth %in% boxplot.stats(wealth)$out)\n\n# Summarize how many outliers\nmodel_data |&gt;\n  summarize(\n    gdp_outliers = sum(gdp_outlier),\n    percent_outliers = round(100 * mean(gdp_outlier), 1)\n  )\n\n  gdp_outliers percent_outliers\n1            6              3.4\n\n# View GDP outlier countries\nmodel_data |&gt;\n  filter(gdp_outlier) |&gt;\n  arrange(desc(wealth)) |&gt;\n  select(country, wealth, lib_dem)\n\n                   country wealth lib_dem\n1               Luxembourg 92.389   0.798\n2                    Qatar 80.190   0.084\n3                  Ireland 75.467   0.825\n4                Singapore 72.025   0.333\n5     United Arab Emirates 64.628   0.092\n6 United States of America 60.641   0.737\n\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nIn the code above, we use the base R function boxplot.stats() to identify GDP outliers. This function returns several components, including a named element called $out that contains the values considered outliers.\nSo when we write wealth %in% boxplot.stats(wealth)$out we are checking which values of wealth are included in the set of outliers returned by boxplot.stats(). The result is a logical vector (TRUE for outliers, FALSE otherwise), which we store in a new column using mutate().\n\n\nIdentifying Influential Points in Regression\nTo assess the quality of a regression model and identify observations that may unduly affect the results, we can compute diagnostic statistics. These help us detect data points that are surprising, unusual, or overly influential in determining the regression line. While there are multiple diagnostic measures, three are commonly used: leverage, standardized residuals, and Cook’s distance.\nWe use the broom package’s augment() function to compute these diagnostics in a tidy, data-frame format. augment() takes a fitted model object (like one created by lm()) and returns the original data along with new columns containing fitted values, residuals, and other diagnostic measures:\n\n\n.fitted: the predicted values from the model\n\n.resid: the raw residuals (observed – fitted)\n\n.std.resid: standardized residuals (adjusted for their expected variance)\n\n.hat: leverage values, which measure how extreme the predictor values are\n\n.cooksd: Cook’s distance, which combines leverage and residual size to estimate how much a point influences the model\n\nThese statistics allow us to flag potentially problematic points:\n\n\nHigh leverage points have unusual predictor values. They don’t necessarily distort the model, but they have the potential to. A common rule of thumb is that leverage values greater than twice the mean are worth inspecting.\n\nHigh residual points are poorly fit by the model—they lie far from the regression line. Standardized residuals larger than ±2 are typically considered large.\n\nHigh influence points affect the model’s coefficients disproportionately. Cook’s distance greater than 4/n (where n is the number of observations) is a common informal threshold.\n\nBy combining these diagnostics, we can identify cases that might merit further attention—due to unusual inputs, poor fit, or disproportionate influence on the model’s results.\n\nlibrary(broom)\n\n# Fit model\ndemocracy_model &lt;- lm(lib_dem ~ log(wealth), data = model_data)\n\n# Get model diagnostics (with original data attached)\nmodel_diagnostics &lt;- augment(democracy_model, data = model_data)\n\n# Add thresholds for outlier detection\nmodel_diagnostics &lt;- model_diagnostics |&gt;\n  mutate(\n    high_leverage = .hat &gt; 2 * mean(.hat, na.rm = TRUE),\n    high_residual = abs(.std.resid) &gt; 2,\n    high_influence = .cooksd &gt; 4 / nrow(model_data)\n  )\n\n# Filter problematic cases\nmodel_diagnostics |&gt;\n  filter(high_leverage | high_residual | high_influence) |&gt;\n  select(country, wealth, lib_dem, high_leverage, high_residual, high_influence)\n\n# A tibble: 16 × 6\n   country             wealth lib_dem high_leverage high_residual high_influence\n   &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt; &lt;lgl&gt;         &lt;lgl&gt;         &lt;lgl&gt;         \n 1 Venezuela            1.19    0.059 TRUE          FALSE         FALSE         \n 2 Niger                1.19    0.399 TRUE          FALSE         FALSE         \n 3 Burundi              0.735   0.054 TRUE          FALSE         FALSE         \n 4 Central African Re…  0.827   0.22  TRUE          FALSE         FALSE         \n 5 Ireland             75.5     0.825 TRUE          FALSE         FALSE         \n 6 Liberia              1.19    0.412 TRUE          FALSE         FALSE         \n 7 Malawi               1.31    0.412 TRUE          FALSE         FALSE         \n 8 Qatar               80.2     0.084 TRUE          TRUE          TRUE          \n 9 Democratic Republi…  0.913   0.145 TRUE          FALSE         FALSE         \n10 Eritrea             21.1     0.009 FALSE         TRUE          FALSE         \n11 Madagascar           1.31    0.258 TRUE          FALSE         FALSE         \n12 Turkmenistan        25.0     0.037 FALSE         TRUE          FALSE         \n13 Bahrain             30.0     0.052 FALSE         TRUE          TRUE          \n14 Luxembourg          92.4     0.798 TRUE          FALSE         FALSE         \n15 Saudi Arabia        33.3     0.047 FALSE         TRUE          TRUE          \n16 United Arab Emirat… 64.6     0.092 FALSE         TRUE          TRUE          \n\n\nThe analysis identified a fairly large number of cases as having high leverage, large residuals, or high influence. But it is important to note that these are not necessarily errors in the data or points that necessarily have to be removed. Rather, they are observations that deviate from what the model expects and exert a disproportionate pull on the regression line.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nTry running the regression with polarization as the independent variable instead of wealth.\nNow use the code above to identify influential observations in this new model.\nWhat do you notice? Are there any countries that are influential in both models? Or are most of the influential points new ones?"
  },
  {
    "objectID": "modules/module-4.3.html#dealing-with-outliers-strategies-and-considerations",
    "href": "modules/module-4.3.html#dealing-with-outliers-strategies-and-considerations",
    "title": "Module 4.3",
    "section": "Dealing with Outliers: Strategies and Considerations",
    "text": "Dealing with Outliers: Strategies and Considerations\nOutliers and influential observations warrant closer inspection. Outliers may represent countries with unusual political or economic profiles, or cases that do not conform well to the general trend. Influential observations often reveal the limits of a simple model and can point to deeper questions about the structure of the data or the need for additional variables.\nAfter considering extreme or influential data points more closely, we can decide what to do with them. One option is to do nothing and leave them in the dataset as is. Another option to assess how much they affect the results by re-estimating the model with and without them. Another option is to tranform the data to reduce their influence. Finally, we can perform an operation called winsorizing, which caps extreme values at a specified percentile rather than removing them entirely.\nRemoving Outliers\nOne approach is to remove outliers entirely. This is appropriate when the outliers represent data entry errors, they come from a different population than your main analysis, or you want to understand the relationship for the “typical” cases. But another option is to remove outliers or influential points termporarily to see how they affect the results. If they change the results significantly, this should be acknowledged and discussed.\nLet’s start by rerunning the regression model with the original data, including all observations:\n\ndemocracy_model &lt;- lm(lib_dem ~ log(wealth), data = model_data)\n\nsummary(democracy_model)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,    Adjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n\n\nNow let’s remove some of the problematic points we found earlier. Since we identified several countries as having high leverage, large residuals, or high influence, let’s see how removing them affects our regression results. We’ll fit the model both with and without these outliers and compare the results. Let’s start by removing the outliers.\n\n# Remove countries with extreme leverage or influence\nmodel_data_no_outliers &lt;- model_diagnostics |&gt;\n  filter(!high_leverage | !high_residual | !high_influence) |&gt;\n  select(country, wealth, lib_dem,)\n\n\n\n\n\n\n\nUnderstanding the Code\n\n\n\nHere we use the filter() function in conjunction with the ! and | operators. The ! operator negates the condition, so !high_leverage means “not high leverage.” The | operator means “or,” so we are filtering out any rows that have high leverage, large residuals, or high influence.\n\n\nNow let’s fit the model without the outliers to see if our results substantially change.\n\nmodel_without_outliers &lt;- lm(lib_dem ~ log(wealth), data = model_data_no_outliers)\n\nsummary(model_without_outliers)\n\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data_no_outliers)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55358 -0.14282  0.04255  0.18092  0.37282 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.12166    0.03756   3.239  0.00144 ** \nlog(wealth)  0.12568    0.01459   8.614 4.53e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2195 on 171 degrees of freedom\nMultiple R-squared:  0.3026,    Adjusted R-squared:  0.2985 \nF-statistic:  74.2 on 1 and 171 DF,  p-value: 4.534e-15\n\n\nHere we do not notice a substantial difference between these results and those of the original model. We see that the coefficient for log(wealth) remains positive and statistically significant, and the R-squared value is similar. This suggests that while the outliers were influential, they did not fundamentally change the relationship between GDP and democracy in this case.\nData Transformation\nNow that we’ve identified outliers in our raw data, let’s see how transformations can help reduce their influence. We’ve already been using one transformation—taking the log of GDP! Let’s see why this helps with outliers.\n\n\n\n\n\n\n\n\nNotice how the log transformation improves the model fit. This is because logging compresses large values, reducing the influence of those extremely wealthy outlier countries.\nOther common transformations, like the square root and Box-Cox transformations, can also help manage skewed data and reduce the influence of outliers. Like the log transformation, these approaches work by compressing large values, which pulls extreme observations closer to the main body of the data. This can lead to a better-fitting model and more stable inferences.\nThe square root transformation is useful when data are moderately skewed and values are all positive. It’s a simpler alternative to logging and is often used when data contain some high values but not extreme outliers.\nThe Box-Cox transformation is more flexible—it finds the “best” power transformation (e.g., square root, log, reciprocal) based on the data itself. It’s especially helpful when you’re unsure which transformation is appropriate. Like log and square root transformations, Box-Cox can improve linearity and reduce the impact of outliers.\nWhen we compare models using different transformations, we often look at goodness-of-fit measures like R-squared, adjusted R-squared, and AIC. These metrics help us assess whether the transformation improves model fit while avoiding overfitting. In general, a higher adjusted R-squared and a lower AIC suggest a better-fitting model.\nWe won’t go deep into the code here, but it’s important to know that transformations are a standard part of the data science toolkit. They help models perform better when relationships are nonlinear or when extreme values distort the picture. The log transformation is one example—but not the only one—of how transforming variables can lead to more meaningful and interpretable results.\nWinsorizing\nWinsorizing is another technique to reduce the impact of extreme values, especially when we don’t want to throw out data points entirely. Unlike trimming (which removes outliers), Winsorizing caps them at a specified percentile. For example, using the 95th percentile replaces all values above it with the 95th percentile value.\nThis can help tame the influence of outliers in a way that’s less aggressive than deletion and doesn’t distort model assumptions as much as leaving extreme values untouched. It’s especially useful when you suspect that very high or low values are distorting the fit of your regression.\nLet’s Winsorize our GDP data at the 95th percentile using the winsorize() function from the datawizard package. This will cap extreme values at the 95th percentile, reducing their influence without removing them entirely.\n\nlibrary(datawizard)\n\n# Winsorize the data\nmodel_data_winsorized &lt;- model_data |&gt;\n  mutate(\n    wealth_win95 = winsorize(wealth, threshold = 0.05)\n  )\n\nNow let’s visualizing the winsorizing effects.\n\n# Show the effect of winsorizing on raw GDP data\noriginal_plot &lt;- ggplot(model_data, aes(x = wealth)) +\n  geom_histogram(bins = 30, alpha = 0.7, fill = \"darkblue\") +\n  scale_x_continuous(labels = scales::label_dollar(suffix = \"k\")) +\n  labs(title = \"Original Distribution\", x = \"GDP per Capita\") +\n  theme_minimal()\n\nwinsorized_plot &lt;- ggplot(model_data_winsorized, aes(x = wealth_win95)) +\n  geom_histogram(bins = 30, alpha = 0.7, fill = \"darkred\") +\n  scale_x_continuous(labels = scales::label_dollar(suffix = \"k\")) +\n  labs(title = \"Winsorized Distribution\", x = \"GDP per Capita\") +\n  theme_minimal()\n\noriginal_plot | winsorized_plot\n\n\n\n\n\n\n\nHere we clearly see how winsorizing reduces the impact of extreme values. The histogram on the left shows the original distribution with its long right tail, while the winsorized histogram on the right caps those extreme values, making the distribution a bit more symmetric.\nRobust Regression\nOne last approach we will briefly explore is robust regression. This technique is designed to be less sensitive to outliers than ordinary least squares (OLS) regression. It uses different loss functions that reduce the influence of extreme values, making it a good choice when you have outliers that you do not want to remove or transform. We can perform robust regression using the rlm() function from the MASS package, which implements a robust version of linear regression.\n\n# Robust regression is less sensitive to outliers\nlibrary(MASS)\n\nrobust_model &lt;- rlm(lib_dem ~ log(wealth), data = model_data)\n\nsummary(robust_model)\n\n\nCall: rlm(formula = lib_dem ~ log(wealth), data = model_data)\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60880 -0.15368  0.03422  0.17024  0.37024 \n\nCoefficients:\n            Value  Std. Error t value\n(Intercept) 0.1178 0.0385     3.0619 \nlog(wealth) 0.1311 0.0149     8.8195 \n\nResidual standard error: 0.2429 on 172 degrees of freedom\n\n\nHere we see that the coefficients and significance levels are similar to our original OLS model, but we can be more confident that the results are not overly influenced by the outliers we identified earlier.\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nTry removing the influential points in the polarization model that you created earlier.\nRerun the model with those influential points removed and compare the results to the original model.\nHow do the results change? Do you notice any differences in the coefficients or significance levels?\nNow try winsorizing the polarization data at the 95th percentile and rerunning the model. How does this affect the results?\nFinally, try running a robust regression on the polarization data. How do the results compare to the original model and the winsorized model?"
  },
  {
    "objectID": "modules/module-4.3.html#making-the-right-choice",
    "href": "modules/module-4.3.html#making-the-right-choice",
    "title": "Module 4.3",
    "section": "Making the Right Choice",
    "text": "Making the Right Choice\nWhen dealing with outliers and influential observations, the most important step is to understand their nature and context. Outliers caused by data entry errors or coming from clearly different populations (like microstates in a global dataset) may justify removal. But many outliers are legitimate and may be the most substantively interesting cases in your data. In those situations, it’s often better to retain them and use transformations (like logging or Box-Cox) to reduce their influence, especially when the data are naturally skewed. Winsorizing offers a middle ground—keeping all observations while capping extreme values—particularly useful in predictive modeling or when you suspect noise in the extreme tails.\nWhatever approach you take—removal, transformation, winsorizing, or none at all—it’s essential to report your decisions clearly and check how they affect your results. Describe how you identified outliers, explain your reasoning for handling them in a specific way, and conduct a brief sensitivity analysis to see if your main conclusions change. Outlier handling is not just a technical step; it’s a modeling decision that should be transparent and justifiable to your audience."
  },
  {
    "objectID": "modules/module-4.4.html",
    "href": "modules/module-4.4.html",
    "title": "Module 4.4",
    "section": "",
    "text": "Prework\n\n\n\n\nInstall the vdemdata package (install.packages(\"vdemdata\")). Note that this is different from the vdemlite package that we have been using this semester. vdemdata is comprised of one function, vdem, which loads the entire V-Dem dataset into R. We are using vdemdata instead of vdemlite because it has some indicators that are not yet available in vdemlite.\nRestart your R session by going to Session &gt; Restart R in RStudio. The reason is that packages we used in the last model may result in conflicts in this one.\nRun the following code chunk to set up the data and packages required for this module. Then take a look to explore what is in the data frame.\n\n\nCodelibrary(tidyverse)\nlibrary(vdemdata)\n\nmodel_data &lt;- vdem |&gt;\n  filter(year == 2006) |&gt; \n  select(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc, \n    oil_rents = e_total_oil_income_pc,\n    polarization = v2cacamps, \n    corruption = v2x_corr, \n    judicial_review = v2jureview_ord, \n    region = e_regionpol_6C, \n    regime = v2x_regime) \n\n#glimpse(model_data)"
  },
  {
    "objectID": "modules/module-4.4.html#overview",
    "href": "modules/module-4.4.html#overview",
    "title": "Module 4.4",
    "section": "Overview",
    "text": "Overview\nIn our previous work with linear regression, we explored relationships between two continuous variables like the relationship between a country’s wealth and its level of democracy. But the real world is more complex than simple two-variable relationships. Countries differ not just in wealth, but also in their political institutions, regional contexts, historical experiences, and cultural factors. How do we account for these multiple influences simultaneously?\nMultiple linear regression allows us to model these complex relationships by including multiple predictor variables in a single model. This powerful technique helps us understand not just whether variables are related, but how they relate to each other while controlling for other factors.\nBefore we dive into the technical details, let’s hear from Andrew Ng, one of the leading experts in machine learning and statistical modeling, as he explains the fundamentals of multiple linear regression:\n\nBy the end of this module, you’ll be able to use categorical variables as predictors in linear regression models, interpret dummy variables and understand reference categories, build and interpret multiple regression models with both categorical and continuous predictors, understand the concept of “controlling for” other variables, and make informed decisions about which variables to include in your models."
  },
  {
    "objectID": "modules/module-4.4.html#categorical-predictors-in-regression",
    "href": "modules/module-4.4.html#categorical-predictors-in-regression",
    "title": "Module 4.4",
    "section": "Categorical Predictors in Regression",
    "text": "Categorical Predictors in Regression\nSo far, we have worked with continuous predictors like GDP per capita and democracy scores. But many important variables in political science are categorical, representing distinct groups or categories rather than numerical measurements. In our democracy research, examples include regime type (democratic, autocratic, hybrid), world region (Europe, Asia, Americas, Africa), institutional features such as the presence or absence of judicial review, and historical experiences like colonial history or a communist past.\nAs we learned in a previous module, categorical variables can take different forms. Nominal variables have categories with no natural order, like world region. Ordinal variables have categories with a meaningful order, like education levels from primary through tertiary. Binary variables have just two categories, like the presence or absence of an institution.\nLet’s have a quick look at our data for this module to see whether we can spot our categorical variables:\n\nglimpse(model_data)\n\nRows: 177\nColumns: 9\n$ country         &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\"…\n$ lib_dem         &lt;dbl&gt; 0.480, 0.655, 0.882, 0.842, 0.633, 0.652, 0.776, 0.017…\n$ wealth          &lt;dbl&gt; 14.584, 10.008, 42.378, 48.410, 3.284, 10.580, 37.068,…\n$ oil_rents       &lt;dbl&gt; 694.847, 639.506, 0.000, 0.000, 6.380, 10.060, 2.635, …\n$ polarization    &lt;dbl&gt; 0.102, -1.783, -2.254, -1.732, -0.615, 0.065, -2.261, …\n$ corruption      &lt;dbl&gt; 0.608, 0.212, 0.004, 0.023, 0.630, 0.423, 0.108, 0.888…\n$ judicial_review &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, …\n$ region          &lt;dbl&gt; 2, 2, 5, 5, 4, 4, 6, 6, 1, 1, 3, 3, 2, 1, 2, 5, 5, 2, …\n$ regime          &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 3, 0, 1, 2, 1, 1, 2, 3, 2, 3, 3, 2, …\n\n\nHopefully you can see that we have several categorical variables in our dataset: judicial_review, region and regime. judicial_review is a binary indicator (or “dummy”) variable indicating whether a country has judicial review, region is a nominal variable representing the world region, and regime is an ordinal variable representing different types of political regimes.\nFactors in R\nWhen we include categorical variables in regression models, we need to convert them into a format that the model can understand. In R, this is done using factors. Factors are a special data type that tells R to treat the variable as categorical, even if the categories are represented by numbers.\nRight now, if we look at our data types, we can see that judicial_review, region, and regime are in the dbl (double) format, which means they are treated as continuous numeric variables. To use them as categorical predictors, we need to convert them to factors.\nLet’s start with converting our binary judicial_review variable into a factor:\n\nmodel_data &lt;- model_data |&gt; \n  mutate(judicial_review = factor(judicial_review, \n                                   labels = c(\"No\", \"Yes\")))\n\nglimpse(model_data)\n\nRows: 177\nColumns: 9\n$ country         &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\"…\n$ lib_dem         &lt;dbl&gt; 0.480, 0.655, 0.882, 0.842, 0.633, 0.652, 0.776, 0.017…\n$ wealth          &lt;dbl&gt; 14.584, 10.008, 42.378, 48.410, 3.284, 10.580, 37.068,…\n$ oil_rents       &lt;dbl&gt; 694.847, 639.506, 0.000, 0.000, 6.380, 10.060, 2.635, …\n$ polarization    &lt;dbl&gt; 0.102, -1.783, -2.254, -1.732, -0.615, 0.065, -2.261, …\n$ corruption      &lt;dbl&gt; 0.608, 0.212, 0.004, 0.023, 0.630, 0.423, 0.108, 0.888…\n$ judicial_review &lt;fct&gt; Yes, No, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, N…\n$ region          &lt;dbl&gt; 2, 2, 5, 5, 4, 4, 6, 6, 1, 1, 3, 3, 2, 1, 2, 5, 5, 2, …\n$ regime          &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 3, 0, 1, 2, 1, 1, 2, 3, 2, 3, 3, 2, …\n\n\nNow judicial_review is a factor with two levels: “No” and “Yes”. This tells R to treat it as a categorical variable in our regression models.\nLinear Regression with a Categorical Predictor\nNow that we have our categorical variable set up, we can include it in a linear regression model. Our judicial review variable is based on the following question:\n\nDo high courts (Supreme Court, Constitutional Court, etc) have the power to rule on whether laws or policies are constitutional/legal? (Yes or No)\n\nWe can use this variable to explore whether countries with judicial review tend to have higher levels of democracy. Let’s start by visualizing the relationship between judicial review and democracy levels.\n\nCodeggplot(model_data, aes(x = wealth, y = lib_dem, \n                       color=judicial_review)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"GPD per capita\", y = \"Liberal Democracy Index\") +\n  theme_bw() +\n  scale_x_log10(labels = scales::dollar_format(suffix = \"k\")) +\n  scale_color_manual(name = \"Judicial Review\", \n                     values = c(\"steelblue3\", \"coral\"), \n                     labels = c(\"No\", \"Yes\")) \n\n\n\n\n\n\n\nHere we can see that at every level of wealth, countries with judicial review (in coral) tend to have higher democracy levels than those without (in blue).\nWe can more precisely quantify this relationship between judical review and democracy using a linear regression model. Let’s fit a model predicting democracy (lib_dem) with judicial review (judicial_review):\n\njudicial_model &lt;- lm(lib_dem ~ judicial_review, data = model_data)\n\nsummary(judicial_model)\n\n\nCall:\nlm(formula = lib_dem ~ judicial_review, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41492 -0.18692 -0.05246  0.20308  0.66654 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.17946    0.04927   3.643 0.000355 ***\njudicial_reviewYes  0.26746    0.05334   5.014  1.3e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2512 on 175 degrees of freedom\nMultiple R-squared:  0.1256,    Adjusted R-squared:  0.1206 \nF-statistic: 25.14 on 1 and 175 DF,  p-value: 1.297e-06\n\n\nThe output shows us the coefficients for the model. The intercept represents the average democracy level for countries without judicial review (the reference category), and the coefficient for judicial_reviewYes represents the difference in average democracy level between countries with judicial review and those without.\n\n\n\n\n\n\nUnderstanding Reference Categories\n\n\n\nWhen we include a categorical variable in a regression model, R automatically creates dummy variables for each category except one. The category that does not get its own dummy variable is called the reference category or baseline category. In this example, the reference category is “No” for judicial review.\nWhen we interpret a coefficient for a categorical variable, we are comparing that category to the reference category. So the coefficient for judicial_reviewYes tells us how much higher the average democracy level is for countries with judicial review compared to those without.\n\n\nTo better understand the model, let’s have a look at the model equation:\n\\[ \\widehat{Democracy_{i}} = 0.18 + 0.27*JudicialReview(yes) \\]\nHere the slope of the coefficient for \\(JudicialReview(yes)\\) indicates that. countries with judicial review are expected, on average, to be 0.27 units more democratic on the liberal democracy index. The intercept of 0.18 represents the average democracy score for countries without judicial review."
  },
  {
    "objectID": "modules/module-4.4.html#multiple-categories-and-reference-groups",
    "href": "modules/module-4.4.html#multiple-categories-and-reference-groups",
    "title": "Module 4.4",
    "section": "Multiple Categories and Reference Groups",
    "text": "Multiple Categories and Reference Groups\nBinary categorical variables are straightforward because they create a single dummy variable. But what happens when we have categorical variables with more than two categories, like world regions or regime types?\nWhen a categorical variable has multiple levels, R creates multiple dummy variables, specifically one for each category except for the reference category. For example, if we have a region variable with 6 categories, R will create 5 dummy variables: one for Latin America (1 if Latin America, 0 otherwise), one for MENA (1 if MENA, 0 otherwise), one for Sub-Saharan Africa (1 if SSA, 0 otherwise), one for Western Europe & North America (1 if WENA, 0 otherwise), and one for Asia & Pacific (1 if Asia, 0 otherwise). There is no dummy variable for Eastern Europe, which becomes our reference category.\nThe reference category is crucial because all other categories are interpreted relative to this baseline. Without it, we’d have perfect multicollinearity where the dummy variables would be perfectly correlated with each other and the intercept, making the model impossible to estimate.\nLet’s see this in action with world regions and democracy. Let’s start by converting the region variable into a factor with labels for each region:\n\nmodel_data &lt;- model_data |&gt; \n  mutate(region = factor(region, \n                         labels = c(\"Eastern Europe\", \n                                    \"Latin America\", \n                                    \"MENA\", \n                                    \"Sub-Saharan Africa\", \n                                    \"Western Europe & North America\", \n                                    \"Asia & Pacific\")))\n\nglimpse(model_data)\n\nRows: 177\nColumns: 9\n$ country         &lt;chr&gt; \"Mexico\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Ghana\"…\n$ lib_dem         &lt;dbl&gt; 0.480, 0.655, 0.882, 0.842, 0.633, 0.652, 0.776, 0.017…\n$ wealth          &lt;dbl&gt; 14.584, 10.008, 42.378, 48.410, 3.284, 10.580, 37.068,…\n$ oil_rents       &lt;dbl&gt; 694.847, 639.506, 0.000, 0.000, 6.380, 10.060, 2.635, …\n$ polarization    &lt;dbl&gt; 0.102, -1.783, -2.254, -1.732, -0.615, 0.065, -2.261, …\n$ corruption      &lt;dbl&gt; 0.608, 0.212, 0.004, 0.023, 0.630, 0.423, 0.108, 0.888…\n$ judicial_review &lt;fct&gt; Yes, No, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, N…\n$ region          &lt;fct&gt; Latin America, Latin America, Western Europe & North A…\n$ regime          &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 3, 0, 1, 2, 1, 1, 2, 3, 2, 3, 3, 2, …\n\n\nNow our region variable is a factor with six levels, and Eastern Europe will be our reference category. We can check this by using the base R levels() function:\n\n# Check the levels of our region variable\nlevels(model_data$region)\n\n[1] \"Eastern Europe\"                 \"Latin America\"                 \n[3] \"MENA\"                           \"Sub-Saharan Africa\"            \n[5] \"Western Europe & North America\" \"Asia & Pacific\"                \n\n\nWhen we fit a regression model with this region variable, R will create dummy variables for each region except Eastern Europe:\n\nregion_model &lt;- lm(lib_dem ~ region, data = model_data)\n\nsummary(region_model)\n\n\nCall:\nlm(formula = lib_dem ~ region, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45792 -0.13186 -0.02004  0.11414  0.49320 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           0.43450    0.03608  12.042  &lt; 2e-16 ***\nregionLatin America                   0.06642    0.05352   1.241  0.21629    \nregionMENA                           -0.23570    0.05705  -4.131 5.63e-05 ***\nregionSub-Saharan Africa             -0.13946    0.04564  -3.056  0.00261 ** \nregionWestern Europe & North America  0.37554    0.05412   6.939 7.84e-11 ***\nregionAsia & Pacific                 -0.13364    0.05193  -2.573  0.01092 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1976 on 171 degrees of freedom\nMultiple R-squared:  0.4712,    Adjusted R-squared:  0.4557 \nF-statistic: 30.47 on 5 and 171 DF,  p-value: &lt; 2.2e-16\n\n\nThe equation for this model looks like this:\n\\[\\hat{Democracy} = \\beta_0 + \\beta_1 \\cdot LatinAmerica + \\beta_2 \\cdot MENA + \\beta_3 \\cdot SSAfrica + \\beta_4 \\cdot TheWest + \\beta_5 \\cdot Asia\\]\nIn this equation, \\(\\beta_0\\) (the intercept) represents the average democracy level in Eastern Europe (the reference category). Each of the other coefficients represents the difference in average democracy between that region and Eastern Europe. So \\(\\beta_1\\) is the difference in average democracy between Latin America and Eastern Europe, \\(\\beta_2\\) is the difference between MENA and Eastern Europe, and so on.\nSometimes you may want a different reference category. The relevel() function allows you to change which category serves as the baseline. Let’s change the reference category for our analysis to Sub-Saharan Africa (SSA):\n\n# Make Sub-Saharan Africa the reference category\nmodel_data &lt;- model_data |&gt; \n  mutate(region2 = relevel(region, ref = \"Sub-Saharan Africa\"))\n\nlevels(model_data$region2)\n\n[1] \"Sub-Saharan Africa\"             \"Eastern Europe\"                \n[3] \"Latin America\"                  \"MENA\"                          \n[5] \"Western Europe & North America\" \"Asia & Pacific\"                \n\n\nNow when we fit the model again, Sub-Saharan Africa will be the reference category:\n\nregion_model_ssa &lt;- lm(lib_dem ~ region2, data = model_data)\n\nsummary(region_model_ssa)\n\n\nCall:\nlm(formula = lib_dem ~ region2, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45792 -0.13186 -0.02004  0.11414  0.49320 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                            0.295040   0.027950  10.556  &lt; 2e-16 ***\nregion2Eastern Europe                  0.139460   0.045641   3.056  0.00261 ** \nregion2Latin America                   0.205880   0.048410   4.253 3.47e-05 ***\nregion2MENA                           -0.096240   0.052289  -1.841  0.06742 .  \nregion2Western Europe & North America  0.515002   0.049078  10.494  &lt; 2e-16 ***\nregion2Asia & Pacific                  0.005817   0.046649   0.125  0.90091    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1976 on 171 degrees of freedom\nMultiple R-squared:  0.4712,    Adjusted R-squared:  0.4557 \nF-statistic: 30.47 on 5 and 171 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\nWhich regimes have more corruption?\n\nThere is one more categorical variable in our dataset: regime, which represents different types of political regimes. Convert this variable into a factor.\nCheck the levels of the regime variable and identify which category will be the reference category (you can tell because it is the first level). It should be “Closed Autocracy.”\nNow visualize the differences in corruption levels across regime types using a bar plot. Use ggplot() to create a bar plot with regime on the x-axis and average corruption (corruption) on the y-axis.\nNow fit a regression model predicting corruption (corruption) from regime type (regime). What does the model tell you about the relationship between regime type and corruption levels? How would you interpret the coefficients for each regime type (relative to the baseline category)?\nFinally, siwtch the reference category to “Electoral Democracy” and refit the model. How do the coefficients change?"
  },
  {
    "objectID": "modules/module-4.4.html#multiple-predictors",
    "href": "modules/module-4.4.html#multiple-predictors",
    "title": "Module 4.4",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\nNow we are ready to discuss models with multiple predictors. This is an important step because this is where the real power of multiple regression lies because it allows us to control for confounding variables, isolate the effect of specific predictors, build more accurate predictions, and test complex theories.\nThe fundamental logic of multiple regression centers on asking: “What is the relationship between each predictor and the outcome, holding all other predictors constant?” Or sometimes you will hear this idea of holding everything else constant phrased as ceteris paribus which is Latin for “all things being equal.”\nAdding Continuous Predictors\nLet’s start by adding a second predictor (polarization) to the wealth and democracy model that we worked with in the last module. We’ve seen that polarization is negatively related to democracy. But what happens when we also include wealth? We can compare a model with just polarization to a model with both polarization and wealth:\n\nm1_polarization_democracy &lt;- lm(lib_dem ~ polarization + log(wealth), data = model_data)\n\nsummary(m1_polarization_democracy)\n\n\nCall:\nlm(formula = lib_dem ~ polarization + log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65363 -0.16094  0.04912  0.18077  0.42814 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.19571    0.03399   5.758 3.90e-08 ***\npolarization -0.05837    0.01376  -4.242 3.63e-05 ***\nlog(wealth)   0.09439    0.01512   6.242 3.34e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2196 on 170 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.347, Adjusted R-squared:  0.3393 \nF-statistic: 45.17 on 2 and 170 DF,  p-value: &lt; 2.2e-16\n\n\nThe model equation with multiple predictors becomes:\n\\[\\hat{Democracy} = \\beta_0 + \\beta_1 \\cdot Polarization + \\beta_2 \\cdot \\log(Wealth)\\]\nIn this equation, \\(\\beta_0\\) represents the predicted democracy level when both polarization and log(wealth) equal zero. \\(\\beta_1\\) represents the change in democracy for a 1-unit increase in polarization, holding wealth constant. \\(\\beta_2\\) represents the change in democracy for a 1-unit increase in log(wealth), holding polarization constant. The key phrase is “holding other variables constant.”\nIn this case, we see that polarization is associated with a .058 unit deacrease in the democracy score, while log(wealth) is associated with a .098 unit increase in the democracy score. We can tell the direction of the relationship (positive or negative) based on the sign of the coefficient. For polarization the sign is negative and for wealth it is positive.\nWe can continue adding predictors to build even more complex models. Let’s try adding oil rents per capita (oil_rents) to our model. Oil rents are the income a country receives from oil extraction and, as we have already noticed in our scatter plots, they can have significant effects on democracy.\n\nm2_three_predictors  &lt;- lm(lib_dem ~ polarization + log(wealth) + oil_rents, data = model_data)\n\nsummary(m2_three_predictors)\n\n\nCall:\nlm(formula = lib_dem ~ polarization + log(wealth) + oil_rents, \n    data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54170 -0.14140  0.03997  0.13419  0.67472 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.531e-01  3.174e-02   4.823 3.30e-06 ***\npolarization -5.774e-02  1.283e-02  -4.499 1.32e-05 ***\nlog(wealth)   1.309e-01  1.501e-02   8.720 3.65e-15 ***\noil_rents    -4.133e-05  6.074e-06  -6.805 1.98e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1973 on 158 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.486, Adjusted R-squared:  0.4763 \nF-statistic:  49.8 on 3 and 158 DF,  p-value: &lt; 2.2e-16\n\n\nThe model equation now becomes:\n\\[ \\hat{Y_i} = a + b_1*Polarization + b_2*GDPpc + b_3*OilRents \\]\nWhere \\(Y_i\\) is the predicted democracy level for country \\(i\\), \\(a\\) is the intercept, and \\(b_1\\), \\(b_2\\), and \\(b_3\\) are the coefficients for polarization, log(wealth), and oil rents, respectively. Now, the coefficient for \\(OilRents\\) tells us the predicted change in democracy for a 1-unit increase in oil rents, holding polarization and log(wealth) constant. Similarly, the coefficients for polarization and log(wealth) tell us the predicted change in democracy for a 1-unit increase in those variables, while controlling for the other two predictors.\n\n\n\n\n\n\nNote\n\n\n\nNotice how the coefficients might change when we add predictors. This happens for two main reasons. First, confounding occurs when our predictors are correlated with each other. If polarization and wealth are correlated, the simple regression coefficient for polarization includes both the direct effect of polarization and the indirect effect through its correlation with wealth. Second, when we control for other variables, the multiple regression coefficient for polarization shows only the direct effect, after removing the part that’s explained by wealth.\n\n\nCombining Variable Types\nIt is also entirely possible to combine different types of predictors. We can include both categorical variables (like region or regime type) and continuous variables (like wealth or polarization) in the same model.\nWhen we build mixed models that include both types of predictors, the interpretation becomes even richer. Consider a model that includes all of the predictors we have discussed so far: polarization, wealth, oil rents and world regions:\n\nm3_mixed_model &lt;- lm(lib_dem ~ polarization + log(wealth) + oil_rents + region, data = model_data)\n\nsummary(m3_mixed_model)\n\n\nCall:\nlm(formula = lib_dem ~ polarization + log(wealth) + oil_rents + \n    region, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.47437 -0.09500 -0.00534  0.10656  0.39159 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           1.749e-01  5.583e-02   3.132 0.002079 ** \npolarization                         -3.952e-02  1.177e-02  -3.356 0.000996 ***\nlog(wealth)                           1.060e-01  1.933e-02   5.485 1.67e-07 ***\noil_rents                            -2.740e-05  6.150e-06  -4.456 1.61e-05 ***\nregionLatin America                   8.704e-02  4.832e-02   1.801 0.073643 .  \nregionMENA                           -1.693e-01  5.657e-02  -2.994 0.003216 ** \nregionSub-Saharan Africa              3.241e-02  5.120e-02   0.633 0.527725    \nregionWestern Europe & North America  2.261e-01  5.617e-02   4.025 8.92e-05 ***\nregionAsia & Pacific                 -6.081e-02  5.078e-02  -1.197 0.233031    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1745 on 153 degrees of freedom\n  (15 observations deleted due to missingness)\nMultiple R-squared:  0.6104,    Adjusted R-squared:   0.59 \nF-statistic: 29.96 on 8 and 153 DF,  p-value: &lt; 2.2e-16\n\n\nThis model allows us to explore how democracy varies by region while controlling for wealth, polarization, and oil rents.The regional coefficients now represent regional differences in democracy among countries with the same wealth level, polarization and oil rents. This is a much more precise comparison than simply comparing regional averages, because it accounts for the fact that regions differ systematically in their wealth levels.\nMaking Predictions\nOnce we have estimated a multiple regression model, we can use it to make predictions for new cases. To see how this works, let’s return to our model that includes polarization, log-transformed wealth, oil rents, and region as predictors. Suppose we want to predict the level of democracy for a hypothetical country that has a polarization score of 0.5, a GDP per capita of $8.1k (which corresponds to a log wealth of 2.09), oil rents of 2, and is located in Latin America.\nThe regression model gives us an equation that looks like this:\n\\[\n\\hat{Democracy} = a + b_1 \\cdot Polarization + b_2 \\cdot \\log(Wealth) + b_3 \\cdot OilRents + b_4 \\cdot Region\n\\]\nEach coefficient—\\(b_1\\), \\(b_2\\), \\(b_3\\), and so on—represents the effect of a one-unit change in that variable, holding the others constant. To make a prediction, we simply plug in the values: 0.5 for polarization, 9 for log(wealth), 2 for oil rents, and the appropriate regional adjustment for Latin America. The result is the model’s best guess for the democracy score of a country with those characteristics.\nTo do this, we can use R’s predict() function. This function takes a fitted model and a new set of predictor values and returns the predicted outcome:\n\nnew_data &lt;- tibble(\n  polarization = 0.5,\n  wealth = 8.1,  \n  oil_rents = 2,\n  region = \"Latin America\"\n)\n\npredict(m3_mixed_model, newdata = new_data)\n\n        1 \n0.4639407 \n\n\nSo in other words, with these inputs, the model predicts a democracy score of approximately 0.46 for this hypothetical country. This prediction reflects what the model expects, given the relationships it has learned from the original data. It doesn’t guarantee what will happen in the real world, but it gives us a principled estimate based on the variables we think matter.\n\nIf you’re curious about uncertainty, predict() can also provide a confidence interval for the prediction by adding the argument interval = \"confidence\".\n\n\n\n\n\n\n\nYour Turn!!\n\n\n\n\nTry running a multiple linear regression with corruption as the dependent variable.\nThink about what variables you would want to include as predictors. Use a mix of continuous and categorical variables, such as wealth, oil rents, region, and regime type.\nFit the model and interpret the coefficients. What do they tell you about the relationship between these predictors and corruption levels?\nNow try making some predictions with your model. Create a new data frame with hypothetical values for each predictor (e.g., a country with a wealth of $10k, oil rents of 3, and located in Asia). Use the predict() function to estimate the corruption level for this hypothetical country."
  },
  {
    "objectID": "modules/module-4.4.html#conclusion",
    "href": "modules/module-4.4.html#conclusion",
    "title": "Module 4.4",
    "section": "Conclusion",
    "text": "Conclusion\nMultiple linear regression allows us to model complex relationships while controlling for multiple factors at once. In this module, you learned how to include both continuous and categorical predictors, interpret coefficients while holding other variables constant, and use regression to make informed predictions. You also saw how adding variables can change the interpretation of coefficients and why reference categories matter when working with categorical predictors.\nKey lessons include the importance of controlling for confounders, the role of theory in guiding model building, and the need for careful interpretation. More variables don’t always mean a better model—clarity and purpose matter more. In the next modules, we’ll explore interaction effects, check model assumptions, and distinguish between models built for explanation versus those built for prediction."
  },
  {
    "objectID": "modules/module-4.5.html",
    "href": "modules/module-4.5.html",
    "title": "Module 4.5",
    "section": "",
    "text": "Prework\n\n\n\n\nInstall the olsrr package: install.packages(\"olsrr\") and read the documentation pertaining to variable selection\n\nRun the following code to get set up for this module:\n\n\nCodelibrary(tidyverse)\nlibrary(vdemlite)\n\n# Load V-Dem data for 2019\nmodel_data &lt;- fetchdem(\n  indicators = c(\n  \"v2x_libdem\", \n  \"e_gdppc\", \n  \"v2cacamps\",\n  \"v2x_gender\",\n  \"v2x_corr\",\n  \"e_regionpol_6C\"),\n  start_year = 2006, \n  end_year = 2006\n  ) |&gt;\n  rename(\n    country = country_name, \n    lib_dem = v2x_libdem, \n    wealth = e_gdppc,\n    polarization = v2cacamps,\n    women_empowerment = v2x_gender,\n    corruption = v2x_corr,\n    region = e_regionpol_6C\n    ) |&gt;\n  mutate(\n    region = factor(\n    region,\n    labels = c(\n      \"Eastern Europe\", \n      \"Latin America\", \n      \"MENA\", \n      \"Sub-Saharan Africa\", \n      \"Western Europe & North America\", \n      \"Asia & Pacific\")),\n    log_wealth = log(wealth) # have to manually log-transform wealth for olsrr\n    ) |&gt;\n  drop_na(lib_dem:region) # remove missing values for olsrr\n\n#glimpse(model_data)"
  },
  {
    "objectID": "modules/module-4.5.html#model-building-and-selection",
    "href": "modules/module-4.5.html#model-building-and-selection",
    "title": "Module 4.5",
    "section": "Model Building and Selection",
    "text": "Model Building and Selection\nWith multiple predictors available, how do we decide which variables to include in our model? This is one of the most important questions in applied regression analysis.\nWhen building models, we face a fundamental tradeoff known as the bias-variance tradeoff. More predictors can reduce bias by including important variables, but they can also increase variance by overfitting to the specific sample we’re using. Conversely, fewer predictors can reduce variance but increase bias by omitting important variables that truly affect the outcome.\nSeveral strategies can guide variable selection. Theory-driven selection involves including variables based on theoretical understanding of the phenomenon we’re studying. Statistical criteria use measures like AIC, BIC, or adjusted R-squared to balance model fit with complexity. Cross-validation tests model performance on held-out data to assess how well the model generalizes. Domain expertise involves consulting with subject matter experts who understand the substantive area.\nWhen building multiple regression models, we must also consider several important issues. Multicollinearity occurs when predictors are highly correlated with each other, making it difficult to separate their individual effects. Overfitting happens when the model fits the training data too closely and doesn’t generalize well to new data. More complex models are also harder to interpret, and more predictors require larger sample sizes to provide reliable estimates.\nYour Turn! Build Your Own Model\nFor this final exercise, you’ll design and test your own theory about what drives democracy, corruption, or another outcome of interest using the V-Dem data.\nYour tasks:\n\nChoose your research question: What would you like to understand about democracy, governance, or political outcomes? Some ideas include what predicts corruption levels, what drives human rights protection, what factors explain economic inequality, or what predicts civil society strength.\nDevelop your theory: What variables do you think are important? Why? Write a brief paragraph explaining your theoretical expectations.\nSelect your variables: Choose 3-5 predictor variables from the V-Dem dataset. Include a mix of at least one categorical variable, at least one continuous variable, and variables that test your theory.\nExplore your data: Create visualizations showing the relationships between your predictors and outcome.\nBuild your model: Fit a multiple regression model testing your theory.\nInterpret your results: Which predictors are significant? Do the results match your expectations? What do the coefficients tell you substantively?\nAssess model quality: What’s the R-squared? Are there any concerning patterns in the residuals? How confident are you in your conclusions?\nRefine your model: Based on your initial results, would you add or remove any variables? Try fitting an alternative specification.\n\nAvailable variables in the V-Dem dataset include corruption index (v2x_corr), freedom of expression (v2x_freexp), rule of law (v2xcl_rol), civil liberties (v2x_civlib), GDP per capita (e_migdppc), population (e_pop), party ban (v2psbars), opposition autonomy (v2psoppaut), and many more.\n# Explore available variables\nglimpse(vdem)\n\n# Your model building code here\n# Start by exploring the data and developing your theory\nTake 25 minutes to develop, test, and refine your own model. This is your chance to be the researcher!"
  },
  {
    "objectID": "modules/module-4.5.html#overview",
    "href": "modules/module-4.5.html#overview",
    "title": "Module 4.5",
    "section": "Overview",
    "text": "Overview\nNow that we know how to run a regression we can talk about how to select the best model. In this module, we will explore the process of model selection in multiple regression contexts. Model selection involves choosing which predictor variables to include in a regression model to best explain the outcome variable while balancing complexity and interpretability.\nThis decision process involves navigating the fundamental bias-variance tradeoff in statistical modeling. Including more predictors can reduce bias by capturing important relationships between variables and the outcome. However, additional predictors also increase model variance by making the model more susceptible to overfitting to the specific sample under study. Conversely, models with fewer predictors may exhibit reduced variance but increased bias through the omission of important explanatory variables.\nSeveral approaches can guide variable selection decisions. Theory-driven selection incorporates variables based on theoretical understanding of the phenomenon under investigation based on domain expertise. Statistical criteria employ measures such as AIC, BIC, or adjusted R-squared to balance model fit against complexity. Cross-validation assesses model performance on held-out data to evaluate generalizability.\nMultiple regression modeling also requires consideration of several methodological concerns. Multicollinearity occurs when predictor variables exhibit high correlations with one another, complicating the separation of individual variable effects. Overfitting represents the tendency for models to fit training data too closely, resulting in poor generalization to new observations. Additionally, complex models present interpretation challenges, and larger numbers of predictors necessitate correspondingly larger sample sizes for reliable parameter estimation."
  },
  {
    "objectID": "modules/module-4.5.html#model-evaluation-criteria",
    "href": "modules/module-4.5.html#model-evaluation-criteria",
    "title": "Module 4.5",
    "section": "Model Evaluation Criteria",
    "text": "Model Evaluation Criteria\nModel selection procedures require criteria for comparing alternative specifications. Each criterion approaches the balance between model fit and complexity differently, leading to potentially different optimal models.\nAdjusted R-squared\nAdjusted R-squared addresses a fundamental limitation of regular R-squared: the tendency for R-squared to increase mechanically with the addition of any predictor variable, regardless of that variable’s true explanatory value. The adjusted R-squared applies a penalty for additional parameters according to the formula:\n\\[R_{adj}^2 = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1}\\]\nwhere \\(n\\) represents the number of observations and \\(k\\) represents the number of predictor variables. Higher adjusted R-squared values indicate superior model performance, as they reflect greater explained variance while accounting for model complexity.\nAkaike Information Criterion (AIC)\nThe Akaike Information Criterion provides an alternative approach to model evaluation grounded in information theory. AIC balances model fit against complexity through the formula:\n\\[AIC = -2(\\text{log-likelihood}) + 2k\\]\nwhere \\(k\\) represents the number of parameters (including predictors and intercept) and log-likelihood measures model fit to the observed data. Lower AIC values indicate superior models, representing the opposite interpretation from R-squared measures. AIC typically favors slightly more complex models than adjusted R-squared due to its lighter penalty structure for additional parameters.\nP-values as Selection Criteria\nRecall that p-values represent the probability of observing a test statistic as extreme or more extreme than what was actually observed, assuming the null hypothesis is true. In regression contexts, p-values test whether individual regression coefficients differ significantly from zero. p-values have historically been employed in variable selection procedures, with common approaches involving the removal of variables with p-values exceeding predetermined thresholds (typically 0.05 or 0.10).\nHowever, p-value-based variable selection introduces important methodological concerns. Stepwise selection increases the risk of a false positive (Type 1 error) by overfitting to noise. In the standard hypothesis testing framework featuring a five percent critical value, one out of 20 tests will yield a statistically significant result purely by chance. This risk is compounded in stepwise selection procedures that test multiple variables sequentially, as each test increases the likelihood of identifying spurious relationships.\nStepwise regression can also lead to false negatives (Type II error) by excluding variables that may have substantive theoretical importance but appear non-significant in intermediate steps.\nAnother concern is post-selection inference bias resulting from the fact that the same data are used both to select the model and to conduct inference. In this context, the reported p-values and confidence intervals no longer reflect valid hypothesis tests as they understate the true uncertainty associated with the selected model.\nThese limitations suggest that p-value-based selection should be approached with considerable caution, particularly compared to criteria like adjusted R-squared or AIC that explicitly account for the model selection process and do not suffer from the same multiple testing problems.\n\n\n\n\n\n\nModern Approaches to Model Selection\n\n\n\nWhile stepwise selection remains widely employed, contemporary statistical practice increasingly favors alternative approaches that address some limitations of traditional methods.\nRegularization methods such as LASSO and Ridge regression incorporate variable selection directly into the model fitting process. These techniques apply penalties to coefficient magnitudes, automatically shrinking less important predictors toward zero.\nCross-validation approaches evaluate model performance through systematic partitioning of data into training and testing subsets. This strategy provides more reliable assessments of generalizability compared to information criteria calculated on the same data used for model fitting.\nEnsemble methods acknowledge model uncertainty by averaging predictions across multiple plausible models rather than selecting a single optimal specification. These approaches often demonstrate superior predictive performance compared to any individual model.\nThese advanced techniques are important to be aware of even though they exceed the scope of introductory regression courses."
  },
  {
    "objectID": "modules/module-4.5.html#automated-stepwise-selection",
    "href": "modules/module-4.5.html#automated-stepwise-selection",
    "title": "Module 4.5",
    "section": "Automated Stepwise Selection",
    "text": "Automated Stepwise Selection\nAutomated selection procedures can systematically implement variable selection strategies. This section demonstrates these approaches using data from the Varieties of Democracy (V-Dem) project to predict liberal democracy scores across countries.\nThe analysis begins with estimation of a full model incorporating all available predictors:\n\nfull_model &lt;- lm(lib_dem ~ log_wealth + polarization + women_empowerment + \n                 corruption + region, data = model_data)\n\nsummary(full_model)\n\n\nCall:\nlm(formula = lib_dem ~ log_wealth + polarization + women_empowerment + \n    corruption + region, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43252 -0.06331  0.00253  0.07402  0.26311 \n\nCoefficients:\n                                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           0.125962   0.093258   1.351   0.1787    \nlog_wealth                           -0.001450   0.014026  -0.103   0.9178    \npolarization                         -0.009154   0.008323  -1.100   0.2730    \nwomen_empowerment                     0.640860   0.072556   8.833 1.61e-15 ***\ncorruption                           -0.344607   0.055068  -6.258 3.35e-09 ***\nregionLatin America                   0.062175   0.033837   1.837   0.0680 .  \nregionMENA                           -0.004997   0.043303  -0.115   0.9083    \nregionSub-Saharan Africa             -0.011206   0.034987  -0.320   0.7491    \nregionWestern Europe & North America  0.109096   0.039341   2.773   0.0062 ** \nregionAsia & Pacific                 -0.029083   0.036100  -0.806   0.4216    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1228 on 162 degrees of freedom\nMultiple R-squared:  0.8055,    Adjusted R-squared:  0.7947 \nF-statistic: 74.55 on 9 and 162 DF,  p-value: &lt; 2.2e-16\n\n\nThe full model achieves an adjusted R-squared of 0.795, indicating that approximately 79.5 percent of the variance in democracy scores can be explained by the included predictors.\nForward Selection\nForward selection begins with an intercept-only model and sequentially adds predictors. At each step, the procedure selects the variable that produces the greatest improvement in the chosen criterion (adjusted R-squared in this example):\n\nlibrary(olsrr)\n\nforward_model &lt;- ols_step_forward_adj_r2(full_model)\n\nforward_model\n\n\n                                  Stepwise Summary                                   \n-----------------------------------------------------------------------------------\nStep    Variable               AIC         SBC         SBIC        R2       Adj. R2 \n-----------------------------------------------------------------------------------\n 0      Base Model             41.896      48.191    -449.065    0.00000    0.00000 \n 1      women_empowerment    -131.886    -122.444    -622.069    0.64012    0.63801 \n 2      corruption           -217.702    -205.112    -706.085    0.78402    0.78146 \n 3      region               -224.442    -196.115    -720.133    0.80405    0.79568 \n 4      polarization         -223.726    -192.251    -719.231    0.80550    0.79596 \n-----------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.897       RMSE                  0.119 \nR-Squared               0.806       MSE                   0.014 \nAdj. R-Squared          0.796       Coef. Var            29.792 \nPred R-Squared          0.782       AIC                -223.726 \nMAE                     0.090       SBC                -192.251 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n               Sum of                                               \n              Squares         DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression     10.111          8          1.264    84.382    0.0000 \nResidual        2.441        163          0.015                     \nTotal          12.553        171                                    \n--------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n                               model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n-----------------------------------------------------------------------------------------------------------------\n                         (Intercept)     0.120         0.076                  1.580    0.116    -0.030     0.271 \n                   women_empowerment     0.641         0.072        0.471     8.898    0.000     0.499     0.784 \n                          corruption    -0.342         0.047       -0.387    -7.214    0.000    -0.435    -0.248 \n                 regionLatin America     0.063         0.033        0.082     1.871    0.063    -0.003     0.129 \n                          regionMENA    -0.005         0.043       -0.006    -0.128    0.899    -0.090     0.079 \n            regionSub-Saharan Africa    -0.009         0.030       -0.016    -0.311    0.756    -0.069     0.050 \nregionWestern Europe & North America     0.109         0.039        0.139     2.796    0.006     0.032     0.185 \n                regionAsia & Pacific    -0.028         0.035       -0.038    -0.811    0.418    -0.096     0.040 \n                        polarization    -0.009         0.008       -0.045    -1.105    0.271    -0.026     0.007 \n-----------------------------------------------------------------------------------------------------------------\n\n\nThe forward selection results demonstrate the sequential addition process. Women’s empowerment enters first due to its highest individual contribution to adjusted R-squared, followed by corruption control, with the process continuing until no remaining variables provide meaningful improvements to model fit.\nBackward Elimination\nBackward elimination employs the opposite strategy, beginning with the full model and sequentially removing predictors. At each step, the procedure eliminates the variable whose removal least deteriorates model performance:\n\nbackward_model &lt;- ols_step_backward_adj_r2(full_model)\n\nbackward_model\n\n\n                               Stepwise Summary                               \n----------------------------------------------------------------------------\nStep    Variable        AIC         SBC         SBIC        R2       Adj. R2 \n----------------------------------------------------------------------------\n 0      Full Model    -221.737    -187.115    -717.119    0.80552    0.79471 \n 1      log_wealth    -223.726    -192.251    -719.294    0.80550    0.79596 \n----------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.897       RMSE                  0.119 \nR-Squared               0.806       MSE                   0.014 \nAdj. R-Squared          0.796       Coef. Var            29.792 \nPred R-Squared          0.782       AIC                -223.726 \nMAE                     0.090       SBC                -192.251 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n               Sum of                                               \n              Squares         DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression     10.111          8          1.264    84.382    0.0000 \nResidual        2.441        163          0.015                     \nTotal          12.553        171                                    \n--------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n                               model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n-----------------------------------------------------------------------------------------------------------------\n                         (Intercept)     0.120         0.076                  1.580    0.116    -0.030     0.271 \n                        polarization    -0.009         0.008       -0.045    -1.105    0.271    -0.026     0.007 \n                   women_empowerment     0.641         0.072        0.471     8.898    0.000     0.499     0.784 \n                          corruption    -0.342         0.047       -0.387    -7.214    0.000    -0.435    -0.248 \n                 regionLatin America     0.063         0.033        0.082     1.871    0.063    -0.003     0.129 \n                          regionMENA    -0.005         0.043       -0.006    -0.128    0.899    -0.090     0.079 \n            regionSub-Saharan Africa    -0.009         0.030       -0.016    -0.311    0.756    -0.069     0.050 \nregionWestern Europe & North America     0.109         0.039        0.139     2.796    0.006     0.032     0.185 \n                regionAsia & Pacific    -0.028         0.035       -0.038    -0.811    0.418    -0.096     0.040 \n-----------------------------------------------------------------------------------------------------------------\n\n\nBidirectional Selection\nBidirectional selection combines forward and backward approaches, allowing both addition and removal of variables at each step. This method can potentially identify optimal models that pure forward or backward procedures might miss:\n\nboth_model &lt;- ols_step_both_adj_r2(full_model)\n\nboth_model\n\n\n                                    Stepwise Summary                                     \n---------------------------------------------------------------------------------------\nStep    Variable                   AIC         SBC         SBIC        R2       Adj. R2 \n---------------------------------------------------------------------------------------\n 0      Base Model                 41.896      48.191    -449.065    0.00000    0.00000 \n 1      women_empowerment (+)    -131.886    -122.444    -622.069    0.64012    0.63801 \n 2      corruption (+)           -217.702    -205.112    -706.085    0.78402    0.78146 \n 3      region (+)               -224.442    -196.115    -720.133    0.80405    0.79568 \n 4      polarization (+)         -223.726    -192.251    -719.231    0.80550    0.79596 \n---------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.897       RMSE                  0.119 \nR-Squared               0.806       MSE                   0.014 \nAdj. R-Squared          0.796       Coef. Var            29.792 \nPred R-Squared          0.782       AIC                -223.726 \nMAE                     0.090       SBC                -192.251 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n               Sum of                                               \n              Squares         DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression     10.111          8          1.264    84.382    0.0000 \nResidual        2.441        163          0.015                     \nTotal          12.553        171                                    \n--------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n                               model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n-----------------------------------------------------------------------------------------------------------------\n                         (Intercept)     0.120         0.076                  1.580    0.116    -0.030     0.271 \n                   women_empowerment     0.641         0.072        0.471     8.898    0.000     0.499     0.784 \n                          corruption    -0.342         0.047       -0.387    -7.214    0.000    -0.435    -0.248 \n                 regionLatin America     0.063         0.033        0.082     1.871    0.063    -0.003     0.129 \n                          regionMENA    -0.005         0.043       -0.006    -0.128    0.899    -0.090     0.079 \n            regionSub-Saharan Africa    -0.009         0.030       -0.016    -0.311    0.756    -0.069     0.050 \nregionWestern Europe & North America     0.109         0.039        0.139     2.796    0.006     0.032     0.185 \n                regionAsia & Pacific    -0.028         0.035       -0.038    -0.811    0.418    -0.096     0.040 \n                        polarization    -0.009         0.008       -0.045    -1.105    0.271    -0.026     0.007 \n-----------------------------------------------------------------------------------------------------------------\n\n\nComparison of the full and stepwise-selected models reveals how little changes in explanatory power despite model simplification. The adjusted R-squared for the full model is 0.7947, while the adjusted R-squared for the model selected via stepwise procedures is 0.796. At the same time, the number of predictors decreases slightly, from 9 in the full model to 8 in the selected model.\nThis minimal difference in fit highlights an important point: variable selection procedures may reduce complexity but often yield models with similar performance. This highlights the relevance of domain expertise in making meaningful modeling decisions."
  },
  {
    "objectID": "modules/module-4.5.html#incorporating-domain-knowledge",
    "href": "modules/module-4.5.html#incorporating-domain-knowledge",
    "title": "Module 4.5",
    "section": "Incorporating Domain Knowledge",
    "text": "Incorporating Domain Knowledge\nStatistical criteria alone provide insufficient guidance for model specification. Substantive theory and domain expertise must inform variable selection decisions. In democracy research, certain variables maintain theoretical importance regardless of their statistical significance in particular samples.\nFor example, modernization theory establishes wealth (GDP per capita) as a fundamental predictor of democratic development while comparative political analysis routinely includes regional controls to account for spatial diffusion effects and shared historical experiences.\nTo this end, the olsrr package accommodates theoretical constraints through the include argument, which forces retention of specified variables throughout the selection process:\n\ntheory_guided_model &lt;- ols_step_both_adj_r2(full_model, include = c(\"log_wealth\", \"region\"))\n\ntheory_guided_model\n\n\n                                    Stepwise Summary                                     \n---------------------------------------------------------------------------------------\nStep    Variable                   AIC         SBC         SBIC        R2       Adj. R2 \n---------------------------------------------------------------------------------------\n 0      Base Model                -76.144     -50.964    -576.247    0.53048    0.51341 \n 1      women_empowerment (+)    -180.455    -152.127    -678.110    0.74694    0.73614 \n 2      corruption (+)           -222.457    -190.982    -718.039    0.80406    0.79445 \n 3      polarization (+)         -221.737    -187.115    -717.119    0.80552    0.79471 \n---------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.898       RMSE                  0.119 \nR-Squared               0.806       MSE                   0.014 \nAdj. R-Squared          0.795       Coef. Var            29.883 \nPred R-Squared          0.779       AIC                -221.737 \nMAE                     0.090       SBC                -187.115 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n               Sum of                                               \n              Squares         DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression     10.111          9          1.123    74.552    0.0000 \nResidual        2.441        162          0.015                     \nTotal          12.553        171                                    \n--------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n                               model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n-----------------------------------------------------------------------------------------------------------------\n                         (Intercept)     0.126         0.093                  1.351    0.179    -0.058     0.310 \n                          log_wealth    -0.001         0.014       -0.006    -0.103    0.918    -0.029     0.026 \n                 regionLatin America     0.062         0.034        0.081     1.837    0.068    -0.005     0.129 \n                          regionMENA    -0.005         0.043       -0.006    -0.115    0.908    -0.091     0.081 \n            regionSub-Saharan Africa    -0.011         0.035       -0.019    -0.320    0.749    -0.080     0.058 \nregionWestern Europe & North America     0.109         0.039        0.140     2.773    0.006     0.031     0.187 \n                regionAsia & Pacific    -0.029         0.036       -0.039    -0.806    0.422    -0.100     0.042 \n                   women_empowerment     0.641         0.073        0.471     8.833    0.000     0.498     0.784 \n                          corruption    -0.345         0.055       -0.391    -6.258    0.000    -0.453    -0.236 \n                        polarization    -0.009         0.008       -0.045    -1.100    0.273    -0.026     0.007 \n-----------------------------------------------------------------------------------------------------------------\n\n\nThe mandatory inclusion of wealth and regional indicators alters the selection process, demonstrating how theoretical considerations should guide rather than merely supplement statistical procedures. In this case, the theory guided approach essentially brought us back to the full model, but in other contexts you may find that the model is more parsimonious while still retaining theoretically important variables.\n\n\n\n\n\n\nYour Turn!!\n\n\n\nApply model selection using AIC as the evaluation criterion for the democracy data by using the ols_step_both_aic() function from the olsrr package.\n\nPerform bidirectional stepwise selection using AIC\nCompare the selected variables to those chosen by adjusted R-squared\nCalculate and compare AIC values for both the full model and your selected model\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Bidirectional selection using AIC\naic_model &lt;- ols_step_both_aic(full_model)\naic_model\n\n\n                                    Stepwise Summary                                     \n---------------------------------------------------------------------------------------\nStep    Variable                   AIC         SBC         SBIC        R2       Adj. R2 \n---------------------------------------------------------------------------------------\n 0      Base Model                 41.896      48.191    -449.065    0.00000    0.00000 \n 1      women_empowerment (+)    -131.886    -122.444    -622.069    0.64012    0.63801 \n 2      corruption (+)           -217.702    -205.112    -706.085    0.78402    0.78146 \n 3      region (+)               -224.442    -196.115    -720.133    0.80405    0.79568 \n---------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                           \n----------------------------------------------------------------\nR                       0.897       RMSE                  0.120 \nR-Squared               0.804       MSE                   0.014 \nAdj. R-Squared          0.796       Coef. Var            29.812 \nPred R-Squared          0.784       AIC                -224.442 \nMAE                     0.090       SBC                -196.115 \n----------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                               ANOVA                                 \n--------------------------------------------------------------------\n               Sum of                                               \n              Squares         DF    Mean Square      F         Sig. \n--------------------------------------------------------------------\nRegression     10.093          7          1.442    96.133    0.0000 \nResidual        2.460        164          0.015                     \nTotal          12.553        171                                    \n--------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n                               model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n-----------------------------------------------------------------------------------------------------------------\n                         (Intercept)     0.135         0.075                  1.798    0.074    -0.013     0.283 \n                   women_empowerment     0.640         0.072        0.470     8.874    0.000     0.498     0.783 \n                          corruption    -0.359         0.045       -0.407    -8.050    0.000    -0.447    -0.271 \n                 regionLatin America     0.064         0.033        0.083     1.900    0.059    -0.003     0.130 \n                          regionMENA    -0.007         0.043       -0.008    -0.163    0.871    -0.092     0.078 \n            regionSub-Saharan Africa    -0.013         0.030       -0.021    -0.430    0.668    -0.072     0.046 \nregionWestern Europe & North America     0.110         0.039        0.141     2.822    0.005     0.033     0.186 \n                regionAsia & Pacific    -0.031         0.035       -0.041    -0.884    0.378    -0.099     0.038 \n-----------------------------------------------------------------------------------------------------------------\n\n# Compare AIC values\ncat(\"Full Model AIC:\", round(AIC(full_model), 2), \"\\n\")\n\nFull Model AIC: -221.74 \n\ncat(\"Selected Model AIC:\", round(AIC(aic_model$model), 2), \"\\n\")\n\nSelected Model AIC: -224.44 \n\n# Compare which variables were selected\ncat(\"\\nVariables in Adjusted R-squared model:\", \n    paste(names(coef(both_model$model))[-1], collapse = \", \"), \"\\n\")\n\n\nVariables in Adjusted R-squared model: women_empowerment, corruption, regionLatin America, regionMENA, regionSub-Saharan Africa, regionWestern Europe & North America, regionAsia & Pacific, polarization \n\ncat(\"Variables in AIC model:\", \n    paste(names(coef(aic_model$model))[-1], collapse = \", \"), \"\\n\")\n\nVariables in AIC model: women_empowerment, corruption, regionLatin America, regionMENA, regionSub-Saharan Africa, regionWestern Europe & North America, regionAsia & Pacific"
  },
  {
    "objectID": "modules/module-4.5.html#additional-methodological-considerations",
    "href": "modules/module-4.5.html#additional-methodological-considerations",
    "title": "Module 4.5",
    "section": "Additional Methodological Considerations",
    "text": "Additional Methodological Considerations\nMulticollinearity\nMulticollinearity occurs when predictor variables exhibit substantial correlations with one another. This condition creates several analytical problems including unstable coefficient estimates, inflated standard errors that reduce the power to detect significant relationships, and interpretation difficulties when attempting to isolate individual variable effects.\nVariance Inflation Factors (VIF) provide a diagnostic tool for assessing multicollinearity severity:\nWe can use the ols_vif_tol() function from the olsrr package to calculate VIF values for the full model:\n\nols_vif_tol(full_model)\n\n                             Variables Tolerance      VIF\n1                           log_wealth 0.3106153 3.219416\n2                         polarization 0.7322931 1.365573\n3                    women_empowerment 0.4230084 2.364019\n4                           corruption 0.3080812 3.245898\n5                  regionLatin America 0.6160077 1.623356\n6                           regionMENA 0.4755160 2.102979\n7             regionSub-Saharan Africa 0.3557731 2.810780\n8 regionWestern Europe & North America 0.4714850 2.120958\n9                 regionAsia & Pacific 0.5080389 1.968353\n\n\nConventional guidelines suggest that VIF values exceeding 5-10 indicate problematic multicollinearity requiring remedial action. Here we do not see any variables exceeding that threshold so we should feel comfortable ruling out multicollinearity as a major concern in this model.\n\n\n\n\n\n\nNote\n\n\n\nAnother common package for calculating VIF is car, which provides the vif() function. You can use it by simply loading car (after you have installed it of course!) and running vif(full_model).\n\n\nExamination of correlations among continuous predictors provides additional insight into potential multicollinearity issues:\n\n# Correlation matrix for numeric predictors\nmodel_data |&gt;\n  select(log_wealth, polarization, women_empowerment, corruption) |&gt;\n  cor() |&gt;\n  round(3)\n\n                  log_wealth polarization women_empowerment corruption\nlog_wealth             1.000       -0.381             0.340     -0.674\npolarization          -0.381        1.000            -0.312      0.500\nwomen_empowerment      0.340       -0.312             1.000     -0.594\ncorruption            -0.674        0.500            -0.594      1.000\n\n\nHere we would be looking for correlations exceeding 0.7 or 0.8, which would indicate potential multicollinearity concerns. In this case, we do not see any such high correlations, again giving us confidence to proceed with the analysis.\nSample Size Requirements\nAdequate sample sizes ensure reliable parameter estimation in multiple regression models. The conventional guideline recommends minimum ratios of 10-15 observations per predictor variable. The current analysis includes 172 observations and 9 predictors, yielding a ratio of 19.1:1, which meets standard adequacy thresholds.\nModel Validation\nSelected models require validation to assess their reliability and generalizability. Residual analysis examines whether regression assumptions hold in the fitted model. Cross-validation techniques test model performance on independent data subsets. Out-of-sample prediction provides the strongest test of model generalizability when feasible. Substantive interpretation ensures that statistical results align with theoretical expectations and domain knowledge. We will discuss some of these issues in greater detail in a subsequent module on regression diagnostics."
  },
  {
    "objectID": "modules/module-4.5.html#summary",
    "href": "modules/module-4.5.html#summary",
    "title": "Module 4.5",
    "section": "Summary",
    "text": "Summary\nModel selection involves balancing competing objectives of explanatory power and parsimony. Multiple evaluation criteria exist, including adjusted R-squared, AIC, and p-values, each with distinct properties that can lead to different optimal models. Automated stepwise procedures provide systematic approaches to variable selection, though they should supplement rather than replace theoretical reasoning.\nDomain knowledge plays a crucial role in model specification, often requiring the retention of theoretically important variables regardless of their statistical performance in particular samples. Additional methodological considerations include multicollinearity assessment, sample size adequacy, and model validation procedures.\nThe ultimate goal of model selection extends beyond maximizing statistical criteria to developing models that are simultaneously statistically sound and substantively meaningful. This dual objective requires integrating statistical techniques with theoretical understanding and domain expertise throughout the modeling process."
  }
]