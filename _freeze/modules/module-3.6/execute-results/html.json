{
  "hash": "d065b9eb17de228ff7a17bd5b8df28e3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Module 3.6\"\nsubtitle: \"Differences Between Two Groups\"\nformat: \n  html:\n    code-link: true\nfilters:\n  - webr\nhighlight-style: atom-one\nexecute: \n  echo: true\n  message: false\n  warning: false\n---\n\n## Overview\n\nIn this module, you'll learn how to test whether relationships exist between two variables using permutation tests. We'll explore this through a case study examining potential racial discrimination in hiring practices, using data from résumés sent to employers with randomly assigned names. Here is a video that introduces the concept of using bootstrapping methods to evaluate discrimination but through the lense of gender:  \n\n{{< video https://www.youtube.com/watch?v=2pHhjx9hyM4 title = \"Using Randomization to Analyze Gender Discrimination\">}}\n\n## Understanding Hypotheses for Group Comparisons\n\nWhen we want to determine whether a treatment or grouping variable has a real effect on an outcome, we need to set up two competing hypotheses. The **null hypothesis** states that there is no relationship between treatment and outcome, meaning any difference we observe is due to chance. The **alternative hypothesis** proposes that there is a genuine relationship, and the difference is not due to chance alone.\n\nThe key insight behind our approach is that under the null hypothesis, treatment has no impact on the outcome variable. This means that if we were to change the values of the treatment variable, the values on the outcome would stay the same. We can use this logic to simulate what we would expect to see if there truly was no effect.\n\nOur strategy involves reshuffling the treatment variable, calculating the treatment effect, and repeating this process many times. This allows us to ask a fundamental question: how likely would we be to observe the treatment effect in our data if there really is no effect of the treatment?\n\n## The Résumé Experiment\n\nTo illustrate these concepts, we'll examine a study by Bertrand and Mullainathan that investigated racial discrimination in responses to job applications in Chicago and Boston. The researchers sent 4,870 résumés to potential employers, randomly assigning names associated with different racial groups to otherwise identical résumés.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(openintro)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nresume_dta <- resume \n```\n:::\n\n\n## Analyzing Callback Rates by Race\n\nSince race of applicant was randomly assigned in this experiment, any systematic differences in callback rates can be attributed to the racial associations of the names. Let's examine the callback rates for each group:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeans <- resume_dta |>\n  group_by(race) |> \n  summarize(calls = mean(received_callback))\n\nmeans\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  race   calls\n  <chr>  <dbl>\n1 black 0.0645\n2 white 0.0965\n```\n\n\n:::\n:::\n\n\nWe can save these means for easier access and then calculate the treatment effect, which is simply the difference in means between the two groups:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_white = means$calls[2]\nmean_black = means$calls[1]\n\nteffect <- mean_white - mean_black\nteffect\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03203285\n```\n\n\n:::\n:::\n\n\n## Examining the Data with Confidence Intervals\n\nBefore conducting formal hypothesis tests, it's valuable to examine both the point estimates and their confidence intervals. This gives us a sense of the precision of our estimates and whether the observed differences might be meaningful. \n\nLet's start by calculating the mean callback rates for each racial group:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap CIs for black applicants\nci_black <- resume_dta |>\n  filter(race == \"black\") |>\n  specify(response = received_callback) |>\n  generate(reps = 10000, type = \"bootstrap\") |>\n  calculate(stat = \"mean\") |>\n  get_ci(level = 0.95)\n\n# Bootstrap CIs for white applicants  \nci_white <- resume_dta |>\n  filter(race == \"white\") |>\n  specify(response = received_callback) |>\n  generate(reps = 10000, type = \"bootstrap\") |>\n  calculate(stat = \"mean\") |>\n  get_ci(level = 0.95)\n```\n:::\n\n\n::: {.callout-important}\n## Understanding the Code\n\nThese two code chunks create bootstrap confidence intervals for the callback rates of Black and White applicants in a resume audit study. `specify()` identifies the response variable of interest (`received_callback`), `generate()` creates 10,000 bootstrap resamples for each group, `calculate(stat = \"mean\")` computes the callback rate in each resample and `get_ci()` calculates a 95% confidence interval from these bootstrap statistics.\n:::\n\nNext, let's put together the means and confidence intervals for both groups into a tibble so we can visualize them together:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine for plotting\nplot_dta <- tibble(\n  race = c(\"black\", \"white\"),\n  mean_calls = c(mean_black, mean_white),\n  lower_95 = c(ci_black$lower_ci, ci_white$lower_ci),\n  upper_95 = c(ci_black$upper_ci, ci_white$upper_ci)\n)\n\nplot_dta\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  race  mean_calls lower_95 upper_95\n  <chr>      <dbl>    <dbl>    <dbl>\n1 black     0.0645   0.0546   0.0743\n2 white     0.0965   0.0850   0.108 \n```\n\n\n:::\n:::\n\n\nNow we can use `ggplot2` to create a visualization that shows both the callback rates and their uncertainty:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(plot_dta, aes(\n  y = mean_calls, \n  x = race, \n  ymin = lower_95, \n  ymax = upper_95\n  )) +\n  geom_col(fill = \"steelblue4\") +\n  geom_errorbar(width = .05) +\n  theme_bw()  +\n ylim(0, .15) +\n  labs(x = \"Race of Applicant\",\n       y = \"Call Back Rate\")\n```\n\n::: {.cell-output-display}\n![](module-3.6_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-important}\n## Understanding the Code\n\nThis `ggplot` creates a bar chart with error bars to visualize callback rates by race. The `geom_col()` function creates bars showing the mean callback rates, while `geom_errorbar()` adds the 95% confidence intervals. The `ymin` and `ymax` aesthetics define the error bar endpoints. We use `theme_bw()` for a clean appearance and `ylim()` to set consistent y-axis limits for better comparison between groups.\n:::\n\nLooking at this plot, we can see clear differences between the groups and non-overlapping confidence intervals, which suggests there may be evidence of racial discrimination. But how can we formally test the null hypothesis to decide whether to reject it?\n\n## The Logic of Permutation Testing\n\nTo conduct a formal hypothesis test, we need to simulate what would happen under the null hypothesis. Our approach involves calculating the difference in means between white and black applicants, then shuffling the race variable and calculating the difference in means for the shuffled data. By repeating this process many times, we can simulate the null distribution of differences in callbacks.\n\n## Understanding the Permutation Process\n\nLet's walk through this process using a simplified hypothetical example with just six applicants:\n\n### Hypothetical Original Data \n\n| Applicant | Race  | Callback |\n|-----------|-------|----------|\n| A         | Black | Yes      |\n| B         | Black | No       |\n| C         | Black | No       |\n| D         | White | Yes      |\n| E         | White | No       |\n| F         | White | No       |\n\nThe first step is to calculate the original difference in callback rates. This establishes our baseline understanding of the initial association between race and callback rates.\n\nThe second step involves shuffling or permuting the race variable. We randomly reassign race labels while keeping the callback outcomes exactly the same. This simulation reflects what we would expect to see if race truly had no effect on callbacks.\n\n### Hypothetical Shuffled Data\n\n| Applicant | Race (Shuffled) | Callback |\n|-----------|-----------------|----------|\n| A         | White           | Yes      |\n| B         | Black           | No       |\n| C         | White           | No       |\n| D         | White           | Yes      |\n| E         | Black           | No       |\n| F         | Black           | No       |\n\nAfter shuffling, we calculate the difference in callback rates again between the Black and White groups. This tells us what kind of difference we might observe purely due to chance.\n\nWe repeat this shuffling process thousands of times to generate a distribution of differences that could occur by chance alone. If our observed difference is extreme compared to this null distribution (meaning the p-value is low), we have strong evidence to reject the null hypothesis.\n\n## Implementing the Permutation Test\n\nIn practice, we use the `tidymodels` package to handle the computational details of this simulation. The `infer` package provides a clean workflow for permutation testing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist <- resume_dta |>\n  specify(response = received_callback, explanatory = race) |>\n  hypothesize(null = \"independence\") |>\n  generate(10000, type = \"permute\") |>\n  calculate(stat = \"diff in means\", \n            order = c(\"white\", \"black\"))\n```\n:::\n\n\nOnce we have our null distribution, we can calculate the p-value using the `get_pvalue` function from the `infer` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_p_value(null_dist, obs_stat = teffect, direction = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0\n```\n\n\n:::\n:::\n\nHere we get a p-value of exactly zero, suggesting that none of the simulated differences in means was as extreme as the observed difference. This indicates that the observed racial gap in callbacks is highly unlikely to have occurred by chance alone.\n\n::: {.callout-note}\nYou may notice a message saying to \"be careful of reporting a p-value of zero\" and that this is an artifact of the number of reps chosen in the `generate()` step. If you increase the number of reps, you may get a p-value that is very close to zero but not exactly zero because the p-value is calculated as the proportion of simulated differences that are greater than or equal to the observed difference. But if you have the patience (or a fast computer), you can increase the number of reps to 100,000 or more and see what happens!\n:::\n\n## Visualizing the Results\n\nWe can visualize our null distribution along with our observed statistic to better understand our results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualize(null_dist) +\n  shade_p_value(obs_stat = teffect, direction = \"greater\") +\n  labs(\n    x = \"Estimated Difference under the Null\",\n    y = \"Count\"\n  ) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](module-3.6_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Drawing Conclusions\n\nThe p-value we obtained is very small, well below the conventional 0.05 threshold. This means that if there were truly no racial discrimination, we would almost never observe a difference as large as what we found in the data. Therefore, we reject the null hypothesis and conclude that the racial gap is extremely unlikely to have occurred due to chance alone. This provides statistical evidence of racial discrimination in hiring.\n\n::: {.callout-warning icon=false}\n## Your Turn!\n\nNow apply these same methods to investigate a different question using the same dataset:\n\n- Use the **gender** variable in the `resume` data to assess whether there is gender discrimination in call backs\n- Plot means and 95% confidence intervals for the call back rate for men and women\n- Write the null and alternative hypotheses\n- Simulate the null distribution\n- Visualize the null distribution and the gender gap\n- Calculate the p-value\n- What do you conclude from your test?\n:::",
    "supporting": [
      "module-3.6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}