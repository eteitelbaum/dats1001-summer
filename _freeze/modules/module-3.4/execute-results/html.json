{
  "hash": "3e790289f2a76dfaa21cf42dce666745",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Module 3.4\"\nsubtitle: \"Sampling and Uncertainty\"\nformat: \n  html:\n    code-link: true\nfilters:\n  - webr\nhighlight-style: atom-one\nexecute: \n  echo: true\n  message: false\n  warning: false\n---\n\n::: {.callout-tip}\n## Prework\n\nComplete the following before diving into this module:\n\n- Install the `tidymodels` package in R. This includes the `infer` and `rsample` packages, which we will use to simulate sampling and construct confidence intervals.\n- Review the documentation for the [`infer` package](https://infer.tidymodels.org/) and the [`rsample` package](https://rsample.tidymodels.org/).\n- Think about sampling. What does the word \"sample\" mean to you? What is a sample you have encountered in your own life? What larger group was that sample meant to represent?\n:::\n\n## Overview\n\nIn this module, we will explore the concept of *statistical inference*, which is the process through which we use samples to make informed guesses about a broader population. Since we rarely have access to data from every individual or item in a population, we must rely on samples to estimate quantities of interest, like proportions or averages.\n\nBut sampling introduces uncertainty. What if your sample isn't typical? How much might your estimate differ if you sampled again?\n\nWe’ll build an understanding of what it means to sample from a population, how repeated sampling leads to a *sampling distribution*, and how to quantify uncertainty using standard errors and confidence intervals. We explore two ways to estimate uncertainty: 1) using mathematical formulas; and 2) using a computational approach called *bootstrapping*.\n\nAlong the way, you'll simulate sampling, compute estimates, and build confidence intervals with R. By the end, you’ll be able to explain what a confidence interval means and construct one from data using the `infer` package.\n\nLet’s begin by understanding what sampling is, and why it’s essential to doing data science.\n\n## What is Sampling?\n\nImagine trying to learn something about a large group of people, like how many hours college students sleep each night. You could try asking every student on Earth, but that's not very practical. Instead, you look at a smaller group that hopefully represents the whole or, in other words, you take a *sample*. \n\nThis is the core idea of sampling: selecting a subset of individuals from a larger population to learn something about the population as a whole. The **population** is the full group you're interested in studying. The **sample** is a smaller subset of the population that you actually observe.\n\nRelated to this, we have two important concepts: a **parameter** and a **statistic**. A parameter is a true, but usually unknown, characteristic of the population (like the actual average sleep time). A **statistic** is the number you compute from your sample (like the sample average sleep time).\n\nYour goal as a data scientist is to use statistics to make educated guesses about parameters. This process is what we refer to as **inference**.\n\n### The Target Population and Sampling Frame\n\nIn practice, it’s important to define your **target population**—the group you want to learn about. Then you have to find a **sampling frame**, which is the actual list or method you use to select your sample.\n\nFor example:\n\n* **Target population**: All high school students in the U.S.\n* **Sampling frame**: Students enrolled in a particular school district's database\n\nOften, the sampling frame doesn’t perfectly match the target population. This mismatch can introduce **sampling bias**, which we'll revisit in a future module.\n\nNow that you understand what sampling is, let’s explore what happens when we repeat the sampling process.\n\n### Sampling Distributions and Uncertainty\n\nIf you take a single sample from a population and compute a statistic—say, the average—what do you get? One answer. But what if you took a different sample? Would your answer be the same? Probably not.\n\nThis variability is the heart of **sampling distributions**: the idea that every time you take a sample and compute a statistic, the result can change. If you repeat the process many times, those statistics themselves form a distribution.\n\n::: {.callout-warning icon=false}\n## Activity: Sampling with M&Ms\nImagine you have a big bowl of M&Ms in front of you. You reach in and grab 20 at random. Count how many are blue and calculate the proportion. Now, put them all back and grab 20 more. Do it again. And again.\n\nEach time you grab a handful, you’re taking a **sample**. Because you're putting them back in the bowl each time, this is called **sampling with replacement**. You’ll notice the proportion of blue M&Ms changes slightly from sample to sample.\n\nTry this at home if you have M&M fun packs where each fun pack represents a random sample of M&Ms. Record the proportion of blue M&Ms (your sample statistic) in a CSV file. This is your sampling distribution. You can even upload your CSV to R and use your data to create a histogram of the proportions.\n\nNow take an average of all of your recorded sample statistics. How close does it come to the actual known population parameter (the proportion of M&Ms that are blue)? \n\nThis simple activity mirrors what we mean by repeated sampling and helps you build intuition for how a **sampling distribution** works.\n:::\n\n## Standard Error: Measuring the Spread\n\nThe spread of this sampling distribution is called the **standard error (SE)**. It tells us, on average, how much a statistic varies from one sample to another. A smaller SE means more precise estimates while a larger SE means more variability from sample to sample.\n\nStandard errors depend on both the sample size and the variability in the data. Bigger samples tend to produce smaller standard errors.\n\nUnderstanding this variability is key to making smart inferences. Next, we’ll learn how to use this idea to construct **confidence intervals**—a powerful way to express uncertainty in our estimates.\n\n## Estimating Uncertainty with Confidence Intervals \n\nA **confidence interval** gives us a range of values within which we believe the true population parameter likely falls. It’s based on the idea of the sampling distribution and its standard error.\n\n{{< video https://www.youtube.com/watch?v=5q2ac412hv4 title = \"The Right and Wrong Way to Interpret a Confidence Interval\">}}\n\nWhen we interpret a confidence interval we say that we are \"X% confident\" that the true parameter lies within the interval. For example, if we say we are 95% confident that the true proportion of M&Ms that are blue is between 0.2 and 0.3, it means that if we repeated our sampling many times, about 95% of those intervals would contain the true proportion.\n\nThere are two distinct ways to calculate confidence intervals: using mathematical formulas or through computational methods like bootstrapping. Most of our focus in this course is going to be on the computational approach, or bootstrapping, but it’s important to understand the math behind confidence intervals as well.\n\n## Math-Based Confidence Intervals\n\nConfidence intervals can be calculated using formulas based on the sampling distribution of the statistic. The most common type is the **normal approximation** method, which assumes that the sampling distribution of the sample proportion is approximately normal when the sample size is large enough.\n\nHere’s the basic formula for a confidence interval:\n\n$$\n\\text{Estimate} \\pm z \\times \\text{Standard Error}\n$$\n\nThe $z$ value depends on how confident you want to be:\n\n* For a 95% confidence level, $z \\approx 1.96$\n* For a 90% confidence level, $z \\approx 1.645$\n\n### Example: Confidence Interval for a Proportion\n\nSuppose we survey 100 people, and 64 say they like pineapple on pizza. Our sample proportion $\\hat{p}$ is 0.64.\n\nWe estimate the standard error using:\n\n$$\nSE = \\sqrt{ \\frac{ \\hat{p}(1 - \\hat{p}) }{n} }\n$$\n\nPlugging in the numbers:\n\n$$\nSE = \\sqrt{ \\frac{0.64 \\times 0.36}{100} } \\approx 0.048\n$$\n\nThe 95% confidence interval is:\n\n$$\n0.64 \\pm 1.96 \\times 0.048 = (0.546, 0.734)\n$$\n\nWe are 95% confident that between 54.6% and 73.4% of the population likes pineapple on pizza.\n\nIn the next section, we’ll explore a more flexible approach: **bootstrapping**.\n\n## Estimating Uncertainty with Bootstrapping\n\nBootstrapping is a computational method—and an example of a **nonparametric** approach to inference. *Nonparametric* means that we don't assume a specific shape or distribution for the population (like normality). Instead of relying on formulas, we use the data we have to approximate what repeated sampling might look like.\n\n{{< video https://www.youtube.com/watch?v=Xz0x-8-cgaQ title = 'Bootstrapping Main Ideas' >}}\n\nBootstrapping is a computational method for estimating the variability of a statistic when you only have one sample from the population. It works by simulating what might happen if you could sample again and again—from your existing data.\n\nTo understand the idea, imagine that your original sample is the \"best guess\" we have of the population. If we randomly resample *with replacement* from that sample, we can simulate what other samples might have looked like.\n\nBy computing the same statistic (e.g., a proportion or mean) from each resample, we build up a **bootstrap distribution**. From this, we can estimate standard errors and construct confidence intervals.\n\nThere are a number of good reasons to use bootstrapping:\n\n1) It doesn’t require formulas or assumptions about the shape of the distribution;\n2) It works well even with small sample sizes or skewed data;\n3) It’s easy to implement using R and the `infer` package.\n\n### Worked Example Using `openintro` and `tidymodels`\n\nLet’s use a dataset from the `openintro` package. In this Pew Research survey, 506 Russians were asked whether they believe their country tried to interfere in the 2016 U.S. presidential election. We'll recode the responses into a binary variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nlibrary(openintro)\nlibrary(tidyverse)\n\n# Load and inspect data\nglimpse(russian_influence_on_us_election_2016)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 506\nColumns: 1\n$ influence_2016 <chr> \"Did not try\", \"Did not try\", \"Did not try\", \"Don't kno…\n```\n\n\n:::\n\n```{.r .cell-code}\n# Recode as binary variable\nrussiaData <- russian_influence_on_us_election_2016 |> \n  mutate(try_influence = ifelse(influence_2016 == \"Did try\", 1, 0))\n\n# Summary stats\nrussiaData |> \n  summarize(mean = mean(try_influence), sd = sd(try_influence))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n   mean    sd\n  <dbl> <dbl>\n1 0.150 0.358\n```\n\n\n:::\n:::\n\n\nThis tells us the observed proportion who believe Russia tried to influence the election.\n\nNext, we will create the bootstrap distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n\nset.seed(66)\n\nboot_dist <- russiaData |>\n  specify(response = try_influence) |>\n  generate(reps = 10000, type = \"bootstrap\") |>\n  calculate(stat = \"mean\")\n```\n:::\n\n\nNow, let's use `get_ci()` and `visualize()` from the `infer` package to compute and visualize the confidence interval for our bootstrap distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get confidence interval\nci <- boot_dist |> get_ci(level = 0.95)\n\n# Visualize bootstrap distribution with CI\nboot_dist |>\n  visualize() +\n  shade_ci(ci, color = \"red\", fill = NULL) +\n  labs( \n    title = \"Distribution of the Means of the Bootstrap Samples\",\n    x = \"Mean\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](module-3.4_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThis plot shows the bootstrap distribution of the mean proportion of Russians who believe their country interfered in the election. The shaded region marks the 95% confidence interval.\n\n::: {.callout-warning icon=false}\nThe 95% confidence interval was calculated as (`lower_bound`, `upper_bound`). Which of the following best describes the correct interpretation?\n\n* (a) 95% of the time the percentage of Russians who believe that Russia interfered is between these values.\n* (b) 95% of all Russians believe the probability of interference is within the interval.\n* (c) We are 95% confident that the proportion of Russians who believe that Russia interfered in the 2016 election is between these values.\n* (d) We are 95% confident that the proportion of Russians who supported interference is between these values.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# The answer is (c) We are 95% confident that the proportion of Russians who believe that Russia interfered in the 2016 election is between these values.\n```\n:::\n\n\n:::\n",
    "supporting": [
      "module-3.4_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}