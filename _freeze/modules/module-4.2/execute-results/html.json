{
  "hash": "d3d75ff5a2658f005a5ea45bd07edeba",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Module 4.2\"\nsubtitle: \"Least Squares Regression\"\nformat: \n  html:\n    code-link: true\nfilters:\n  - shinylive\n  - webr\nhighlight-style: atom-one\nexecute: \n  echo: true\n  message: false\n  warning: false\n---\n\n::: {.callout-tip}\n## Prework\n\nRun this code chunk to load the necessary packages and data for this module:\n\n\n::: {.cell}\n\n:::\n\n:::\n\n## Overview\n\nIn Module 4.1, you learned how to fit regression lines and interpret them. But how does R actually find the \"best\" line among all possible lines? This module dives into the mathematical optimization behind least squares regression. You'll understand why it's called \"least squares\" and develop intuition for the cost function that R minimizes when fitting your models.\n\nBy the end of this module, you'll be able to:\n- Explain why we minimize the sum of squared residuals\n- Calculate and interpret a cost function\n- Understand the optimization process behind `lm()`\n- Connect mathematical theory to practical regression output\n\n## The Optimization Problem\n\nWhen we fit a regression line in Module 4.1, we used R's `lm()` function and got specific values for our intercept and slope. But think about it - there are infinitely many possible lines we could draw through any set of points. How does the computer choose which one is \"best\"?\n\nLet's return to our democracy and GDP example from Module 4.1. We found that the relationship between log GDP per capita and democracy scores could be modeled as:\n\n$$\\widehat{Democracy}_i = 0.13 + 0.12 \\times \\log(GDP)_i$$\n\nBut why these specific numbers? Why not $\\widehat{Democracy}_i = 0.10 + 0.15 \\times \\log(GDP)_i$ or any other combination?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](module-4.2_files/figure-html/multiple-lines-1.png){width=672}\n:::\n:::\n\n\nThe answer lies in a mathematical optimization problem. We want to find the line that makes the \"best\" predictions - the line that minimizes our prediction errors across all the data points.\n\n## Understanding the Cost Function\n\nTo understand how we measure \"best,\" let's watch Andrew Ng explain the intuition behind the cost function:\n\n{{< video \"https://www.youtube.com/embed/CFN5zHzEuGY\" title = 'Cost Function Intuition' >}}\n\nAs Andrew explains, we need a way to measure how well our line fits the data. Remember from Module 4.1 that a **residual** is the difference between an actual value and our predicted value:\n\n$$\\text{residual}_i = y_i - \\hat{y}_i$$\n\nThe **cost function** (also called the **loss function**) measures the total error across all our predictions. For least squares regression, we use the **sum of squared residuals** (SSR):\n\n$$\\text{Cost} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n\nBut why do we *square* the residuals instead of just adding them up? There are several good reasons.First, squaring the residuals ensures that the cost function is always positive, which is important for optimization. Squaring ensures that a prediction that's too high (+2) is penalized the same as a prediction that's too low (-2). Second, larger errors get bigger penalties. An error of 4 contributes 16 to the cost, while an error of 2 contributes only 4. Finally, for mathematical convenience. Squared functions have nice mathematical properties that make optimization easier.\n\n::: {.callout-note}\nSome definitions of the cost function divide the SSR by the number of observations $n$ (or two times the number of observations $2n$), yielding the mean squared error (MSE). This doesn't change which line is best—it just rescales the cost. \n:::\n\nThe line that minimizes this cost function is our \"best\" line - the least squares regression line.\n\nLet's build intuition with a very simple example. Consider these three data points:\n- (1, 1)\n- (2, 2) \n- (3, 3)\n\nWhat line would you draw through these points? Let's test different lines and see which has the lowest cost.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create our simple dataset\nsimple_data <- tibble(\n  x = c(1, 2, 3),\n  y = c(1, 2, 3)\n)\n\nsimple_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n      x     y\n  <dbl> <dbl>\n1     1     1\n2     2     2\n3     3     3\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](module-4.2_files/figure-html/best-line1-1.png){width=672}\n:::\n:::\n\n\n**Test Line 1**: $\\hat{y} = 0 + 1 \\times x$ (intercept = 0, slope = 1)\n\nFor each point, let's calculate the predicted value and residual:\n- Point (1,1): $\\hat{y} = 0 + 1(1) = 1$, residual = $1 - 1 = 0$\n- Point (2,2): $\\hat{y} = 0 + 1(2) = 2$, residual = $2 - 2 = 0$  \n- Point (3,3): $\\hat{y} = 0 + 1(3) = 3$, residual = $3 - 3 = 0$\n\nSum of squared residuals = $0^2 + 0^2 + 0^2 = 0$\n\nPerfect! This line goes exactly through all points.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](module-4.2_files/figure-html/best-line2-1.png){width=672}\n:::\n:::\n\n\n**Test Line 2**: $\\hat{y} = 0 + 2 \\times x$ (intercept = 0, slope = 2)\n\nThis is a steeper line with a slope of 2. Let's calculate the residuals:\n\n- Point (1,1): $\\hat{y} = 0 + 0(1) = 0$, residual = $1 - 2 = -1$\n- Point (2,2): $\\hat{y} = 0 + 0(2) = 0$, residual = $2 - 4 = -2$\n- Point (3,3): $\\hat{y} = 0 + 0(3) = 0$, residual = $3 - 6 = -3$\n\nSum of squared residuals = $-1^2 + -2^2 + -3^2 = 1 + 4 + 9 = 14$\n\nMuch worse!\n\n::: {.callout-warning icon=false}\n## Your Turn!!\nAssuming the same data points as in the above example, calculate the sum of squared residuals for the following lines:\n\n- $\\hat{y} = 0 + 3 \\times x$ (intercept = 0, slope = 3)\n- $\\hat{y} = 0 + 0 \\times x$ (intercept = 0, slope = 0)\n- $\\hat{y} = 0 - 1 \\times x$ (intercept = 0, slope = -1)\n\nChange the values in this interactive code chunk to perform your calculations:\n\n\n::: {.cell}\n\n```{.webr .cell-code}\nssr <- (1-1)^2 + (2-2)^2 + (3-3)^2\n\nssr\n```\n:::\n\n\nCheck your answers below when you are finished:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Calculate the sum of squared residuals for the line ŷ = 0 + 3x\n#ssr1 <- (1-3)^2 + (2-6)^2 + (3-9)^2\n#ssr1\n# Answer: 56\n\n# Calculate the sum of squared residuals for the line ŷ = 0 + 0x\n#ssr2 <- (1-0)^2 + (2-0)^2 + (3-0)^2\n#ssr2\n# Answer: 14\n\n# Calculate the sum of squared residuals for the line ŷ = 0 -1x\n#ssr3 <- (1+1)^2 + (2+2)^2 + (3+3)^2\n#ssr3\n# Answer: 56\n```\n:::\n\n:::\n\n## Visualizing the Optimization\n\nLet's see how the cost function behaves as we change the slope parameter. Andrew Ng provides excellent visualization of this concept:\n\n{{< video \"https://www.youtube.com/embed/peNRqkfukYY\" title='Cost Function Visualization'>}}\n\nFor our simple three-point example, let's use this Shiny app to plot the cost function for different slope values (keeping intercept = 0). Move the slide to choose a different slope. See how this changes the fit of the line relative to the points on the left, and how it affects the cost function on the right. \n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 500\n\nlibrary(shiny)\n\ndata_points <- data.frame(x = c(1, 2, 3), y = c(1, 2, 3))\n\nui <- fluidPage(\n  titlePanel(\"Interactive Cost Function Explorer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"slope\", \n                  \"Slope Parameter:\", \n                  min = -2, max = 4, value = 1, step = 0.1),\n      br(),\n      h4(\"Current Values:\"),\n      textOutput(\"current_slope\"),\n      textOutput(\"current_equation\"),\n      textOutput(\"current_ssr\"),\n      br(),\n      p(\"Move the slider to see how the slope affects:\"),\n      tags$ul(\n        tags$li(\"The regression line (left plot)\"),\n        tags$li(\"Your position on the cost function (right plot)\")\n      )\n    ),\n    \n    mainPanel(\n      plotOutput(\"combined_plot\", height = \"400px\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  \n  # Fixed: Calculate predictions and residuals properly\n  current_calculations <- reactive({\n    predictions <- input$slope * data_points$x\n    residuals <- data_points$y - predictions\n    ssr <- sum(residuals^2)\n    list(predictions = predictions, residuals = residuals, ssr = ssr)\n  })\n  \n  cost_data <- reactive({\n    slopes <- seq(-2, 4, by = 0.1)\n    ssr_values <- sapply(slopes, function(b) {\n      preds <- b * data_points$x\n      resids <- data_points$y - preds\n      sum(resids^2)\n    })\n    list(slopes = slopes, ssr = ssr_values)\n  })\n  \n  output$current_slope <- renderText({\n    paste(\"Slope:\", round(input$slope, 2))\n  })\n  \n  output$current_equation <- renderText({\n    paste0(\"Equation: Ŷ = 0 + \", round(input$slope, 2), \" * X\")\n  })\n  \n  # Fixed: Use the corrected reactive calculations\n  output$current_ssr <- renderText({\n    calc <- current_calculations()\n    ssr_terms <- paste0(\"(\", data_points$y, \" - \", round(calc$predictions, 2), \")^2\", collapse = \" + \")\n    ssr_value <- round(calc$ssr, 2)\n    paste0(\"SSR: \", ssr_terms, \" = \", ssr_value)\n  })\n  \n  output$combined_plot <- renderPlot({\n    par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))\n    \n    # Left: Regression plot\n    plot(data_points$x, data_points$y, pch = 19, col = \"blue\",\n         xlim = c(0, 4), ylim = c(-2, 6),\n         xlab = \"x\", ylab = \"y\", main = \"Regression Line\")\n    abline(0, input$slope, col = \"red\", lwd = 2)\n    \n    # Add residual lines for visualization\n    calc <- current_calculations()\n    segments(data_points$x, data_points$y, data_points$x, calc$predictions, \n             col = \"gray\", lty = 2)\n    \n    # Right: Cost function\n    cost <- cost_data()\n    plot(cost$slopes, cost$ssr, type = \"l\", col = \"darkred\", lwd = 2,\n         xlab = \"Slope Parameter\", ylab = \"Sum of Squared Residuals\",\n         main = \"Cost Function\")\n    points(input$slope, calc$ssr, col = \"red\", pch = 19, cex = 1.5)\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\nNotice that the cost function forms a parabola with its minimum at slope = 1. This is exactly where we found SSR = 0! The optimization problem is to find the slope (and intercept) that minimizes this cost function.\n\n## Worked Example: Democracy and GDP\n\nLet's apply this same thinking to our democracy and GDP data. We'll manually test a few different potential regression lines and calculate their costs.\n\nFirst, let's fit the actual least squares line to remind ourselves what R found:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model\ndemocracy_model <- lm(lib_dem ~ log(wealth), data = model_data)\n\n# Show the summary\nsummary(democracy_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,\tAdjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n```\n\n\n:::\n:::\n\n\nThe least squares solution is approximately: $\\widehat{Democracy} = 0.13 + 0.12 \\times \\log(GDP)$\n\nLet's test some alternative lines and see how they compare:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](module-4.2_files/figure-html/visualize-alternatives-1.png){width=672}\n:::\n:::\n\n\nNow let's calculate the sum of squared residuals (SSR) for each of these lines to see which one has the lowest cost:\n\n\n\n- **Least squares line** (intercept = 0.13, slope = 0.12): SSR = 8.57\n- **Steeper slope line** (intercept = 0.13, slope = 0.15): SSR = 9.58\n- **Gentler slope line** (intercept = 0.13, slope = 0.08): SSR = 10.49\n- **Different intercept line** (intercept = 0.00, slope = 0.12): SSR = 11.58\n\nNotice how the least squares line has the lowest SSR! Any other combination of intercept and slope results in a higher cost, confirming that our optimization algorithm found the truly optimal solution.\n\nNotice how the least squares line has the lowest SSR! Any other combination of intercept and slope will result in a higher cost.\n\n\n## From Math to R Output\n\nWhen you run `lm()` in R, the computer is solving this optimization problem automatically. It searches through all possible combinations of intercept and slope values to find the ones that minimize the sum of squared residuals.\n\nFor simple linear regression, there's actually a mathematical formula to find the optimal parameters directly (no searching required). But the key insight is that R is giving you the parameter values that make your predictions as accurate as possible, on average, across all your data points.\n\nThis is why we can trust the output from `lm()` - it's not arbitrary, it's the result of a principled mathematical optimization.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Our model from before\nsummary(democracy_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = lib_dem ~ log(wealth), data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57441 -0.14334  0.03911  0.18730  0.37017 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.13051    0.03806   3.429 0.000758 ***\nlog(wealth)  0.12040    0.01471   8.188 5.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2233 on 172 degrees of freedom\nMultiple R-squared:  0.2805,\tAdjusted R-squared:  0.2763 \nF-statistic: 67.04 on 1 and 172 DF,  p-value: 5.754e-14\n```\n\n\n:::\n\n```{.r .cell-code}\n# The SSR for this model\nactual_ssr <- sum(residuals(democracy_model)^2)\ncat(\"Sum of squared residuals for least squares line:\", round(actual_ssr, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSum of squared residuals for least squares line: 8.574\n```\n\n\n:::\n:::\n\n\n::: {.callout-important}\n## Understanding the Code\n\nIn the last line of this code chunk we apply the `cat()` function to print the sum of squared residuals (SSR) for the least squares line. The `cat()` function is used to concatenate and print text and variables together in a single output. The `round()` function is applied to format the SSR value to three decimal places for better readability.\n:::\n\n::: {.callout-warning icon=false}\n## Your Turn!\n- Try running a regression with the `polarization` variable as the predictor of liberal democracy instead of `wealth`.\n- Calculate the sum of squared residuals (SSR) for this new model.\n:::\n\n## Summary\n\nThe \"least squares\" in least squares regression refers to the optimization principle: find the line that minimizes the sum of squared residuals. This mathematical framework ensures that: 1) your predictions are as accurate as possible on average across all data points; 2) the solution is unique in that there  only one best line for any dataset; the method is principled (not arbitrary) because it is based on mathematical optimization; and 4) R's output is trustworthy because `lm()` is finding the genuinely best-fitting line.\n\nUnderstanding this optimization principle helps you appreciate why regression works and gives you confidence in interpreting the results. When you see regression coefficients, you now know they represent the solution to a well-defined mathematical problem: finding the line that makes the best predictions for your data.\n\n",
    "supporting": [
      "module-4.2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}