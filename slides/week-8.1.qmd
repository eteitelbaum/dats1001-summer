---
title: Linear Regression
date: today
date-format: long
footer: "[DATS 1001 Website](https://ds4all.rocks)"
logo: images/ds4all-logo.png
format:
  revealjs:
    theme: [simple, custom.scss]
    transition: fade
    slide-number: true
    #multiplex: true
    chalkboard: true
execute:
  echo: true
  message: false
  warning: false
  freeze: auto
---


```{r} 
#| label: packages
#| echo: false

library(tidyverse)
library(dplyr)
library(ggplot2)
library(tidymodels)

set.seed(1234)
```


## Modeling 

::: {.incremental}
- Use models to explore the relationship between variables and to make predictions
- Explaining relationships (usually interested in causal relationships, but not always)
    - Does oil wealth impact regime type?
- Predictive modeling
    - Where is violence most likely to happen in (country X) during their next election?
    - Is this email spam?
:::
    
## Modeling

```{r}
#| label: linear-model
#| echo: false

df1 <- tibble(x = 1:100, y = x + rnorm(100, mean = 0, sd = 5))

ggplot(df1, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  labs(title = "Linear", x = NULL, y = NULL) +
  theme_bw()
```

## Modeling

```{r}
#| label: nonlinear-model
#| echo: false

df2 <- tibble(x = seq(-6, 5.9, 0.1), y = (1 / (1+exp(-2*x))) + rnorm(120, mean = 0, sd = 0.1))

ggplot(df2, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "loess", color = "#8E2C90", se = FALSE) +
  labs(title = "Non-linear", x = NULL, y = NULL) +
  theme_bw()
```

# Example: GDP per capita and Democracy

## Pull in the VDEM Data

<br>

```{r}
#| label: wrangle-vdem
#| output: false

library(vdemlite)

model_data <- fetchdem(indicators = c("v2x_libdem", "e_gdppc"),
                       start_year = 2019, end_year = 2019) |>
  rename(
    country = country_name, 
    lib_dem = v2x_libdem, 
    wealth = e_gdppc
    ) 

glimpse(model_data)
```

## Plot the Data

```{r}
#| label: plot-wealth-dem-1
#| echo: false

ggplot(model_data, aes(x = wealth, y = lib_dem)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  scale_x_log10(label = scales::label_dollar(suffix = "k")) +
  labs(
    title = "Wealth and Democracy, 2019",
    x = "GPD per capita", 
    y = "Liberal Democracy Index") +
  theme_bw()
```

## Plot the Data

<br>

```{r}
#| label: plot-wealth-dem-2
#| code-line-numbers: "0,1-3|4|5-10"
#| eval: false

ggplot(model_data, aes(x = wealth, y = lib_dem)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  scale_x_log10(label = scales::label_dollar(suffix = "k")) +
  labs(
    title = "Wealth and Democracy, 2019",
    x = "GPD per capita", 
    y = "Liberal Democracy Index") +
  theme_bw()
```

## Models as Functions

::: {.incremental}
- We can represent relationships between variables using **functions**
- A function is a mathematical concept: the relationship between an output and one or more inputs
  - Plug in the inputs and receive back the output
- Example: The formula $y = 3x + 7$ is a function with input $x$ and output $y$. 
    - If $x$ is $5$, $y$ is $22$, 
    - $y = 3 \times 5 + 7 = 22$
:::

## Quant Lingo {.smaller}

<br>

::: {.incremental}
- **Response variable:** Variable whose behavior or variation you are trying to understand, on the y-axis in the plot
    - **Dependent** variable
    - **Outcome** variable
    - **Y** variable
- **Explanatory variables:** Other variables that you want to use to explain the variation in the response, on the x-axis in the plot
    - **Independent** variables
    - **Predictors**
:::

## 

<br>

Linear model with one explanatory variable...

::: {.incremental}
- $Y = a + bX$
- $Y$ is the outcome variable
- $X$ is the explanatory variable
- $a$ is the intercept: the predicted value of $Y$ when $X$ is equal to 0
- $b$ is the slope of the line (rise over run)
:::

## Quant Lingo {.smaller}

<br>

::: {.incremental}
- **Predicted value:** Output of the **model function**
   - The model function gives the typical (expected) value of the response variable *conditioning* on the explanatory variables
   - We often call this $\hat{Y}$ to differentiate the predicted value from an observed value of Y in the data
- **Residuals:** A measure of how far each case is from its predicted value (based on a particular model)
  - Residual = Observed value ($Y$) - Predicted value ($\hat{Y}$)
  - How far above/below the expected value each case is
:::

##

<br>

::: {.callout-caution}
Note that for the next few examples we will be analyzing GDP per capita on a log scale.
:::

## Residuals

```{r}
#| label: residuals
#| echo: false

modelData <- model_data |>
  mutate(log_wealth = log(wealth))

mod_fit <- linear_reg() |>
  set_engine("lm") |>
  fit(lib_dem ~ log_wealth, data = modelData)

fit_tidy <- tidy(mod_fit$fit) 
fit_aug  <- augment(mod_fit$fit) |>
  mutate(res_cat = ifelse(.resid > 0, TRUE, FALSE))

a <- round(fit_tidy$estimate[1], 2)
b <- round(fit_tidy$estimate[2], 2)

ggplot(data = fit_aug) +
  geom_point(aes(x = log_wealth, y = lib_dem, color = res_cat)) +
  geom_line(aes(x = log_wealth, y = .fitted), size = 0.75, color = "#8E2C90") + 
  labs(
    title = "GDP per Capita and Democracy",
    x = "Log GDP per Capita",
    y = "Libearl Democracy Index"
  ) +
  guides(color = "none") +
  scale_color_manual(values = c("#260b27", "darkorange")) +
  theme_bw()
```

## Linear Model

$\hat{Y} = a  + b \times X$

$\hat{Y} = `r a`  + `r b` \times X$

```{r}
#| echo: false

ggplot(modelData, aes(x = log_wealth, y = lib_dem)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  labs(x = "GPD per capita", y = "Liberal Democracy Index") +
  theme_bw()
```

## Linear Model: Interpretation

<br>

| $\hat{Y} = a  + b \times X$
| $\hat{Y} = `r a`  + `r b` \times X$

What is the interpretation of our estimate of $a$?

. . .

<br>

| $\hat{Y} = `r a`  + `r b` \times 0$
| $\hat{Y} = `r a`$

$a$ is our predicted level of democracy when GDP per capita is 0.


## Linear Model: Interpretation 
<br>


| $\hat{Y} = a  + b \times X$
| $\hat{Y} = `r a`  + `r b` \times X$

What is interpretation of our estimate of $b$?

. . . 

<br>

| $\hat{Y} = a  + \frac{Rise}{Run} \times X$
| $\hat{Y} = a  + \frac{Change Y}{Change X} \times X$

## Linear Model: Interpretation {.smaller}

<br>

| $b = \frac{Change Y}{Change X}$
| $`r b` = \frac{Change Y}{Change X}$
| ${Change Y} = `r b` * {ChangeX}$

. . .

<br>

| When $ChangeX = 1$:
| ${Change Y = `r b`}$

. . .

<br>

| $b$ is the predicted change in $Y$ **associated with** a ONE unit change in X.

## Linear Model: Interpretation


```{r}
#| echo: false

ggplot(modelData, aes(x = log_wealth, y = lib_dem)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  labs(x = "Log GPD per capita", y = "Liberal Democracy Index") +
  theme_bw()
```

## Linear Model: Interpretation


```{r}
#| echo: false

ggplot(modelData, aes(x = log_wealth, y = lib_dem)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  labs(x = "Log GPD per capita", y = "Liberal Democracy Index") +
  geom_segment(aes(x = 2, xend= 3, y=.37, yend=.37), colour="darkblue", linewidth=1.5, arrow = arrow(length = unit(0.5, "cm"))) +
  theme_bw()
  
#Ŷ =0.13+0.12×X
```

## Linear Model: Interpretation


```{r}
#| echo: false

ggplot(modelData, aes(x = log_wealth, y = lib_dem)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  labs(x = "Log GPD per capita", y = "Liberal Democracy Index") +
  geom_segment(aes(x = 2, xend= 3, y=.37, yend=.37), colour="darkblue", linewidth=1.5) +
  geom_segment(aes(x = 3, xend= 3, y=.37, yend=.49), colour="darkblue", linewidth=1.5, arrow = arrow(length = unit(0.5, "cm"))) +
  geom_text(x=3.2, y=.43, label="0.12", color="darkblue", size=4) +
  theme_bw()
  
#Ŷ =0.13+0.12×X
```

## Interpreting the Coefficient for log(Wealth)

$$
\text{Democracy} = 0.12 \times \log(\text{Wealth})
$$

For **small** percentage changes in a "log-linear" model, we can use the approximation rule (divide by 100):

- A **1% increase** in GDP per capita is associated with a **0.0012** point increase in the democracy score.  
  - **Rule:** $0.12 / 100 = 0.0012$  

## For Larger Changes We Log the Change

<br>

- A **10% increase** (e.g., from \$10,000 to \$11,000) increases the democracy score by **0.0114 points** since:

  $$
  0.12 \times \ln(1.1) \approx 0.12 \times 0.0953 = 0.0114
  $$  

## Similarly...

- **Doubling** GDP per capita (e.g., \$10,000 → \$20,000) increases the democracy score by:

  $$
  0.12 \times \ln(2) \approx 0.12 \times 0.693 = 0.083
  $$

- **Tripling** GDP per capita (e.g., \$10,000 → \$30,000) increases the democracy score by:

  $$
  0.12 \times \ln(3) \approx 0.12 \times 1.099 = 0.132
  $$  

## Linear Model: Interpretation

<br>

Is this the **causal** effect of GDP per capita on liberal democracy?

. . .

<br>

No! It is only the association...

. . .

<br>

To identify causality we need other methods (beyond the scope of this course).

## Your Task {.smaller}

<br> 

An economist is interested in the relationship between years of education and hourly wages.  They estimate a linear model with estimates of $a$ and $b$ as follows:

<br>

$\hat{Y} = 9 + 1.60*{YrsEdu}$

<br>

| 1. Interpret $a$ and $b$
| 2. What is the predicted hourly wage for those with 10 years of education?
| 3. How about for those with a high school diploma? (12 yrs)
| 4. What about a college degree? (16 yrs)

## Next step

<br>

- Linear model with one predictor: $Y = a + bX$
- For any given data...
- How do we figure out what the best values are for $a$ and $b$??

```{r}
#| label: setup
#| echo: false

library(tidyverse)
library(tidymodels)
library(vdemdata)
```

# Estimation

## Linear Model with Single Predictor

<br>

Goal: Estimate Democracy score ($\hat{Y_{i}}$) of a country given level of GDP per capita ($X_{i}$).

<br>

Or: Estimate relationship between GDP per capita and democracy.

## Linear Model with Single Predictor

```{r}
#| label: model-data
#| echo: false

modelData <- vdem |> 
  filter(year == 2019) |> 
  select(
    country = country_name, 
    lib_dem = v2x_libdem, 
    wealth = e_gdppc, 
    corruption = v2x_corr, 
    ) |>
  mutate(log_wealth = log(wealth)) 
  

ggplot(modelData, aes(x = log_wealth, y = lib_dem)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  labs(x = "GDP per capita", y = "Liberal Democracy Index") +
  theme_bw()
```

## Estimate Model 

```{r}
#| label: fit-model
#| echo: true

model1 <-  lm(lib_dem ~ log_wealth, data = modelData) 

summary(model1)
```

## 

In equation form... How do we interpret the model?

<br>

$$\widehat{Democracy}_{i} = 0.13 + 0.12 * {loggdppc}_{i}$$

## Question

<br>

How do we get the "best" values for the slope and intercept?


## How would you draw the "best" line?

```{r}
#| label: best-line
#| echo: false

ggplot(modelData, aes(x = log_wealth, y = lib_dem)) +
  geom_point() +
 # geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  labs(x = "GDP per capita", y = "Liberal Democracy Index") +
  theme_bw()
```

## How would you draw the "best" line?

```{r}
#| label: best-line2
#| echo: false

ggplot(modelData, aes(x = log_wealth, y = lib_dem)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  labs(x = "GDP per capita", y = "Liberal Democracy Index") +
  theme_bw()
```

## Least squares regression

<br>

- Remember the residual is the difference between the actual value and the predicted value

. . .

- The regression line minimizes the sum of squared residuals.

## Least squares regression

<br>

- Residual for each point is:  $e_i = y_i - \hat{y}_i$

- Least squares regression line minimizes $\sum_{i = 1}^n e_i^2$.

. . .

- Why do we square the residual?

. . .

- Why not take absolute value?

    - Principle: larger penalty for residuals further away
    - Math: makes the math easier and some nice properties (not our concern here...)

## Least squares regression


```{r}
#| label: best-line3 
#| echo: false

mod_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(lib_dem ~ log_wealth, data = modelData)

fit_tidy <- tidy(mod_fit$fit) 
fit_aug  <- augment(mod_fit$fit) %>%
  mutate(res_cat = ifelse(.resid > 0, TRUE, FALSE))

a <- round(fit_tidy$estimate[1], 2)
b <- round(fit_tidy$estimate[2], 2)

ggplot(data = fit_aug) +
  geom_point(aes(x = log_wealth, y = lib_dem, color = res_cat)) +
  geom_line(aes(x = log_wealth, y = .fitted), size = 0.75, color = "#8E2C90") + 
  labs(
    title = "GDP per Capita and Democracy",
    x = "GDP per Capita",
    y = "Libearl Democracy Index"
  ) +
  guides(color = "none") +
  scale_color_manual(values = c("#260b27", "darkorange")) +
  theme_bw()
#+
#  geom_text(aes(x = 0, y = 150), label = "Positive residual", color = "#e6b0e7", hjust = 0, size = 8) +
 # geom_text(aes(x = 150, y = 25), label = "Negative residual", color = "#260b27", hjust = 0, size = 8)

```

## Very Simple Example

What should the slope and intercept be?

```{r}
#| label: best-line4
#| echo: false

# create data
dat <- tibble(
    x = c(1, 2, 3),
    y = c(1, 2, 3)
)

ggplot(dat, aes(y=y, x=x)) +
  geom_point(size=3, color="darkblue") +
  xlim(0, 4) + ylim(0,4) +
  theme_bw()
```


## Example

$\hat{Y} = 0 + 1*X$

```{r}
#| label: best-line5
#| echo: false

# create data
ggplot(dat, aes(y=y, x=x)) +
  geom_point(size=3, color="darkblue") +
  xlim(0, 4) + ylim(0,4) +
  geom_segment(x=0, y=0, xend=4, yend=4, color="darkorange") +
  theme_bw()
```

## Example

What is the sum of squared residuals?

```{r}
#| label: best-line6
#| echo: false

# create data
ggplot(dat, aes(y=y, x=x)) +
  geom_point(size=3, color="darkblue") +
  xlim(0, 4) + ylim(0,4) +
  geom_segment(x=0, y=0, xend=4, yend=4, color="darkorange") +
  theme_bw()
```

## Example

What is sum of squared residuals for $y = 0 + 0*X$?

```{r}
#| label: best-line7
#| echo: false

# create data
 ggplot(dat, aes(y=y, x=x)) +
  geom_point(size=3, color="darkblue") +
  xlim(0, 4) + ylim(0,4) +
  geom_segment(x=0, y=0, xend=4, yend=0, color="black") +
  theme_bw()
```


## Example

What is sum of squared residuals for $y = 0 + 0*X$?

```{r}
#| label: best-line8
#| echo: false

# create data
 ggplot(dat, aes(y=y, x=x)) +
  geom_point(size=3, color="darkblue") +
  xlim(0, 4) + ylim(0,4) +
  geom_segment(x=0, y=0, xend=4, yend=0, color="black") +
  theme_bw()
```


```{r}
#| label: best-line9

(1-0)^2 + (2-0)^2 + (3-0)^2
```

## Example

What is sum of squared residuals for $y = 0 + 2*X$?

```{r}
#| label: best-line10
#| echo: false

# create data
 ggplot(dat, aes(y=y, x=x)) +
  geom_point(size=3, color="darkblue") +
  xlim(0, 4) + ylim(0,8) +
  geom_segment(x=0, y=0, xend=4, yend=8, color="black") +
  theme_bw()
```

## Example

What is sum of squared residuals for $y = 0 + 2*X$?

```{r}
#| label: best-line11
#| echo: false

 ggplot(dat, aes(y=y, x=x)) +
  geom_point(size=3, color="darkblue") +
  xlim(0, 4) + ylim(0,8) +
  geom_segment(x=0, y=0, xend=4, yend=8, color="black") +
  theme_bw()
```


```{r}
#| label: best-line12

(1-2)^2 + (2-4)^2 + (3-6)^2
```


## One more...

What is sum of squared residuals for $y = 0 + -1*X$?

```{r}
#| label: best-line13
#| echo: false

 ggplot(dat, aes(y=y, x=x)) +
  geom_point(size=3, color="darkblue") +
  xlim(0, 4) + ylim(-4,4) +
  geom_segment(x=0, y=0, xend=4, yend=-4, color="black") +
  theme_bw()
```

## One more...

What is sum of squared residuals for $y = 0 + -1*X$?

```{r}
#| label: best-line14
#| echo: false

 ggplot(dat, aes(y=y, x=x)) +
  geom_point(size=3, color="darkblue") +
  xlim(0, 4) + ylim(-4,4) +
  geom_segment(x=0, y=0, xend=4, yend=-4, color="black") +
  theme_bw()
```


```{r}
#| label: best-line15

(1+1)^2 + (2+2)^2 + (3+3)^2
```

## Cost Function

Sum of Squared Residuals as function of possible values of $b$

```{r}
#| label: cost-function
#| echo: false

sse <- tibble(
          b=c(-2, -1, 0, 1, 2, 3, 4), 
          c=c(81, 56, 14, 0, 14, 56, 81)
          )

ggplot(sse, aes(y=c, x=b)) +
    geom_point(size=3, color="darkred") +
    labs(
    x = "Slope (b)",
    y = "Sum of Squared Residuals"
  ) +
  theme_bw()
```


## Least Squares Regression

<br>

- When we estimate a least squares regression, it is looking for the line that minimizes sum of squared residuals

- In the simple example, I set $a=0$ to make it easier.  More complicated when searching for combination of $a$ and $b$ that minimize, but same basic idea

## Least Squares Regression

<br>

- There is a way to solve for this analytically for linear regression (i.e., by doing math...)

    -- They made us do this in grad school...

. . .

- In machine learning, people also use gradient descent algorithm in which the computer searches over possible combinations of $a$ and $b$ until it settles on the lowest point.    

## Least Squares Regression

```{r}
#| label: ssr-viz
#| echo: false

sse <- tibble(
          b=c(-2, -1, 0, 1, 2, 3, 4), 
          c=c(81, 56, 14, 0, 14, 56, 81)
          )

ggplot(sse, aes(y=c, x=b)) +
    geom_point(size=3, color="darkred") +
  #  geom_line(color = "darkred") +
    labs(
    x = "Slope (b)",
    y = "Sum of Squared Residuals"
  ) +
  theme_bw()
```

## Least Squares Regression

```{r}
#| label: regression-line
#| echo: false

ggplot(modelData, aes(x = log_wealth, y = lib_dem)) +
  geom_point() +
  geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  labs(x = "GDP per capita", y = "Liberal Democracy Index") +
  theme(
    axis.text  = element_blank(),
    axis.ticks = element_blank()
    ) +
  theme_bw()
```
