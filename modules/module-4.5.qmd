---
title: "Module 4.5"
subtitle: "Model Selection"
format: 
  html:
    code-link: true
highlight-style: atom-one
execute: 
  echo: true
  message: false
  warning: false
---

::: {.callout-tip}
## Prework

- Install the `olsrr` package: `install.packages("olsrr")` and read the documentation pertaining to [variable selection](https://olsrr.rsquaredacademy.com/articles/variable_selection#introduction)
- Run the following code to get set up for this module:

```{r}
#| label: setup
#| code-fold: true

library(tidyverse)
library(vdemlite)

# Load V-Dem data for 2019
model_data <- fetchdem(
  indicators = c(
  "v2x_libdem", 
  "e_gdppc", 
  "v2cacamps",
  "v2x_gender",
  "v2x_corr",
  "e_regionpol_6C"),
  start_year = 2006, 
  end_year = 2006
  ) |>
  rename(
    country = country_name, 
    lib_dem = v2x_libdem, 
    wealth = e_gdppc,
    polarization = v2cacamps,
    women_empowerment = v2x_gender,
    corruption = v2x_corr,
    region = e_regionpol_6C
    ) |>
  mutate(
    region = factor(
    region,
    labels = c(
      "Eastern Europe", 
      "Latin America", 
      "MENA", 
      "Sub-Saharan Africa", 
      "Western Europe & North America", 
      "Asia & Pacific")),
    log_wealth = log(wealth) # have to manually log-transform wealth for olsrr
    ) |>
  drop_na(lib_dem:region) # remove missing values for olsrr

#glimpse(model_data)
```
:::

## Overview

Now that we know how to run a regression we can talk about how to select the best model. In this module, we will explore the process of **model selection** in multiple regression contexts. Model selection involves choosing which predictor variables to include in a regression model to best explain the outcome variable while balancing complexity and interpretability.

This decision process involves navigating the fundamental **bias-variance tradeoff** in statistical modeling. Including more predictors can reduce bias by capturing important relationships between variables and the outcome. However, additional predictors also increase model variance by making the model more susceptible to overfitting to the specific sample under study. Conversely, models with fewer predictors may exhibit reduced variance but increased bias through the omission of important explanatory variables.

Several approaches can guide variable selection decisions. *Theory-driven selection* incorporates variables based on theoretical understanding of the phenomenon under investigation based on domain expertise. *Statistical criteria* employ measures such as AIC, BIC, or adjusted R-squared to balance model fit against complexity. *Cross-validation* assesses model performance on held-out data to evaluate generalizability. 

Multiple regression modeling also requires consideration of several methodological concerns. **Multicollinearity** occurs when predictor variables exhibit high correlations with one another, complicating the separation of individual variable effects. **Overfitting** represents the tendency for models to fit training data too closely, resulting in poor generalization to new observations. Additionally, complex models present interpretation challenges, and larger numbers of predictors necessitate correspondingly larger sample sizes for reliable parameter estimation.

{{< video https://youtu.be/4pgaVtVBVjk?si=Wq93tjXuVj42s4Yf title='Principles of Model Selection'>}}

## Model Evaluation Criteria

Model selection procedures require criteria for comparing alternative specifications. Each criterion approaches the balance between model fit and complexity differently, leading to potentially different optimal models.

### Adjusted R-squared

Adjusted R-squared addresses a fundamental limitation of regular R-squared: the tendency for R-squared to increase mechanically with the addition of any predictor variable, regardless of that variable's true explanatory value. The adjusted R-squared applies a penalty for additional parameters according to the formula:

$$R_{adj}^2 = 1 - \frac{(1 - R^2)(n - 1)}{n - k - 1}$$

where $n$ represents the number of observations and $k$ represents the number of predictor variables. Higher adjusted R-squared values indicate superior model performance, as they reflect greater explained variance while accounting for model complexity.

### Akaike Information Criterion (AIC)

The Akaike Information Criterion provides an alternative approach to model evaluation grounded in information theory. AIC balances model fit against complexity through the formula:

$$AIC = -2(\text{log-likelihood}) + 2k$$

where $k$ represents the number of parameters (including predictors and intercept) and log-likelihood measures model fit to the observed data. **Lower AIC values indicate superior models**, representing the opposite interpretation from R-squared measures. AIC typically favors slightly more complex models than adjusted R-squared due to its lighter penalty structure for additional parameters.

### P-values as Selection Criteria

Recall that p-values represent the probability of observing a test statistic as extreme or more extreme than what was actually observed, assuming the null hypothesis is true. In regression contexts, p-values test whether individual regression coefficients differ significantly from zero. p-values have historically been employed in variable selection procedures, with common approaches involving the removal of variables with p-values exceeding predetermined thresholds (typically 0.05 or 0.10).

However, p-value-based variable selection introduces important methodological concerns. Stepwise selection increases the risk of a **false positive** (Type 1 error) by overfitting to noise. In the standard hypothesis testing framework featuring a five percent critical value, one out of 20 tests will yield a statistically significant result purely by chance. This risk is compounded in stepwise selection procedures that test multiple variables sequentially, as each test increases the likelihood of identifying spurious relationships.

Stepwise regression can also lead to **false negatives** (Type II error) by excluding variables that may have substantive theoretical importance but appear non-significant in intermediate steps. 

Another concern is **post-selection inference bias** resulting from the fact that the same data are used both to select the model and to conduct inference. In this context, the reported p-values and confidence intervals no longer reflect valid hypothesis tests as they understate the true uncertainty associated with the selected model. 

These limitations suggest that p-value-based selection should be approached with considerable caution, particularly compared to criteria like adjusted R-squared or AIC that explicitly account for the model selection process and do not suffer from the same multiple testing problems.

::: {.callout-important}
## Modern Approaches to Model Selection

While stepwise selection remains widely employed, contemporary statistical practice increasingly favors alternative approaches that address some limitations of traditional methods.

**Regularization methods** such as LASSO and Ridge regression incorporate variable selection directly into the model fitting process. These techniques apply penalties to coefficient magnitudes, automatically shrinking less important predictors toward zero. 

**Cross-validation** approaches evaluate model performance through systematic partitioning of data into training and testing subsets. This strategy provides more reliable assessments of generalizability compared to information criteria calculated on the same data used for model fitting.

**Ensemble methods** acknowledge model uncertainty by averaging predictions across multiple plausible models rather than selecting a single optimal specification. These approaches often demonstrate superior predictive performance compared to any individual model.

These advanced techniques are important to be aware of even though they exceed the scope of introductory regression courses.
:::

## Automated Stepwise Selection

Automated selection procedures can systematically implement variable selection strategies. This section demonstrates these approaches using data from the Varieties of Democracy (V-Dem) project to predict liberal democracy scores across countries.

The analysis begins with estimation of a full model incorporating all available predictors:

```{r}
#| label: full-model

full_model <- lm(lib_dem ~ log_wealth + polarization + women_empowerment + 
                 corruption + region, data = model_data)

summary(full_model)
```

The full model achieves an adjusted R-squared of `r round(summary(full_model)$adj.r.squared, 3)`, indicating that approximately `r round(summary(full_model)$adj.r.squared * 100, 1)` percent of the variance in democracy scores can be explained by the included predictors.

### Forward Selection

Forward selection begins with an intercept-only model and sequentially adds predictors. At each step, the procedure selects the variable that produces the greatest improvement in the chosen criterion (adjusted R-squared in this example):

```{r}
#| label: forward-selection
library(olsrr)

forward_model <- ols_step_forward_adj_r2(full_model)

forward_model
```

The forward selection results demonstrate the sequential addition process. Women's empowerment enters first due to its highest individual contribution to adjusted R-squared, followed by corruption control, with the process continuing until no remaining variables provide meaningful improvements to model fit.

### Backward Elimination

Backward elimination employs the opposite strategy, beginning with the full model and sequentially removing predictors. At each step, the procedure eliminates the variable whose removal least deteriorates model performance:

```{r}
#| label: backward-elimination

backward_model <- ols_step_backward_adj_r2(full_model)

backward_model
```

### Bidirectional Selection

Bidirectional selection combines forward and backward approaches, allowing both addition and removal of variables at each step. This method can potentially identify optimal models that pure forward or backward procedures might miss:

```{r}
#| label: bidirectional-selection

both_model <- ols_step_both_adj_r2(full_model)

both_model
```

Comparison of the full and stepwise-selected models reveals how little changes in explanatory power despite model simplification. The adjusted R-squared for the full model is `r round(summary(full_model)$adj.r.squared, 4)`, while the adjusted R-squared for the model selected via stepwise procedures is `r round(summary(both_model$model)$adj.r.squared, 4)`. At the same time, the number of predictors decreases slightly, from `r length(coef(full_model)) - 1` in the full model to `r length(coef(both_model$model)) - 1` in the selected model. 

This minimal difference in fit highlights an important point: variable selection procedures may reduce complexity but often yield models with similar performance. This highlights the relevance of domain expertise in making meaningful modeling decisions.

## Incorporating Domain Knowledge

Statistical criteria alone provide insufficient guidance for model specification. Substantive theory and domain expertise must inform variable selection decisions. In democracy research, certain variables maintain theoretical importance regardless of their statistical significance in particular samples.

For example, *modernization theory* establishes wealth (GDP per capita) as a fundamental predictor of democratic development while comparative political analysis routinely includes regional controls to account for spatial diffusion effects and shared historical experiences. 

In this scenario, we have strong reasons to believe that wealth and region are important predictors of democracy. We are, in other words, developing our model based on our **theoretical understanding** of the world, rather than simply relying on statistical criteria. Or, another way of stating it is that we are basing our model on our **hypotheses** about the outcome in question, rather than simply relying on statistical criteria.

To this end, the `olsrr` package accommodates theoretical constraints through the `include` argument, which forces retention of specified variables throughout the selection process:

```{r}
#| label: theory-guided-selection

theory_guided_model <- ols_step_both_adj_r2(full_model, include = c("log_wealth", "region"))

theory_guided_model
```

The mandatory inclusion of wealth and regional indicators alters the selection process, demonstrating how theoretical considerations should guide rather than merely supplement statistical procedures. In this case, the theory guided approach essentially brought us back to the full model, but in other contexts you may find that the model is more parsimonious while still retaining theoretically important variables.

::: {.callout-warning icon=false}
## Your Turn!!

Apply model selection using AIC as the evaluation criterion for the democracy data by using the `ols_step_both_aic()` function from the `olsrr` package. 

1. Perform bidirectional stepwise selection using AIC
2. Compare the selected variables to those chosen by adjusted R-squared
3. Calculate and compare AIC values for both the full model and your selected model
:::

::: {.callout-note collapse="true"}
## Solution

```{r}
#| label: solution-aic-selection

# Bidirectional selection using AIC
aic_model <- ols_step_both_aic(full_model)
aic_model

# Compare AIC values
cat("Full Model AIC:", round(AIC(full_model), 2), "\n")
cat("Selected Model AIC:", round(AIC(aic_model$model), 2), "\n")

# Compare which variables were selected
cat("\nVariables in Adjusted R-squared model:", 
    paste(names(coef(both_model$model))[-1], collapse = ", "), "\n")
cat("Variables in AIC model:", 
    paste(names(coef(aic_model$model))[-1], collapse = ", "), "\n")
```
:::

## Additional Methodological Considerations

### Multicollinearity

**Multicollinearity** occurs when predictor variables exhibit substantial correlations with one another. This condition creates several analytical problems including unstable coefficient estimates, inflated standard errors that reduce the power to detect significant relationships, and interpretation difficulties when attempting to isolate individual variable effects.

**Variance Inflation Factors (VIF)** provide a diagnostic tool for assessing multicollinearity severity:

We can use the `ols_vif_tol()` function from the `olsrr` package to calculate VIF values for the full model:

```{r}
ols_vif_tol(full_model)
```

Conventional guidelines suggest that VIF values exceeding 5-10 indicate problematic multicollinearity requiring remedial action. Here we do not see any variables exceeding that threshold so we should feel comfortable ruling out multicollinearity as a major concern in this model.

::: {.callout-note}
Another common package for calculating VIF is `car`, which provides the `vif()` function. You can use it by simply loading `car` (after you have installed it of course!) and running `vif(full_model)`.
:::

Examination of correlations among continuous predictors provides additional insight into potential multicollinearity issues:

```{r}
# Correlation matrix for numeric predictors
model_data |>
  select(log_wealth, polarization, women_empowerment, corruption) |>
  cor() |>
  round(3)
```

Here we would be looking for correlations exceeding 0.7 or 0.8, which would indicate potential multicollinearity concerns. In this case, we do not see any such high correlations, again giving us confidence to proceed with the analysis. 

### Sample Size Requirements

Adequate sample sizes ensure reliable parameter estimation in multiple regression models. The conventional guideline recommends minimum ratios of 10-15 observations per predictor variable. The current analysis includes `r nrow(model_data)` observations and `r length(coef(full_model)) - 1` predictors, yielding a ratio of `r round(nrow(model_data) / (length(coef(full_model)) - 1), 1)`:1, which meets standard adequacy thresholds.

### Model Validation

Selected models require validation to assess their reliability and generalizability. **Residual analysis** examines whether regression assumptions hold in the fitted model. **Cross-validation** techniques test model performance on independent data subsets. **Out-of-sample prediction** provides the strongest test of model generalizability when feasible. **Substantive interpretation** ensures that statistical results align with theoretical expectations and domain knowledge. We will discuss some of these issues in greater detail in a subsequent module on regression diagnostics.

## Summary

Model selection involves balancing competing objectives of explanatory power and parsimony. Multiple evaluation criteria exist, including adjusted R-squared, AIC, and p-values, each with distinct properties that can lead to different optimal models. Automated stepwise procedures provide systematic approaches to variable selection, though they should supplement rather than replace theoretical reasoning.

Domain knowledge plays a crucial role in model specification, often requiring the retention of theoretically important variables regardless of their statistical performance in particular samples. Additional methodological considerations include multicollinearity assessment, sample size adequacy, and model validation procedures.

The ultimate goal of model selection extends beyond maximizing statistical criteria to developing models that are simultaneously statistically sound and substantively meaningful. This dual objective requires integrating statistical techniques with theoretical understanding and domain expertise throughout the modeling process.